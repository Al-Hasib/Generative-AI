{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# # Access the API key from the environment\n",
    "# api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "\n",
    "# print(api_key)\n",
    "# # Set the API key as an environment variable\n",
    "# os.environ[\"ANTHROPIC_API_KEY\"] = api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI \n",
    "# Create a ChatOpenAI model\n",
    "model = ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    "# Invoke the model with a message\n",
    "result = model.invoke(\"What is 81 divided by 9?\")\n",
    "print(\"Full result:\")\n",
    "print(result)\n",
    "print(\"Content only:\")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "# Now, you can use the API key in your ChatAnthropic model\n",
    "model = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n",
    "# Invoke the model with a message\n",
    "result = model.invoke(\"What is 81 divided by 9?\")\n",
    "print(\"Full result:\")\n",
    "print(result)\n",
    "print(\"Content only:\")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"(Verse 1)\\nIn the realm of data, vast and wide,\\nWhere information flows like a tide,\\nA champion arose, with chains of code,\\nLangChain its name, a tale to be told.\\n\\n(Verse 2)\\nWith links of logic, it weaves its spell,\\nConnecting models, stories to tell,\\nFrom GPT's prose to BERT's keen eye,\\nLangChain empowers, reaching for the sky.\\n\\n(Verse 3)\\nNo more silos, data confined,\\nLangChain breaks barriers, one of a kind,\\nChains of reasoning, a symphony bright,\\nUnlocking knowledge, day and night.\\n\\n(Verse 4)\\nChatbots converse, with wisdom profound,\\nAnswers emerge, with nary a sound,\\nSummarization's art, a breeze so light,\\nLangChain's magic, shining ever so bright.\\n\\n(Verse 5)\\nDevelopers rejoice, a tool so grand,\\nBuilding applications, hand in hand,\\nWith ease and grace, they craft and create,\\nLangChain's power, truly first-rate.\\n\\n(Verse 6)\\nSo let us sing of LangChain's might,\\nA beacon of progress, shining ever so bright,\\nIn the annals of AI, its legend will reside,\\nLangChain, the chain that sets our minds wide. \\n\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}]}, id='run-cff9a368-2e9a-4e02-969b-6ba6358970cb-0', usage_metadata={'input_tokens': 8, 'output_tokens': 285, 'total_tokens': 293})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# Load environment variables from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Access the API key from the environment\n",
    "api_key = os.getenv(\"GOOGLE_GEN_API\")\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\", api_key=api_key)\n",
    "llm.invoke(\"Write me a ballad about LangChain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"```python\\nimport cv2\\n\\n# Load an image\\nimage = cv2.imread('image.jpg')\\n\\n# Convert to grayscale\\ngray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\\n\\n# Apply Gaussian blur\\nblur = cv2.GaussianBlur(gray, (5, 5), 0)\\n\\n# Detect edges using Canny\\nedges = cv2.Canny(blur, 50, 150)\\n\\n# Find contours\\ncontours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\\n\\n# Draw contours on the original image\\ncv2.drawContours(image, contours, -1, (0, 255, 0), 2)\\n\\n# Display the processed image\\ncv2.imshow('Image Processing', image)\\ncv2.waitKey(0)\\ncv2.destroyAllWindows()\\n```\\n\\n**Explanation:**\\n\\n1. **Import cv2:** Imports the OpenCV library.\\n2. **Load an image:** Loads an image named 'image.jpg' using `cv2.imread()`.\\n3. **Convert to grayscale:** Converts the image to grayscale using `cv2.cvtColor()`.\\n4. **Apply Gaussian blur:** Applies Gaussian blur to reduce noise using `cv2.GaussianBlur()`.\\n5. **Detect edges using Canny:** Detects edges in the blurred image using the Canny edge detection algorithm (`cv2.Canny()`).\\n6. **Find contours:** Finds contours (outlines of objects) in the edge-detected image using `cv2.findContours()`.\\n7. **Draw contours:** Draws the detected contours on the original image using `cv2.drawContours()`.\\n8. **Display the image:** Displays the processed image using `cv2.imshow()`.\\n9. **Wait for key press:** Waits for a key press using `cv2.waitKey(0)`.\\n10. **Close windows:** Closes all OpenCV windows using `cv2.destroyAllWindows()`.\\n\\n**To use this code:**\\n\\n1. Make sure you have OpenCV installed (`pip install opencv-python`).\\n2. Replace 'image.jpg' with the path to your image file.\\n3. Run the code.\\n\\n**This code performs the following image processing operations:**\\n\\n- **Grayscale conversion:** Converts the image to grayscale.\\n- **Gaussian blur:** Smooths the image and reduces noise.\\n- **Edge detection:** Detects edges in the image.\\n- **Contour detection:** Finds the outlines of objects in the image.\\n- **Contour drawing:** Draws the detected contours on the image.\\n\\nYou can modify this code to perform other image processing tasks by using different OpenCV functions.\\n\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}]}, id='run-c996fd85-0918-4853-a4f4-4a1c597a13a9-0', usage_metadata={'input_tokens': 8, 'output_tokens': 587, 'total_tokens': 595})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"Give me some code for image processing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full result:\n",
      "content='81 divided by 9 is **9**. \\n' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}]} id='run-1d35ae0e-cdad-40bc-be6a-bff87ada2f6d-0' usage_metadata={'input_tokens': 11, 'output_tokens': 10, 'total_tokens': 21}\n",
      "Content only:\n",
      "81 divided by 9 is **9**. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Invoke the model with a message\n",
    "result = llm.invoke(\"What is 81 divided by 9?\")\n",
    "print(\"Full result:\")\n",
    "print(result)\n",
    "print(\"Content only:\")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Create a ChatOpenAI model\n",
    "model = ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    "# SystemMessage:\n",
    "#   Message for priming AI behavior, usually passed in as the first of a sequenc of input messages.\n",
    "# HumanMessagse:\n",
    "#   Message from a human to the AI model.\n",
    "messages = [\n",
    "    SystemMessage(content=\"Solve the following math problems\"),\n",
    "    HumanMessage(content=\"What is 81 divided by 9?\"),\n",
    "]\n",
    "\n",
    "# Invoke the model with messages\n",
    "result = model.invoke(messages)\n",
    "print(f\"Answer from AI: {result.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer from AI: 10 times 5 is 50. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# AIMessage:\n",
    "#   Message from an AI.\n",
    "messages = [\n",
    "    SystemMessage(content=\"Solve the following math problems\"),\n",
    "    HumanMessage(content=\"What is 81 divided by 9?\"),\n",
    "    AIMessage(content=\"81 divided by 9 is 9\"),\n",
    "    HumanMessage(content=\"What is 10 times 5?\"),\n",
    "]\n",
    "\n",
    "# Invoke the model with messages\n",
    "result = llm.invoke(messages)\n",
    "print(f\"Answer from AI: {result.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "You: give a demo code on langchain as a application\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised InternalServerError: 500 An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: ```python\n",
      "from langchain.llms import OpenAI\n",
      "from langchain.prompts import PromptTemplate\n",
      "from langchain.chains import LLMChain\n",
      "\n",
      "# **1. Setup:**\n",
      "\n",
      "# Replace with your actual OpenAI API key\n",
      "import os\n",
      "os.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\" \n",
      "\n",
      "# **2. Core Components:**\n",
      "\n",
      "# Choose an LLM (Large Language Model)\n",
      "llm = OpenAI(temperature=0.7)  # Using OpenAI's GPT model\n",
      "\n",
      "# Define a prompt template with placeholders\n",
      "template = \"\"\"\n",
      "You are a helpful assistant that helps users write engaging product descriptions.\n",
      "\n",
      "Product Name: {product_name}\n",
      "Key Features: {key_features}\n",
      "\n",
      "Write a compelling product description that highlights the benefits and entices customers to purchase:\n",
      "\"\"\"\n",
      "prompt = PromptTemplate(\n",
      "    input_variables=[\"product_name\", \"key_features\"],\n",
      "    template=template,\n",
      ")\n",
      "\n",
      "# Create an LLM Chain to manage the interaction\n",
      "chain = LLMChain(llm=llm, prompt=prompt)\n",
      "\n",
      "# **3. Application Logic:**\n",
      "\n",
      "# Get user input\n",
      "product_name = input(\"Enter the product name: \")\n",
      "key_features = input(\"Enter the key features (comma-separated): \")\n",
      "\n",
      "# Pass user input to the chain for processing\n",
      "response = chain.run({\"product_name\": product_name, \"key_features\": key_features})\n",
      "\n",
      "# Display the generated product description\n",
      "print(\"\\nGenerated Product Description:\\n\", response.strip())\n",
      "```\n",
      "\n",
      "**Explanation:**\n",
      "\n",
      "1. **Setup:**\n",
      "   - Import necessary libraries: `langchain`, `os` (for API key).\n",
      "   - Set your OpenAI API key as an environment variable.\n",
      "\n",
      "2. **Core Components:**\n",
      "   - **LLM:** Choose a language model (here, OpenAI's GPT).\n",
      "   - **Prompt Template:** Create a structured template with placeholders (`{product_name}`, `{key_features}`) to guide the LLM.\n",
      "   - **LLM Chain:** Combine the LLM and the prompt template into a chain for easier execution.\n",
      "\n",
      "3. **Application Logic:**\n",
      "   - Get product name and key features from the user.\n",
      "   - Pass the user input as a dictionary to the `chain.run()` method.\n",
      "   - The chain formats the input using the prompt template, sends it to the LLM, and receives the generated response.\n",
      "   - Display the generated product description to the user.\n",
      "\n",
      "**How to Run:**\n",
      "\n",
      "1. **Install Langchain:** \n",
      "   ```bash\n",
      "   pip install langchain\n",
      "   ```\n",
      "2. **Save the code:** Save the code as a Python file (e.g., `product_description_app.py`).\n",
      "3. **Run from your terminal:**\n",
      "   ```bash\n",
      "   python product_description_app.py\n",
      "   ```\n",
      "\n",
      "Now you can interact with the application, providing product details and getting creative product descriptions generated using the power of Langchain and OpenAI! \n",
      "\n",
      "\n",
      "\n",
      "You: exit\n",
      "---- Message History ----\n",
      "[SystemMessage(content='You are a helpful ai assistant', additional_kwargs={}, response_metadata={}), HumanMessage(content='give a demo code on langchain as a application', additional_kwargs={}, response_metadata={}), AIMessage(content='```python\\nfrom langchain.llms import OpenAI\\nfrom langchain.prompts import PromptTemplate\\nfrom langchain.chains import LLMChain\\n\\n# **1. Setup:**\\n\\n# Replace with your actual OpenAI API key\\nimport os\\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\" \\n\\n# **2. Core Components:**\\n\\n# Choose an LLM (Large Language Model)\\nllm = OpenAI(temperature=0.7)  # Using OpenAI\\'s GPT model\\n\\n# Define a prompt template with placeholders\\ntemplate = \"\"\"\\nYou are a helpful assistant that helps users write engaging product descriptions.\\n\\nProduct Name: {product_name}\\nKey Features: {key_features}\\n\\nWrite a compelling product description that highlights the benefits and entices customers to purchase:\\n\"\"\"\\nprompt = PromptTemplate(\\n    input_variables=[\"product_name\", \"key_features\"],\\n    template=template,\\n)\\n\\n# Create an LLM Chain to manage the interaction\\nchain = LLMChain(llm=llm, prompt=prompt)\\n\\n# **3. Application Logic:**\\n\\n# Get user input\\nproduct_name = input(\"Enter the product name: \")\\nkey_features = input(\"Enter the key features (comma-separated): \")\\n\\n# Pass user input to the chain for processing\\nresponse = chain.run({\"product_name\": product_name, \"key_features\": key_features})\\n\\n# Display the generated product description\\nprint(\"\\\\nGenerated Product Description:\\\\n\", response.strip())\\n```\\n\\n**Explanation:**\\n\\n1. **Setup:**\\n   - Import necessary libraries: `langchain`, `os` (for API key).\\n   - Set your OpenAI API key as an environment variable.\\n\\n2. **Core Components:**\\n   - **LLM:** Choose a language model (here, OpenAI\\'s GPT).\\n   - **Prompt Template:** Create a structured template with placeholders (`{product_name}`, `{key_features}`) to guide the LLM.\\n   - **LLM Chain:** Combine the LLM and the prompt template into a chain for easier execution.\\n\\n3. **Application Logic:**\\n   - Get product name and key features from the user.\\n   - Pass the user input as a dictionary to the `chain.run()` method.\\n   - The chain formats the input using the prompt template, sends it to the LLM, and receives the generated response.\\n   - Display the generated product description to the user.\\n\\n**How to Run:**\\n\\n1. **Install Langchain:** \\n   ```bash\\n   pip install langchain\\n   ```\\n2. **Save the code:** Save the code as a Python file (e.g., `product_description_app.py`).\\n3. **Run from your terminal:**\\n   ```bash\\n   python product_description_app.py\\n   ```\\n\\nNow you can interact with the application, providing product details and getting creative product descriptions generated using the power of Langchain and OpenAI! \\n', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "chat_history = []\n",
    "\n",
    "system_message = SystemMessage(content=\"You are a helpful ai assistant\")\n",
    "chat_history.append(system_message)\n",
    "\n",
    "# Chat loop\n",
    "while True:\n",
    "    query = input(\"You: \")\n",
    "    print(f\"\\n\\nYou: {query}\")\n",
    "    if query.lower() == \"exit\":\n",
    "        break\n",
    "    chat_history.append(HumanMessage(content=query))  # Add user message\n",
    "\n",
    "    # Get AI response using history\n",
    "    result = llm.invoke(chat_history)\n",
    "    response = result.content\n",
    "    chat_history.append(AIMessage(content=response))  # Add AI message\n",
    "\n",
    "    print(f\"AI: {response}\")\n",
    "\n",
    "\n",
    "print(\"---- Message History ----\")\n",
    "print(chat_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
