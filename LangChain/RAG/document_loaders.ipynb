{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Document Loaders are essential components used to load data from various sources into a format that can be processed by language models. They handle documents in diverse formats like PDFs, text files, web pages, databases, and more. Once the documents are loaded, they are typically broken down into smaller chunks and processed by the language models, enabling tasks such as document search, question answering, or summarization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://python.langchain.com/docs/integrations/document_loaders/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'data/README.md'}, page_content='# Generative-AI\\n\\n```python\\nprint(\"Hello world\")\\n\\n```\\n\\n\\nUse document loaders to load data from a source as `Document`\\'s. A `Document` is a piece of text and associated metadata. For example, there are document loaders for loading a simple `.txt` file, for loading the text contents of any web page, or even for loading a transcript of a YouTube video.\\n\\nDocument loaders provide a \"load\" method for loading data as documents from a configured source. They optionally implement a \"lazy load\" as well for lazily loading data into memory.\\n')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "file_path = \"data/README.md\"\n",
    "loader = TextLoader(file_path)\n",
    "\n",
    "document = loader.load()\n",
    "\n",
    "document\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "API Link : https://python.langchain.com/api_reference/community/document_loaders.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'data/sample.txt'}, page_content=\"LangChain is a popular framework designed to facilitate the development of applications that leverage language models (LLMs). It provides a streamlined way to integrate LLMs with various data sources, APIs, and external tools, enabling complex workflows and making it easier to build advanced AI applications.\\n\\nHereâ€™s an overview of its key features:\\n\\n### 1. **Chains**\\n   - Chains allow you to link together multiple components, such as prompts and models, to create more sophisticated workflows.\\n   - You can build sequential chains (step-by-step tasks) or custom chains depending on the application's needs.\\n\\n### 2. **Agents**\\n   - Agents in LangChain are specialized modules that make decisions about which actions to take. They can autonomously call different tools or APIs based on user input.\\n   - An agent could, for example, process a query by interacting with a search engine, perform calculations, or use APIs to retrieve specific data.\\n\\n### 3. **Memory**\\n   - LangChain supports long-term memory, allowing applications to keep track of conversation history or user preferences across multiple interactions.\\n   - This is especially useful for chatbots, where context needs to be maintained over a conversation.\\n\\n### 4. **Retrieval-Augmented Generation (RAG)**\\n   - LangChain helps implement RAG, a process where language models retrieve external knowledge or documents to produce more accurate or contextual responses.\\n   - This is beneficial when creating models that need access to up-to-date information or niche domain knowledge.\\n\\n### 5. **Prompt Templates**\\n   - Prompt templates allow developers to create dynamic and reusable prompts that can be filled with different inputs. This simplifies the process of managing complex prompts across various tasks.\\n   - LangChain offers utilities for chaining these prompts with other components or integrating them into larger workflows.\\n\\n### 6. **Tools Integration**\\n   - LangChain easily integrates with external tools and APIs, enabling the LLM to extend its capabilities, such as performing searches, accessing databases, or interacting with APIs.\\n   - You can also build custom tools for specific use cases.\\n\\n### 7. **Streaming Output**\\n   - LangChain supports streaming responses, useful for tasks where immediate feedback or gradual responses are required, such as when interacting with large datasets or making real-time decisions.\\n\\n### 8. **Ecosystem Support**\\n   - It has built-in integrations with various LLM providers like OpenAI, Hugging Face, Cohere, and others. It also supports vector databases like Pinecone, Weaviate, and FAISS for retrieving relevant information during query processing.\\n\\n### Use Cases\\n   - **Chatbots and Conversational Agents**: Memory, agents, and chains allow for context-aware chatbots.\\n   - **Document Q&A Systems**: Using RAG, LangChain can retrieve relevant documents for answering specific questions.\\n   - **Data-Driven Applications**: Integration with APIs, databases, and retrieval mechanisms enables building apps that provide real-time data-backed responses.\\n\\nLangChain has a modular architecture, making it flexible and customizable depending on the application. Itâ€™s highly suited for building AI-driven systems that need to integrate LLMs with structured data sources, workflows, or external tools.\\n\\n\")]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = TextLoader(\"data/sample.txt\")\n",
    "\n",
    "document = loader.load()\n",
    "\n",
    "document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"LangChain is a popular framework designed to facilitate the development of applications that leverage language models (LLMs). It provides a streamlined way to integrate LLMs with various data sources, APIs, and external tools, enabling complex workflows and making it easier to build advanced AI applications.\\n\\nHereâ€™s an overview of its key features:\\n\\n### 1. **Chains**\\n   - Chains allow you to link together multiple components, such as prompts and models, to create more sophisticated workflows.\\n   - You can build sequential chains (step-by-step tasks) or custom chains depending on the application's needs.\\n\\n### 2. **Agents**\\n   - Agents in LangChain are specialized modules that make decisions about which actions to take. They can autonomously call different tools or APIs based on user input.\\n   - An agent could, for example, process a query by interacting with a search engine, perform calculations, or use APIs to retrieve specific data.\\n\\n### 3. **Memory**\\n   - LangChain supports long-term memory, allowing applications to keep track of conversation history or user preferences across multiple interactions.\\n   - This is especially useful for chatbots, where context needs to be maintained over a conversation.\\n\\n### 4. **Retrieval-Augmented Generation (RAG)**\\n   - LangChain helps implement RAG, a process where language models retrieve external knowledge or documents to produce more accurate or contextual responses.\\n   - This is beneficial when creating models that need access to up-to-date information or niche domain knowledge.\\n\\n### 5. **Prompt Templates**\\n   - Prompt templates allow developers to create dynamic and reusable prompts that can be filled with different inputs. This simplifies the process of managing complex prompts across various tasks.\\n   - LangChain offers utilities for chaining these prompts with other components or integrating them into larger workflows.\\n\\n### 6. **Tools Integration**\\n   - LangChain easily integrates with external tools and APIs, enabling the LLM to extend its capabilities, such as performing searches, accessing databases, or interacting with APIs.\\n   - You can also build custom tools for specific use cases.\\n\\n### 7. **Streaming Output**\\n   - LangChain supports streaming responses, useful for tasks where immediate feedback or gradual responses are required, such as when interacting with large datasets or making real-time decisions.\\n\\n### 8. **Ecosystem Support**\\n   - It has built-in integrations with various LLM providers like OpenAI, Hugging Face, Cohere, and others. It also supports vector databases like Pinecone, Weaviate, and FAISS for retrieving relevant information during query processing.\\n\\n### Use Cases\\n   - **Chatbots and Conversational Agents**: Memory, agents, and chains allow for context-aware chatbots.\\n   - **Document Q&A Systems**: Using RAG, LangChain can retrieve relevant documents for answering specific questions.\\n   - **Data-Driven Applications**: Integration with APIs, databases, and retrieval mechanisms enables building apps that provide real-time data-backed responses.\\n\\nLangChain has a modular architecture, making it flexible and customizable depending on the application. Itâ€™s highly suited for building AI-driven systems that need to integrate LLMs with structured data sources, workflows, or external tools.\\n\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'data/sample.txt'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PDF Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='      \n",
      " Department of Physics    Introduction to Programming in Python         A Self-Study Course (Version 2.2 – October 2009)   ' metadata={'source': 'data/python book.pdf', 'page': 0}\n",
      "\n",
      "\n",
      "\n",
      "{'source': 'data/python book.pdf', 'page': 0}\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# %pip install --upgrade --quiet pypdf\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"data/python book.pdf\")\n",
    "docs = loader.load()\n",
    "print(docs[0])\n",
    "print(\"\\n\\n\")\n",
    "print(docs[0].metadata)\n",
    "print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'data/python book.pdf', 'page': 0}, page_content='      \\n Department of Physics    Introduction to Programming in Python         A Self-Study Course (Version 2.2 – October 2009)   '), Document(metadata={'source': 'data/python book.pdf', 'page': 1}, page_content='Page 2 of 63 Table of Contents 1 Chapter 1 - Introduction.....................................................................................................6 2 Chapter 2 - Resources Required for the Course.................................................................8 2.1 Programming Language.............................................................................................8 2.2 Computer Operating Systems....................................................................................8 2.3 Additional Libraries (Modules).................................................................................8 2.4 Editors........................................................................................................................9 2.5 Where to do the Work................................................................................................9 2.6 Books.......................................................................................................................10 3 Chapter 3 - Getting Started..............................................................................................11 3.1 Numbers...................................................................................................................11 3.2 Assignments, Strings and Types..............................................................................12 3.2.1 A First Mention of Functions...........................................................................14 3.2.2 A Brief Mention of Methods............................................................................14 3.3 Complex numbers (Advanced Topic).......................................................................15 3.4 Errors and Exceptions..............................................................................................15 3.5 Precision and Overflow............................................................................................16 3.5.1 Precision...........................................................................................................16 3.5.2 Overflow – Large Numbers.............................................................................17 3.6 Getting Help.............................................................................................................17 4 Chapter 4 - Input and Output (IO)...................................................................................18 4.1 Screen Input/Output.................................................................................................18 4.1.1 Output..............................................................................................................18 4.1.2 The Format Conversion Specifier....................................................................18 4.1.3 Special Characters in Output...........................................................................19 4.1.4 Input.................................................................................................................19 4.2 File Input  and Output..............................................................................................20 4.2.1 Saving an Array to File....................................................................................20 4.2.2 Loading an Array from File.............................................................................21 5 Chapter 5 - Programs (Scripts)........................................................................................22 5.1 My First Program – ‘Hello world’...........................................................................22 5.2 Exercises..................................................................................................................22 5.2.1 Exercise 5.1......................................................................................................22 5.2.2 Exercise 5.2......................................................................................................23 '), Document(metadata={'source': 'data/python book.pdf', 'page': 2}, page_content='Page 3 of 63 6 Chapter 6 - Sequences, Lists and Strings.........................................................................24 6.1 Lists..........................................................................................................................24 6.1.1 Slicing Lists.....................................................................................................25 6.1.2 2-d Lists...........................................................................................................26 6.1.3 Basic List Operations.......................................................................................26 6.1.4 Fancy  List Handling – zip() and map() – Advanced Topic.............................27 6.1.5 Tuples...............................................................................................................28 6.2 Strings......................................................................................................................28 6.3 Numpy arrays – An Introduction.............................................................................29 6.3.1 Using NumPy...................................................................................................30 6.3.2 Addressing and Slicing Arrays........................................................................30 6.4 Dictionaries – Advanced Topic................................................................................31 6.5 Exercises..................................................................................................................32 6.5.1 Exercise 6.1 - Lists...........................................................................................32 6.5.2 Exercise 6.3 – Arrays.......................................................................................32 6.5.3 Exercise 6.2 – Dictionaries - Advanced Topic.................................................33 7 Chapter 7 - Conditionals and Loops................................................................................34 7.1 Conditionals.............................................................................................................34 7.2 Loops........................................................................................................................35 7.2.1 Loops - The ‘while’ loop.................................................................................36 7.2.2 Loops – The ‘for’ loop.....................................................................................36 7.2.3 Getting out of Infinite Loops - break...............................................................37 7.3 Exercises..................................................................................................................37 7.3.1 Exercise 7.1......................................................................................................37 7.3.2 Exercise 7.2......................................................................................................37 7.3.3 Exercise 7.3......................................................................................................38 8 Chapter 8 - Functions and Modules.................................................................................39 8.1 A First Function.......................................................................................................39 8.1.1 Default values for parameters in a function.....................................................40 8.1.2 Documentation Strings.....................................................................................40 8.2 Returning more than one value................................................................................41 8.3 Modules and import.................................................................................................41 8.3.1 Import...............................................................................................................41 8.3.2 from <module> import <function>..................................................................42 8.3.3 import <module> as <name>...........................................................................42 '), Document(metadata={'source': 'data/python book.pdf', 'page': 3}, page_content='Page 4 of 63 8.3.4 Allowing Python to find your Modules...........................................................43 8.3.5 What’s in a Module?........................................................................................43 8.3.6 Testing Functions and Modules.......................................................................44 8.3.7 pyc Files...........................................................................................................45 8.4 Exercises..................................................................................................................45 8.4.1 Exercise 8.1......................................................................................................45 8.4.2 Exercise 8.2......................................................................................................45 8.4.3 Exercise 8.3......................................................................................................45 8.4.4 Exercise 8.4......................................................................................................46 8.4.5 Exercise 8.5......................................................................................................46 9 Chapter 9 - Debugging and Exceptions...........................................................................47 9.1 Using print for debugging...................................................................................47 9.2 Use the Command Line...........................................................................................47 9.3 Module Test Code....................................................................................................47 9.4 Handling Exceptions: try / except – Advanced Topic..............................................47 9.4.1 Catching ALL Exceptions................................................................................47 9.4.2 Catching Specific Exceptions..........................................................................48 9.5 Exercises..................................................................................................................49 9.5.1 Exercise 9.1 – Advanced Topic........................................................................49 10 Chapter 10 - Maths Modules: NumPy.........................................................................51 10.1 The math Module.....................................................................................................51 10.2 The NumPy Module.................................................................................................51 10.2.1 Creating Arrays and Some Examples of Basic Manipulation.........................51 10.2.2 Linear Algebra.................................................................................................54 10.3 The SciPy Module – Advanced Topic......................................................................55 10.4 Exercises..................................................................................................................55 10.4.1 Exercise 10.1....................................................................................................55 10.4.2 Exercise 10.2....................................................................................................55 11 Chapter 11 - File Input and Output – The Details.......................................................57 11.1 Line Terminators – The \\\\n character........................................................................57 11.2 Writing to File..........................................................................................................57 11.3 Reading from File....................................................................................................58 11.4 Exercises..................................................................................................................59 11.4.1 Exercise 11.1....................................................................................................59 11.4.2 Exercise 11.2....................................................................................................59 '), Document(metadata={'source': 'data/python book.pdf', 'page': 4}, page_content='Page 5 of 63 11.4.3 Exercise 11.3....................................................................................................59 11.4.4 Exercise 11.4....................................................................................................59 12 Chapter 12 - Plotting Graphs.......................................................................................60 12.1 PyLab: The absolute Basics.....................................................................................60 12.2 GUIs – How do they work?.....................................................................................61 12.3 Exercises..................................................................................................................62 12.3.1 Exercise 12.1....................................................................................................62 13 Chapter 13 - Random Numbers...................................................................................63 13.1 Exercises..................................................................................................................63 13.1.1 Exercise 13.1....................................................................................................63   '), Document(metadata={'source': 'data/python book.pdf', 'page': 5}, page_content='Page 6 of 63 1 Chapter 1 - Introduction The aim of this course is to introduce you to the development of computer programs and to provide you with a working knowledge of the Python language.  In addition to everyday usage, physicists use computers for: • Controlling apparatus and taking readings automatically – EG using LabView • Processing data to extract conclusions • Predicting the results of experiments • Simulations of systems that are too complex to describe analytically  All but the first of these generally require using a computer programming language. The latter two can be described as ‘Computational Physics’. There are computer ‘applications’ that allow you to address some of these problems – you will already have used Excel extensively. However, to obtain the flexibility to solve a particular problem the way YOU want to do it, you must write your own dedicated ‘application’ using a computer programming language. Thus skills in programming are extremely important to the modern physicist. They will be essential to you in later stages of your degree. This course provides you with an introduction to programming that should enable you to develop the more extensive skills required in this and later years. Such skills are also much sought after by a wide variety of employers. There are many programming languages and much argument over which is ‘best’. The skills that you acquire in this course should be easily transferable from the Python language that you will use to other languages. When preparing a course such as this, there is always a decision to be made over which programming language to use: in this case we have chosen Python, a relatively new (dating from 1990) but widely used language. Although not originally designed for scientiﬁc use, it is a powerful modern language, easy to learn and ideal for the rapid development of programs. Python is widely used. For example, the installation scripts for many Linux distributions are written in Python, as is the BitTorrent software. Google uses Python extensively, as do AstraZeneca, the pharmaceutical company, and Industrial Light and Magic, the company behind the special effects for Star Wars and many subsequent movies.  Python is interpreted, which means that the computer runs another program to read your input and then execute your instructions. Programs written in languages like FORTRAN or C++ are compiled, meaning that the computer translates them into a more basic machine-readable form which can be executed much faster. The speed disadvantage of interpreting will not be an issue in this course. In fact, not having to compile and link your code before running it will save you considerable time. Nevertheless, you should be aware of this fundamental difference between interpreted and compiled languages. Python’s high-level language features also mean that programs will typically be much shorter than equivalent ones written in C or C++. Python can be linked to compiled code from other languages, which can largely get over the speed penalty. This is done, for example, in the NumPy and SciPy modules which we’ll encounter during this course. They provide fast vector and matrix handling, as well as Python interfaces to fast routines for tasks like integration, solving differential equations and linear algebra. If you become a Python expert, you’ll be able to make links to your own compiled code.  '), Document(metadata={'source': 'data/python book.pdf', 'page': 6}, page_content='Page 7 of 63 Whilst one can use Python, rather like a powerful programmable editor, from the command line, you will soon want to write many lines of program and save them to a file. You then tell Python to execute the file.  For this you need an editor. You can use any ordinary text editor (NOT a work processor!!). There is a very good editor bundled with Python called IDLE which helps you with your syntax. We recommend that you use this editor. One advantage of Python is that, although it is a complex and rich language, it is easy to learn the basics. Once you have a grounding in Python, you can extend your knowledge later. Some topics in this guide are marked thus: Advanced Topic. You can skip over these and come back to them later if you have time.  In this course you will be learning ‘procedural programming’. That is writing a set of functions to do what you want and arranging them in files called ‘modules’. There is an alternative technique called ‘Object Oriented Programming’ that you will hear mentioned and can read about in Hetland. That is an Advanced Topic and is not covered in this course. When you are writing programs to solve Physics problems, try to keep clear the distinction between your algorithm or method of solution and the overall program. You can then easily change the algorithm function without affecting the way your main code works. All programming languages implement the same basic constructs, loops, decision-making and so on. Once you have seen them in one language (Python) it will be much quicker to pick up a different language later if you need to.  If you are keen to learn a compiled language, either now or later in your degree, the department maintains similar courses in both Fortran and C++. These can be made available to you on request. Note 1: Throughout this document, we have used the Courier font, which looks like this: This is in the Courier font, to represent Python code or input and output from the computer. Note 2: These course notes, and the associated exercises, attempt to be both a tutorial and a reference document. DO skip the advanced topics on your first run through. DO look ahead to the later chapters when you need more detail on a subject. '), Document(metadata={'source': 'data/python book.pdf', 'page': 7}, page_content='Page 8 of 63  2 Chapter 2 - Resources Required for the Course 2.1 Programming Language Python!!  This is available on the ITS Linux service. Type python at the Linux prompt. Python is also available for other operating systems (See below). 2.2 Computer Operating Systems You will already be familiar with the Microsoft Windows operating system. It is widely used but there are others! Physicists very often use the Unix operating system. This comes in several forms. One of the most popular is Linux. As well as learning to use the Python language, you are expected to develop your programs under the Linux system. Knowledge of this system is required for your work in this and later years. The Linux system is provided by the ITS. You can access it from the networked PCs in the computer classrooms or from your own computer. A good introduction to Linux and to using the ITS Linux service is provided on the ITS web pages at: http://www.dur.ac.uk/its/linux/ You are recommended to use the Vega time-saharing service provided by ITS. There is also a very useful guide to Linux provided by ITS at: http://www.dur.ac.uk/resources/its/info/guides/169linux.pdf Although the ITS Linux service provides a window manager (rather like MS Windows) called GNOME, it is common under Linux to work much of the time using a terminal window with a command line prompt. This is something you should get used to doing. 2.3 Additional Libraries (Modules) You are learning to program in order to use computers to do, amongst other things, Computational Physics. Python does not provide sufficient facilities for this purpose built into the language. However, adding ‘modules’ to Python is easy, as you will find out when you add your own! There is a large number of modules (or libraries) already available for Python. We will use just 3 of these: • NumPy and SciPy – These provide fast vector and matrix handling, as well as Python interfaces to fast routines for tasks like integration, solving differential equations and linear algebra.  • Matplotlib or Pylab – This is a standard plotting package for drawing graphs. (Import pylab – it will import the lower level matplotlib for you). These modules have been installed for you on the ITS Linux service and are available within the Enthought Python distribution (See below). There is good documentation for NumPy and SciPy at: http://www.scipy.org/. Try the tutorial under ‘Getting Started’. Importing these or any other module into Python is easy: '), Document(metadata={'source': 'data/python book.pdf', 'page': 8}, page_content='Page 9 of 63 EG  import numpy  # Import the standard module numpy Or import mycleverlib  # Import my own module  Not really too hard! We will return to the use of modules later in the course. (Note: All the text after a # on each line is treated as a comment and is not executed). 2.4 Editors  You can do a lot with Python just from the command prompt within a terminal window. However, at some point you will want to save loads of Python lines as a ‘program’, sometimes called a ‘script’. For this, you need an editor. You CANNOT use word processors (like Word) for this!! There are many suitable editors. It is best to use one that ‘knows’ about Python and will help you get the syntax right. The standard editor for Python is called IDLE. Wherever you find Python you are very likely to find IDLE. It provides a simple (but rather good) editor for writing your code. IDLE also provides an interactive shell (another window) to run the code in. This can be useful but can cause confusion. It is best to run your programs directly from the command line and NOT in this window. IDLE also comes as part of the Enthought distribution of Python (see below). It can be accessed under Linux by typing idle –n &  at a command prompt. The & makes it run in the background so that you get your Linux prompt back to do other things. The –n MUST be included or you will get an error. (Remember that Linux commands are case sensitive). 2.5 Where to do the Work Python will run happily on most operating systems. You could do all of the course work on a Microsoft (MS) Windows machine or a Mac if you wish. However, you are expected, as a part of the course, to learn the basics of the Linux operating system. You should therefore start the course work on the ITS Linux service via the networked PCs in the classroom where the demonstrator sessions are held. (One of these classrooms is in room 140 in the Physics department and you can use this room any time when it is NOT in use for a class). You can log in to the ITS Linux service from your own PC using a VNC client. A free version of such a client (for the MS Windows operating system only) is available from:  http://www.realvnc.com/products/download.html This is only possible if you connect your computer directly to the Durham University system (EG In colleges). It will NOT work from outside the University. Access can be gained from outside the University using ‘Putty’. Details of how to access the Vega service are given by ITS at: http://www.dur.ac.uk/its/linux/remote_access/ You are encouraged to transfer work to your own PC or laptop in order to continue to work anywhere. Whether your PC runs MS Windows, MacOS or Linux, you can download an excellent version of Python, including all of the additional modules you need for the course, from ‘Enthought’. There are free academic versions at: http://www.enthought.com/products/epddownload.php '), Document(metadata={'source': 'data/python book.pdf', 'page': 9}, page_content='Page 10 of 63 2.6 Books You don’t need a book to learn Python! Just sit at a computer and write programs. Having said that, most programmers keep a good Python book nearby for reference. The first port of call however is the web. The Python documentation is very extensive. See: http://www.python.org/doc/. This is the font of all knowledge for Python. (Be sure to use the documentation for version 2.x). You can download much of this to your computer should you wish. We recommend two books: • Beginning Python by Magnus Lie Hetland. This is an excellent introduction to the language with loads of examples • Python in a Nutshell by Alex Martelli. This is more of a handy reference book If you just want one good book to use, get ‘Beginning Python’. We will refer to it in these notes as ‘Hetland’. The course largely follows the early chapters of Hetland. In fact, chapters 1 to 11 (but not 7 and 9 which cover object oriented techniques) match the course well. However, Hetland does not cover scientific computing using NumPy and SciPy which is included in this course. Note: There is a useful reference for much of the more commonly used aspects of Python in Hetland in appendix B.  '), Document(metadata={'source': 'data/python book.pdf', 'page': 10}, page_content='Page 11 of 63  3 Chapter 3 - Getting Started Before you start the course, you may also like to put Python on your computer and try the Python tutorial online at: http://docs.python.org/tut/tut.html (Try section 3 first). The great thing about Python is how fast you can get started. You may like to try it first in the familiar environment of MS Windows before moving to Linux. As you learn the language, ALWAYS have a Python prompt available. If you learn something new, try it at once! Log on to the ITS Linux service and bring up a terminal (one of the icons on the bar at the bottom of the screen). At the terminal prompt, type python. You will get a new prompt in the form: >>>. The Python interpreter is now running. Note: Much of this chapter is covered in more detail in chapter 1 of Hetland. 3.1 Numbers Before we write any programs, we will start by using Python from the command line. It is rather like using a fancy calculator. Even after you have written a load of code, it is still very useful to quickly try things out at the command line. Try this example: >>> (2+4+6)*3-12/3 32 >>> 8**2 # ** is used for power 2. 64 >>> It works! Note the use of a # to introduce a comment. The interpreter ignores all of the rest of the line after the #. These were integers. Let’s try it with real numbers: >>> (2.0+4.0+6.0)*3.0-12.0/3.0 32.0 >>> 8.0**2 64.0 >>>  This looks fine. But now try: >>> 7.0/4.0 1.75 >>> 7/4 '), Document(metadata={'source': 'data/python book.pdf', 'page': 11}, page_content='Page 12 of 63 1 >>> Not so fine! The lesson is that computers distinguish between exact integer arithmetic and real number (or ﬂoating point) arithmetic. In particular, whenever dividing two integers does not result in an integer, the result is truncated (rounded down) to the nearest integer. If you mix integer and ﬂoating point numbers, they are all converted to ﬂoating point by Python:  >>> 34+1.0 35.0 >>> You can also convert from integer to ﬂoat and back again, but note that conversion to integers is done by truncating  >>> float(1) 1.0 >>> int(3.678) 3 >>> Python will do lots of auto converting for you which is very convenient. But beware – sometimes it will not do exactly what you wanted. It is good practice, if you want a floating point number to always include the decimal point. Thus 12.3+1.0 is better style than 12.3+1.  Advanced Topic: If you would like Python to NOT use integer division but to convert for you and give the answer you might expect, you can tell it to do so. How to do this is explained in Hetland chapter 1: >>> from __future__ import division >>> 7/4 1.75 >>> 7//4  # Force python to do proper integer division 1 >>> 3.2 Assignments, Strings and Types You can assign values to variables. The computer will save the value in some memory pointed to by the variable. If you do this, you won’t see the value printed on the screen unless you ask for it. If you just type the variable name, Python will return the value (or you can use the print command: >>> x=3 >>> x 3 >>> y=2 '), Document(metadata={'source': 'data/python book.pdf', 'page': 12}, page_content='Page 13 of 63 >>> z=7 >>> x*y*z 42 >>> a=x*y*z >>> print a 42 >>> Remember that the = sign is used for assignment in Python. This is NOT the same as its use in maths. Thus x=x+1 makes perfect sense. It means increment the value of the x variable by 1. It is NOT a maths equation!  Variables can be of types other than integer and float. For example, they can be strings of characters; in other words text. A string is a special case of a list object as we will see later. String handling is easy in Python. It is well explained in chapter 3 of Hetland.  A string is always enclosed in either single or double quotes (But not a mixture of both!). Strings can be added (concatenated or joined): >>> \"spam\" \\'spam\\' >>> \"spam,\" + \" \" + \"eggs and spam\" \\'spam, eggs and spam\\' >>> Strings can be duplicated by multiplying them by an integer: >>> \"Spam, \"*4 + \\'beans and spam\\' \\'Spam, Spam, Spam, Spam, beans and spam\\' >>>  Python does not require you to be strict about what type a variable is (the language is ‘weakly typed’). However, this is sometimes important to a Physicist. If you must, you can find the type of a variable thus: >>> a=1; b=1.0; c=\\'1.0\\' >>> type(a) <type \\'int\\'> >>> type(b) <type \\'float\\'> >>> type(c) <type \\'str\\'> >>> print a,b,c 1 1.0 1.0 '), Document(metadata={'source': 'data/python book.pdf', 'page': 13}, page_content=\"Page 14 of 63 >>>  You can convert a string to a number (within reason) and visa versa: >>> str(12.3) '12.3' >>> float('12.3') 12.300000000000001 >>> Note that we didn’t quite get what we expected from the float function. The computer has a limited precision. It stores the nearest number to what you want within its ability. We will come back to this issue of precision. Note that you can put several short statements on the same line separated by semi colons. It is very poor style to do this for more than very simple lines as above. 3.2.1 A First Mention of Functions  The use, and creation, of functions will be handled later in this course. However, we have already mentioned the str() and float() functions above.  Python will provide you with a huge number of functions that are already written. Some of these are ‘built-in’ to the language like those above. Others are provided by modules that you ‘include’. We will discuss functions in detail later on. For now, just note the format of a function call: result = function(arguments) For example, we can use 2**3 to raise 2 to the power 3. This is really just a shorthand for the function pow(): >>> 2**3 8 >>> pow(2,3) 8 >>> Note 1: The use of brackets of type () for function calls. Other shapes of brackets mean different things in Python. Don’t muddle them up! We will come across [] and {} later on. 3.2.2 A Brief Mention of Methods We will make brief mention of ‘methods’ here and there. Methods are a part of Object Oriented Programming which we are not considering in this course. We will use functions throughout the course rather than methods. However, you will need to use some methods that are provided by Python so you must learn the syntax. They can be thought of as an alternative way of calling some already-written code and take the form: Result = object.method(arguments). Notice the . (dot) in the syntax. An example is appending to a list (We will talk more about lists in section 6.1): \"), Document(metadata={'source': 'data/python book.pdf', 'page': 14}, page_content='Page 15 of 63 >>> a=[0,1,2] # Make a list >>> a.append(3) # Use the append() method >>> print a [0, 1, 2, 3] >>> 3.3 Complex numbers (Advanced Topic) This is a type not often needed by an everyday Python user but much beloved of Physicists! Python supports complex numbers. They are written in the form 3+5j, corresponding to      3+5i in usual mathematical notation. Here are some simple examples using complex numbers: >>> d=3+5j >>> type(d) <type \\'complex\\'> >>> d.real 3.0 >>> d.imag 5.0 >>> abs(d) # absolute value or magnitude or modulus 5.8309518948452999 >>> e=1+1j >>> print d*e (-2+8j) >>> Note1: We used d.real to get the real part of a complex number. This is a way of getting an ‘attribute’ of the variable. It is a first hint of what is called ‘object oriented programming’. This is not a part of this course but will be mentioned now and again. For now, just remember the syntax. Note 2: There is no separate type for imaginary numbers in Python. They are treated as complex numbers whose real component is zero. 3.4 Errors and Exceptions By now, you will have made some mistakes and will have seen your first Python ‘Traceback’. Be aware that errors are referred to as ‘exceptions’ in Python. For example: >>> float(\\'gumby\\') Traceback (most recent call last):   File \"<pyshell#90>\", line 1, in <module>     float(\\'gumby\\') '), Document(metadata={'source': 'data/python book.pdf', 'page': 15}, page_content='Page 16 of 63 ValueError: invalid literal for float(): gumby >>> When an error occurs in a program, it may occur in a function that is called from another function in another module etc. The traceback gives very valuable information about the error that occurred AND where it occurred (eg the line number in a file). In this case, the error was at the Python prompt (the Python shell). We supplied a string to the function float() which it really can’t convert. You may also get syntax errors. This is where you have written something that Python simply doesn’t understand. For example: >>> str(1.23a) SyntaxError: invalid syntax  Now let’s look at a more detailed traceback: Traceback (most recent call last):   File \"\\\\\\\\elite\\\\home\\\\nad\\\\My Documents\\\\AllDocs\\\\teaching\\\\Computing\\\\2009\\\\TestPrograms\\\\test01.py\", line 7, in <module>     T.theTest(theData)   File \"\\\\\\\\elite\\\\home\\\\nad\\\\My Documents\\\\AllDocs\\\\teaching\\\\Computing\\\\2009\\\\TestPrograms\\\\testModule.py\", line 4, in theTest     y = x.fred() # Cause an error AttributeError: \\'numpy.ndarray\\' object has no attribute \\'fred\\' The traceback starts with what was happening at the top level. An error occurred in my main program (called <module>) at line 7. I am told the name of the file and the line number. However, there is more… The actual error was in another module file (name given) that contains the function called ‘theTest()’ that I called from my main program. The line that caused the error is identified and the cause of the error is given. The function ‘fred()’ does not exist (No such attribute). The error descriptions become clearer as you get to know the terminology. These tracebacks provide you with the main information you need to ‘debug’ your program. IE Find out what went wrong. Debugging is discussed further in chapter 9.  3.5 Precision and Overflow As physicists, we are very often using floating point numbers (real numbers) to do calculations. There are some pitfalls with using floats of which you should be aware. 3.5.1 Precision Computers have a limited dynamic range for storing numbers and also, for real numbers, a limited precision. This can be a significant problem in computational physics where the ‘rounding errors’ in real numbers may add up to a large error in your result. As a trivial example: '), Document(metadata={'source': 'data/python book.pdf', 'page': 16}, page_content='Page 17 of 63 >>> 1.0/3.0 0.33333333333333331 What is that 1 doing on the end? Python has done its best but cannot give the answer to an infinite number of decimal places. The precision of Python floating point numbers is about 1 part in 1016. 3.5.2 Overflow – Large Numbers Python can store really large numbers. At a certain point it will automatically start using long integers. These require more storage. Try:  >>> 1000000*1000000 1000000000000L >>>  The L on the end shows that Python has used a long integer but you don’t really need to know. For real numbers, Python can store enormous numbers. It will eventually give an error when the exponent is just too large: >>> 1e200**2 Traceback (most recent call last):   File \"<pyshell#119>\", line 1, in <module>     1e200**2 OverflowError: (34, \\'Result too large\\') >>> Note 1: We use ** to mean ‘raise to the power’ Note 2: We can use 1e200 to represent the number 10200. 3.6 Getting Help There is a LOT of online help for Python. The full documentation set for Python is online at: http://www.python.org/doc/. The IDLE editor will prompt you with popup ‘tips’. These are very useful for remembering for example the parameters required by a function call. There is a built-in help system. Use the function help(). Enter a Python keyword or function name between the brackets. Try just typing help() at the Python prompt to get started. '), Document(metadata={'source': 'data/python book.pdf', 'page': 17}, page_content=\"Page 18 of 63  4 Chapter 4 - Input and Output (IO)  This chapter describes how to get data in and out of Python programs. Section 4.1 describes input and output to/from the screen and contains much of the detail that you need to know about IO, such as the % format specifier. You will also want to save and load data to/from data files. This is described in detail in Chapter 11. However, there is excellent support for simple file IO of numbers within the NumPy module. This technique is described here in section 4.2 so that you can do file IO without reading the gory details of chapter 11. 4.1 Screen Input/Output 4.1.1 Output We have already seen how to get output from the command line. Generally, within a program, we use the print command to output things to the screen. If you don’t tell Python what format to use on output, it will decide for you: >>> a=1.123456789 >>> print a 1.123456789  - but you can tell Python exactly how to write to the screen using a format conversion specifier (%): >>> print 'a= %.2f' % a, ‘cms’ a= 1.12 cms >>> To output more than one thing, we separate them with commas. 4.1.2 The Format Conversion Specifier The % sign in the above example is used to tell Python what format to use to output to the screen. The details of how to use % are in chapter 3 of Hetland. Here we have specified a float format (f) with 2 decimal places (.2). The % in the string tells the print function that a format specifier is coming next. There must be one format specifier for each item you output: >>> a=1.23 >>> b=123.45 >>> print 'a = %5.2f' % a, 'b = %.4e' % b a =  1.23 b = 1.2345e+002 The format specifier %5.2f means: Output the variable as a floating point number with a total field width (all digits plus the decimal point) of 5 and with 2 decimal places.  We can get a similar result like this: >>> a=1.23 >>> b=123.45 >>> print 'a = %5.2f\\\\tb = %.4e' % (a,b) \"), Document(metadata={'source': 'data/python book.pdf', 'page': 18}, page_content='Page 19 of 63 a =  1.23 b = 1.2345e+002 Here the string contains two conversion specifiers and the two variables have been put at the end as the tuple: (a,b). (A tuple is just a list of variables – see section 6.1.5). The \\\\t special character used here is described in section 4.1.3. There are quite a lot of conversion specifier types. The most useful are: • f or F – Floating point (EG 1.234) • d or I – Integer (EG 123) • e or E – Exponential format (EG 1.234e+002) 4.1.3 Special Characters in Output There are some special characters that it is useful to output to the screen. They are represented within a string by a backslash followed by a character. The most useful examples are: • \\\\n – This inserts a ‘carriage return’ • \\\\t – This inserts a tab We used the \\\\t special character in the above example to insert a tab character. Note: If you actually want the backslash character in your output string, you must enter it twice. 4.1.4 Input Sometimes you will want the user of your program to type in some input from the terminal. You can ‘prompt’ him to do so by outputting a string and then wait for his input. To do this, there is an input() function. This can be used to input directly to a Python variable. >>> a=input(\"Input a number: \") Input a number: 123.45 >>> print a 123.45 >>>  This is fine for the input of numbers but not great for the input of strings. The problem is that input() assumes that what you type is valid Python.  >>> b=input(\"Input your name: \") Input your name: Ron Traceback (most recent call last):   File \"<pyshell#2>\", line 1, in <module>     b=input(\"Input your name: \")   File \"<string>\", line 1, in <module> NameError: name \\'Ron\\' is not defined >>> To avoid this error, the user must enter the string with quotes around it thus:  '), Document(metadata={'source': 'data/python book.pdf', 'page': 19}, page_content='Page 20 of 63 >>> b=input(\\'Input your name: \\') Input your name: \"Ron\" >>> print b Ron >>> To avoid this, you can use the function raw_input(). (See Hetland Page 26). Doing this however means that your input is all treated as a string and, if you want another type, you must convert it: >>> a = float(raw_input(\\'Type a number: \\')) Type a number: 23 >>> print a 23.0 >>> Which input function you use is up to you. The raw_input() and input() functions will use the string that you give it as a parameter as a ‘prompt’. It writes this on the screen and then waits for some input terminated by a ‘carriage return’ (The ‘enter’ key). Note: You can apply a function to the result of another function directly as shown above. raw_input() returns a string that is the input to the function float(). 4.2 File Input and Output If you need to write a lot of complicated mixed text and numbers to a file, you need to understand how file IO is done in Python (Chapter 11). However, saving or loading a NumPy array can be done very easily.  This section makes use of quite a few concepts that you have not met yet. Just try this example so that you can see how easy it is to input and output data to file. Before we start, what is a NumPy array? There is an introduction to NumPy in section 6.3. Take a quick look now or just try the example and learn more about NumPy later. 4.2.1 Saving an Array to File The NumPy module provides the savetxt() function to do this. Just get your data into an array of suitable size and shape and this function will write it to file for you. You should only do this with NumPy arrays. If you try to read from a file that is a mixture of types, NumPy will do its best but may give you confusing results. Here is an example. The first line loads (imports) the NumPy module so that you can use it: >>> import numpy as N >>> data = N.zeros((5,2), dtype=\\'float\\') # Make an empty 2-d array >>> data[:,0] = N.arange(5.0) # Fill one column with numbers >>> data[:,1] = data[:,0] ** 2.0 # Fill second column with squares '), Document(metadata={'source': 'data/python book.pdf', 'page': 20}, page_content=\"Page 21 of 63 >>> numpy.savetxt('myFile.txt', data, fmt='%.3f') # Save to file Try all this from the command line. Then look at the text in your file. It should look like this: 0.000 0.000 1.000 1.000 2.000 4.000 3.000 9.000 4.000 16.000 Note that savetxt() has used a blank space between entries in each row. You can change this (if you must) using the delimiter parameter. For example: >>> numpy.savetxt('myFile.txt', data, fmt='%.3f', delimiter=’,’) will use a comma as the delimiter. 4.2.2 Loading an Array from File Loading the data back from your file to a NumPy array is equally simple using loadtxt(). Be sure to tell loadtxt() what the delimiter is (or it will assume a blank space): >>> import numpy as N >>> data=N.loadtxt('temp.txt') >>> print data [[  0.   0.]  [  1.   1.]  [  2.   4.]  [  3.   9.]  [  4.  16.]]  \"), Document(metadata={'source': 'data/python book.pdf', 'page': 21}, page_content='Page 22 of 63  5 Chapter 5 - Programs (Scripts) If you already know how to create a program in a file using an editor and run the program, you can skip this section, but do the exercises anyway. 5.1 My First Program – ‘Hello world’ Before we go any further, its time to write a few ‘scripts’ or programs. Do this with the IDLE editor on the Linux system. Type your program into the editor, save it as a file then run the file The first program that everyone writes in every computer language is the ‘Hello world’ program. This just writes ‘Hello world’ on the screen when it is run. This can take 10 or even 20 lines to do in a language such as C++ or Java. In Python, it is 1 line. So let’s do it… • Log on to the Linux system • Open a terminal (Click on the relevant icon) • At the Linux prompt type: idle –n & • Select   File -> New window   to get the editor window open • Type in: print “Hello world” followed by a carriage return • Select File -> Save as… Give it a name (with .py extension)  and click Save • The file will be saved in the directory from which you ran IDLE  • In the Linux window, type: ls and check that your program file is there • Type: python myprogram.py to run your program (With your own program name) • It should write  Hello world  to the screen. • Congratulations! You wrote a working program. Note 1: All Python program files should have the .py extension. EG myprogram.py In all sections from now on, we provide exercises for you to try. Model programs are provided on DUO for all of these exercises. However, do write your own first then look at the model solution. 5.2 Exercises 5.2.1 Exercise 5.1 Write a program that evaluates the function:  y = x2-1 at a single point. The program should request a value of x from the screen, print out the equivalent value of y and stop. Suggestions: Put a comment at the top of the program. This comment at the top of a program file is special. Enter it as a string (IE In quotes) rather than using a #. You will see why later. '), Document(metadata={'source': 'data/python book.pdf', 'page': 22}, page_content='Page 23 of 63 Make the program work with floating point numbers – much more useful than just integers. Print the result to say 3 decimal places. (Use the % format descriptor). Test it with various numbers. What happens if you don’t do as you are asked and enter a string instead of a valid number? Model solution is in file: Exercise5.1.py on DUO. 5.2.2 Exercise 5.2 Modify your program so that it will evaluate any quadratic: y =ax2 + bx + c You will need to ask for the coefficients from the screen. Model solution is in file: Exercise5.2.py Enter the coefficients used in the previous example: a=1, b=0, c=-1 and check that you get the same answer using both programs. Model solution is in file: Exercise5.2.py on DUO. '), Document(metadata={'source': 'data/python book.pdf', 'page': 23}, page_content=\"Page 24 of 63  6 Chapter 6 - Sequences, Lists and Strings Suppose we want to evaluate the function in exercise 5.1 at more than 1 point and save all of the results? This is just one example of where you might need a list or array. We will soon see that what physicists really want much of the time is arrays that represent vectors and matrices. These are just a special sort of list and we will meet them later. Strictly speaking, Python provides various sorts of ‘sequences’ but the most important sequences within Python are lists and strings. The most important sequences for physicists however are NumPy arrays. These are introduced here. So…Lists and strings are useful but don’t spend too much time on them. All of the physics will be done using NumPy arrays. Note 1:  You can have 2-d lists but, if you are handling numbers, NumPy arrays are much better. Note 2: You will come across a sequence called a ‘tuple’ (section 6.1.5). It is just a list whose elements cannot be changed. 6.1 Lists We will look briefly here at how lists are created and used. For more detail, see chapter 2 of Hetland. You can create a list of objects. The objects don’t have to be all of the same type and could even include other lists. We use square brackets [] for lists: >>> a=[3,54,26,90] >>> print a [3, 54, 26, 90] >>> b=[3,54,'llama',a] # Include the list a in the list b >>> print b [3, 54, 'llama', [3, 54, 26, 90]] # Note the nested brackets >>> The elements of a list are numbered from zero and you get an error if you ask for an element that is out of range. It is a very common error to forget that the elements of a list start at zero (not 1). So the list above called a has 4 elements numbered 0,1,2 and 3. The address of an element in a list is known as its ‘index’. You access one or more elements of a list by giving the index(s) of the element(s) you want: >>> a=[3,54,26,90] >>> b=[3,54,'llama',a] >>> print b [3, 54, 'llama', [3, 54, 26, 90]] >>> a[0] 3 \"), Document(metadata={'source': 'data/python book.pdf', 'page': 24}, page_content='Page 25 of 63 >>> b[3] [3, 54, 26, 90] >>> b[3][2] # Interpret as (b[3])[2] 26 >>> b[4] Traceback (most recent call last):   File \"<pyshell#40>\", line 1, in <module>     b[4] IndexError: list index out of range >>> Note 1: You can access elements of a list from the right hand end using negative indexes. Note 2: You can get the length of a list using the len() function. Note 3: You can change any element of a list. In the list called a above: >>> a[-1] 90 >>> a[-2] 26 >>> len(a) 4 >>> a[2] = \\'gecko\\' >>> print a [3, 54, \\'gecko\\', 90] >>> 6.1.1 Slicing Lists You can get a ‘slice’ of a list. This is a very important technique which is also used for NumPy arrays. You will often want to access just one part of a list or array. To do this, use two indices (start and end) separated by a colon. Note that the ‘end’ index actually refers to the element after the last one you want!! >>> a=[12,23,34,45,56,67,78] >>> print a[3:6] [45, 56, 67] >>> If you miss out one of the indices, Python will assume you want all the rest, from the beginning or up to the end: >>> a[3:] '), Document(metadata={'source': 'data/python book.pdf', 'page': 25}, page_content='Page 26 of 63 [45, 56, 67, 78] >>> a[:3] [12, 23, 34] >>> 6.1.2 2-d Lists – Advanced Topic You can have 2-d lists – it’s just a list of lists! - but, especially when manipulating numbers, it is far better to use NumPy arrays (see section 6.3). Addressing a 2-d list is done thus:  >>> a=[[0,1,2],[3,4,5],[6,7,8]] # Create a 2-d list >>> print a [[0, 1, 2], [3, 4, 5], [6, 7, 8]] >>> print a[1][2] # Addressing a 2-d list 5 6.1.3 Basic List Operations There are lots of operations that can be performed on lists. Here we mention just a few of the most useful: You can add one list to another: >>> a=[1,2,3] >>> b=[4,5,6] >>> c=a+b >>> c [1, 2, 3, 4, 5, 6] >>> You can append a new value to the end. This is one of the most useful features of a list. You cannot append to a NumPy array. >>> a=[1,2,3] >>> a.append(4) # Note the dot!  This is a method. >>> print a [1, 2, 3, 4] >>> You can insert a new value in the middle of a list: >>> a=[1,2,3,4,5,6,7,8,9] >>> a.insert(3,99) >>> print a [1, 2, 3, 99, 4, 5, 6, 7, 8, 9] >>> '), Document(metadata={'source': 'data/python book.pdf', 'page': 26}, page_content='Page 27 of 63 You can look to see if a list contains a particular value. For this we use the membership operator  known as in: >>> a=[1,2,3,4,5,6] >>> 3 in a # Think of it as a question: Is 3 in a? True >>> 7 in a False >>> What you get back is a Boolean or logical value. This can take only the value True or False. The in operator is most useful in loops which we will look at later. 6.1.4 Fancy  List Handling – zip() and map() – Advanced Topic You can do some very clever things with lists. In general however, the ‘array’ type that we introduce later in the course (in module NumPy) is more useful to physicists. Have a look at appendix B of Hetland where he has a summary of built in functions and methods. I will give just two examples here: Example 1. Try the zip() function: >>> a=[1,2,3] >>> b=[4,5,6] >>> c = zip(a,b) Now print c and see what zip() has done to your 2 lists. Example 2. Try the map() function: You may have a list of integers and want to make them all into floats. Python won’t let you do this in case the list contains types that cannot be converted. However, the map() function allows you to ‘map’ any function onto all the elements of a list. Of course, if the list contains unsuitable elements, you will get an error: >>> a=[1,2,3,4,5] >>> b=map(float,a)# Apply the float() function to all elements >>> print b [1.0, 2.0, 3.0, 4.0, 5.0] >>> a.append(\\'fred\\') >>> print a [1, 2, 3, 4, 5, \\'fred\\'] >>> b=map(float,a) Traceback (most recent call last):   File \"<pyshell#16>\", line 1, in <module>     b=map(float,a) ValueError: invalid literal for float(): fred '), Document(metadata={'source': 'data/python book.pdf', 'page': 27}, page_content='Page 28 of 63 6.1.5 Tuples Tuples are lists that cannot be changed. In general, don’t worry about them, just use lists! However, they can be useful and you will come across them. If you use a library function that happens to return a tuple, just treat it like you would a list but don’t try to change it!  Tuples use round brackets for assignment and display: >>> a=(1,2,3) >>> a (1, 2, 3) >>> type(a) <type \\'tuple\\'> >>> print a[1] 2 >>> a[1] = 99 Traceback (most recent call last):   File \"<pyshell#88>\", line 1, in <module>     a[1] = 99 TypeError: \\'tuple\\' object does not support item assignment >>> 6.2 Strings We have already come across strings. They are very like a list in which every element is a character. You can access their individual characters by indexing or slicing. There are some differences to lists. You cannot, for example, change the elements of a string: >>> a=\\'This parrot is dead\\' >>> print a[5:11] parrot >>> a[5] = \\'f\\' Traceback (most recent call last):   File \"<pyshell#100>\", line 1, in <module>     a[5] = \\'f\\' TypeError: \\'str\\' object does not support item assignment >>> Python is very good at handling strings. There is a rich set of methods to do things to them. See chapter 3 of Hetland for details. A few useful examples are: find(): Find a substring within a larger string split(): Splits a string up into its elements into a list.  '), Document(metadata={'source': 'data/python book.pdf', 'page': 28}, page_content='Page 29 of 63 join(): Joins a list of strings (The opposite of split) For example: >>> a=\"Monty Python\\'s Flying Circus\" >>> a.find(\\'Monty\\') 0 >>> a.find(\\'Python\\') 6 >>> a.find(\\'Gumby\\') -1 >>> Note 1: If you want to use a quote in a string (EG for an apostrophe), you must use the OTHER type of quotes around the string Note 2: find() returns the index where the substring starts or -1 if the substring is not found. 6.3 Numpy arrays – An Introduction Whilst Python lists can be very powerful, what the physicist really needs is arrays that can be used to represent vectors and matrices. Python was not specifically designed for physics so it has no such arrays. Nor does it have much in the way of mathematical functions. All of this is added to the language by importing an essential module called NumPy. We have not yet covered the writing of functions or the importing of modules (which are essentially just a collection of functions). However, the arrays and functions provided by NumPy are so useful for solving physics problems that we will take a first look at them now. More detail on writing functions and using modules are given in chapter 8. More details of the NumPy module are given in chapter 10. There is an excellent introduction to NumPy (and its partner module SciPy) on the web at: http://www.scipy.org/Getting_Started Have a quick look at it now. However, some of the detail and examples in that introduction will mean more when you have looked at the later chapters of this course. Arrays are very like lists. The most important differences are: • All of their elements must be of the same type • Their ‘shape’ can be changed (EG from 1-d to 2-d) • Multi-dimensional arrays are addressed differently to lists • You can’t append to an array. Once its length is set, it’s fixed. • Manipulating arrays is much faster than manipulating lists For the technically minded, NumPy is implemented in the C language and is a compiled library. All Python list handling is interpreted and thus much slower. Thus using NumPy is the secret of doing complex physics problems in a slow language (Python) very quickly. For solving physics problems, if you are in doubt which to use, use arrays, not lists. '), Document(metadata={'source': 'data/python book.pdf', 'page': 29}, page_content=\"Page 30 of 63 6.3.1 Using NumPy The process of adding a module (or library) to Python is very simple and is called ‘importing’. At the Python prompt, type  import numpy   (Note the lower case name). This will, as if by magic, add a vast number of functions to Python. If you want to see how many, try typing dir(numpy).  To use them, just prefix their names with the name of the module. EG One way to create an array is using the NumPy  function called arange(). Try this: >>> a=numpy.arange(9, dtype='float') >>> print a [ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.] >>> We have specifically told NumPy that we want floats using the dtype parameter. If you don’t specify what you want you will get the default on your system. This may be integers or floats. You can also tell arrange you want floats by using a float as an argument: a=numpy.arange(9.0) You can change this 9 element 1-d vector into a 3x3 matrix thus: >>> a.shape=(3,3) # Use the shape method >>> print a [[ 0.  1.  2.]  [ 3.  4.  5.]  [ 6.  7.  8.]] >>> You can manipulate an array just like a variable. You can’t (easily) do this with a list: >>> b=a**2 >>> print b [[  0.   1.   4.]  [  9.  16.  25.]  [ 36.  49.  64.]] >>> 6.3.2 Addressing and Slicing Arrays Addressing and slicing arrays is done in a very similar way to lists. We use the same square brackets: []. So, in 1-d: >>> a=numpy.arange(9, dtype='float') >>> print a [ 0.  1.  2.  3.  4.  5.  6.  7.  8.] \"), Document(metadata={'source': 'data/python book.pdf', 'page': 30}, page_content=\"Page 31 of 63 >>> print a[1:5] # A slice [ 1.  2.  3.  4.] >>> Just as for a list, we have taken a ‘slice’ from the array, a[1:5]. Remember that the slice starts at index 1 but ends at index 4. IE One less than the last index given. The difference comes with arrays of more than one dimension. So in 2-d: >>> a=numpy.arange(9,dtype='float') >>> a.shape=(3,3) # Make it 2-d >>> print a [[ 0.  1.  2.]  [ 3.  4.  5.]  [ 6.  7.  8.]] >>> print a[2,1] #Print one element of the matrix 7.0 >>> We have addressed one element of the matrix at row 2, column 1 as: a[2,1]. Note that we address a 2-d NumPy array with the ‘y’ coordinate first, then the ‘x’ coordinate. So think of the element that you want from your matrix ‘a’ as having an address a(y,x). This form of addressing, with a comma, is not valid for lists. This section was just a taster. We will return to arrays in chapter 10. Remember, if you are manipulating sequences of numbers, especially in more than one dimension, use arrays not lists. 6.4 Dictionaries – Advanced Topic Dictionaries are a very powerful type in Python. However, they are not essential for this course and are not covered in any detail here. They are covered in chapter 4 of Hetland. They are another form of sequence, like a list, but the order of the elements is not fixed. Thus there is a key for every element. They use curly brackets like {}. They can be used to form a sort of mini database. For example, you could store your friend’s ‘phone numbers in a dictionary: >>> phonebook = {} >>> phonebook['Fred'] = '12345' >>> phonebook['Claire'] = '0234 7432' >>> print phonebook {'Claire': '0234 7432', 'Fred': '12345'} >>> print phonebook['Claire'] 0234 7432 >>> phonebook.has_key('Fred') True \"), Document(metadata={'source': 'data/python book.pdf', 'page': 31}, page_content='Page 32 of 63 >>> len(phonebook) 2 The entries in a dictionary are key/value pairs separated by commas. Both the key and the value can be of most types so the ‘phone numbers could have been integers. The key and the value are separated by a colon. You can add an entry that is not already there. There is no fixed index for the elements. There is no need for an ‘append’ method. Note: the method has_key() is used to check if the dictionary has such a key in it. If you try to access a non-existent key, you will get an error. Check it exists first. >>> print phonebook[\\'Clive\\'] Traceback (most recent call last):   File \"<pyshell#118>\", line 1, in <module>     print phonebook[\\'Clive\\'] KeyError: \\'Clive\\' >>> 6.5 Exercises 6.5.1 Exercise 6.1 - Lists Write a small database and some code to query it using lists. Create a list with the names of 10 ‘friends’. Create a second matching list with their years of birth. Ask the user to input a name. Tell him the place (index) of that person in the list, how many friends he has in total and the year of birth of the person. The input and output should look like this: Enter a name: Will You have 10 friends Will is number 8 in your list Will was born in 1991 Try entering a name that is not in the list. The program will crash. We will look at how to handle such conditions later. Model solution is in file: Exercise6.1.py on DUO. 6.5.2 Exercise 6.2 – Arrays Create a 10x10 matrix of integers starting at 1 and ending at 100 as a NumPy array. The first row is the integers 1 to 10, the second 11 to 20 etc. (Use numpy.arange() and the shape() method). Print the matrix to the screen. Now slice out the column from the matrix where all of the numbers end in the digit 5 (the 5th column). Print this to the screen. It should look like this: [ 5 15 25 35 45 55 65 75 85 95] Now slice out a sub-matrix from the original matrix to look like this: [[35 36 37 38] '), Document(metadata={'source': 'data/python book.pdf', 'page': 32}, page_content='Page 33 of 63  [45 46 47 48]  [55 56 57 58]] Print it to the screen. Now convert the sub matrix to a vector to look like: [35 36 37 38 45 46 47 48 55 56 57 58] This last stage is best done using the flatten() method. If you try to convert the sub-matrix to a vector using the shape() method you will run into problems. You have to make a copy of the sub-matrix first and flatten() does this for you. It is important when using arrays to be aware of when you need a copy. Python will try to not make a copy if it can avoid it because it wastes memory and is slow. In our example of making a sub-matrix, it will just point to parts of the original matrix. Model solution is in file: Exercise6.2.py on DUO. 6.5.3 Exercise 6.3 – Dictionaries - Advanced Topic The program for exercise 6.1 would have been better done with a dictionary rather than two lists. Repeat the exercise but using a single dictionary to hold the data. Which of the outputs from example 6.1 cannot be generated from data stored in a dictionary and why? If you enter a friend who is not in the list, the program will crash with a different error to the version that used lists. Both errors are worth looking at.  Model solution is in file: Exercise6.3.py on DUO. '), Document(metadata={'source': 'data/python book.pdf', 'page': 33}, page_content=\"Page 34 of 63  7 Chapter 7 - Conditionals and Loops Conditionals allow a program to make decisions. They are often called ‘if/then’ or ‘if/then/else’ for obvious reasons. Loops allow the computer to loop over a set of data. You might want to evaluate a function at 100 different values for example. Conditionals and loops are dealt with in chapter 5 of Hetland. 7.1 Conditionals You will often want to execute a different bit of your program depending on some condition or other. This is where the Boolean or logical variables come in. You can decide whether to execute some code depending on whether a condition is True or False. Now that we have started writing programs, and there are quite a few lines of code involved, the examples should be typed into the editor, saved as a file and run. You therefore won’t see the Python prompt (>>>) in the examples any more. Type this example in and try it: a = float(raw_input('Please enter a positive number: ')) if a < 0:     print 'That is a negative number'     print 'That is not what I asked for' else:     print 'Thank you'     print 'That is a positive number'  Note 1:  The < symbol means ‘is less than’. Note 2: You execute a block of code lines after the if. Python knows how much to do because there is a semi colon (:) after the if statement and the block to be executed is indented. This indenting is done for you by the IDLE editor but can be put in by hand. Any number of spaces (or a tab) is an indent. Lines within the block to be executed must all have the same indentation. Beware: Getting the indenting wrong is a very common error in Python. You can test for more than one condition at a time using elif (short for else if): a = float(raw_input('Please enter a positive number: ')) if a < 0:     print 'That is a negative number'     print 'That is not what I asked for' elif a> 0:     print 'Thank you'     print 'That is a positive number' else: \"), Document(metadata={'source': 'data/python book.pdf', 'page': 34}, page_content=\"Page 35 of 63     print 'That looks like zero to me'          You can have conditionals within conditionals (an if within an if) but be careful to get the indentation right. The comparison operators are not all so obvious as < and >. Here is a table of them:   Expression Description x = = y x equals y  x < y x is less than y x > y x is greater than y x >= y x is greater than or equal to y x <= y x is less than or equal to y x != y x is not equal to y x is y x and y are the same object x is not y x and y are different object x in y x is a member of the sequence y x not in y x is not a member of the sequence y  Note 1: It is a very common error to use x=y to test if x is equal to y (Rather than x = = y). This usually gives a syntax error. There are situations where it will not! Python will just do as it is told and set x equal to y - VERY confusing! Note 2: The last 4 are less used but quite interesting. Try using them! There is also an and operator and an or operator. These are used to test more than one condition at the same time: if a < -10 or a > 10:  print ‘Magnitude is too large’  if a < 0 or b < 0:  print ‘One of the values is negative’ Note: These examples are code fragments. They won’t work on their own.   7.2 Loops Let’s go back to the example of evaluating a function. We may want to evaluate it for 100 values of x and save all of the values somewhere. (Later, we will also draw a graph of it). \"), Document(metadata={'source': 'data/python book.pdf', 'page': 35}, page_content='Page 36 of 63 There are two sorts of loops in Python that allow you to execute a block of code many times: for loops and while loops. 7.2.1 Loops - The ‘while’ loop These do exactly as they say. They will execute a block of code while some condition is true: prompt = \\'Please enter a positive number less than 10: \\' x=-1 # Initialise x to an invalid number while x<=0 or x>9:     x = float(raw_input(prompt)) print \\'Thank you. That number is fine.\\'  Try this. The while loop will repeat the indented line until the user inputs a valid number. Note: We put the prompt string into a variable so that we can more easily change it. 7.2.2 Loops – The ‘for’ loop While loops are very versatile and useful. However, physicists are often looping over a set of numbers as in the example of evaluating a function for many values. For these applications, we use a for loop to loop over all of the elements of a list or array. First, let’s loop over a list of strings. Type this in and try it: cheeseList = [\\'Wensleydale\\', \\'Stilton\\', \\'Danish Blue\\', \\'Red Leicester\\', \\'Brie\\'] for cheese in range(0, len(cheeseList)):     print \"Do you have some\", cheeseList[cheese], \"?\"     print \"Not as such\"  Note: This could have been written more concisely as: cheeseList = [\\'Wensleydale\\', \\'Stilton\\', \\'Danish Blue\\', \\'Red Leicester\\', \\'Brie\\'] for cheese in cheeseList:     print \"Do you have some\", cheese, \"?\"     print \"Not as such\"  In the first version, we have used a very useful Python function range(). Because we so often want to iterate (or loop) over a list of numbers, Python provides this to create a suitable list of integers. Try this function out from the command line: >>> range(10) [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] >>> range(1,10) '), Document(metadata={'source': 'data/python book.pdf', 'page': 36}, page_content='Page 37 of 63 [1, 2, 3, 4, 5, 6, 7, 8, 9] >>> range(0,10,2) [0, 2, 4, 6, 8] Note: You can give it a start and end value and an increment, or just an end value. It returns a list up to one less than the end parameter. So now let’s use it to loop over a set of numbers: for number in range(10):  print \"The number:\", number, \"It\\'s square:\", number**2  Note 1: The variable ‘number’ is just used to store the element from the list generated by the range function that we use on each go round the loop. Note 2: As you can see, Python is happy to square the value of ‘number’ before it prints it. 7.2.3 Getting out of Infinite Loops - break It is often useful to use an infinite loop, typically using while True:. You can break out of such a loop on a condition using the break command. You should not use this more than is necessary as it can make code somewhat less clear to read. Here is an example in ‘pseudo code’. This is a useful way of showing some code structure without sticking to the language syntax: while True:  Do something  if <some test>:   break  else   do some other stuff carry on # The break causes a jump to here  You can also escape from an infinite loop by entering <CTRL C> from the keyboard. 7.3 Exercises 7.3.1 Exercise 7.1 – Advanced topic Improve the program that you wrote for exercise 6.3 using a dictionary : • The new program should loop after each name has been dealt with, and exit if the user just hits <return> with no name. It should print a message on exit. • The program should handle names that are not in the database cleanly by issuing a message and asking for another name Model solution is in file: Exercise7.1.py on DUO. 7.3.2 Exercise 7.2 Write a program to list some integers, their squares and their cubes. Read the start value, the '), Document(metadata={'source': 'data/python book.pdf', 'page': 37}, page_content='Page 38 of 63 end value and the increment from the command line. Try to do this with just one input. Use a for loop and the range() function. Test your program starting at 2 and ending at 10 with an increment of 2. You should see some input and output like this: Enter Start, Stop, Increment: 2,10,2 Number Square Cube    2     4       8    4    16    64    6    36  216    8    64   512 Model solution is in file: Exercise7.2.py on DUO. 7.3.3 Exercise 7.3 Improve the program you wrote for exercise 7.2: • Make it loop until exit is requested by the user hitting <Return> only. • Allow the Stop and Increment parameters to be optional. We indicate this using square brackets in the prompt. Supply defaults for when they are not entered. A session might now look like this: Enter <Carriage return> only to exit Enter Start, [Stop], [Increment]: 2,6,2 Number Square Cube    2     4        8    4    16     64    6    36   216 Enter Start, [Stop], [Increment]: 1,3 Number Square Cube    1     1      1    2      4       8    3      9   27 Enter Start, [Stop], [Increment]: 2 Number Square Cube    2      4     8 Enter Start, [Stop], [Increment]:  Exit requested '), Document(metadata={'source': 'data/python book.pdf', 'page': 38}, page_content='Page 39 of 63  8 Chapter 8 - Functions and Modules The whole principal of writing software for Physics depends on good program structure. Much of the functionality that you require will be provided by modules that you import into your programs. In a similar way, nearly all of the functionality of your own code should be built as a set of modules (just a separate file), each of which contains a group of related functions. Every function that you write is a separate entity and can, and should, be tested independently to be sure that it works as you expect it to. Note 1: You can include one or two functions with your main program in these simple examples, but note that the function definition must occur before the main program. Once all of your functions are in modules, you import them at the top of the main program file. Note 2: In his wisdom, Hetland calls his chapters on functions Abstraction; a very correct name but a touch confusing. His chapter 6 covers the writing and use of functions. His chapter 7 covers the alternative technique of using methods (a part of object-oriented programming). These are not included in this course but feel free to learn such techniques if you have time. Beware: Because the use of methods is becoming more and more popular, many programmers will refer to their functions as methods. 8.1 A First Function So, let us now look at writing some code as a function and then calling that function from a main program. We want to evaluate the polynomial x3 – 7x2 +14x -5: def poly(x):     \\'\\'\\'     Evaluate the polynomial x^3 - 7x^2 + 14x - 5     \\'\\'\\'     return x**3 - 7*x**2 + 14*x - 5.0  # Test program for the function poly() while True:     xValue = float(raw_input(\"Enter a value for x:\"))     yValue = poly(xValue)     print \"The value of the polynomial is:\", yValue  There are various points to note here: Syntax: Note that the line with the def() command on it ends in a colon. You must then indent the block of code that is the function, just as you did for conditional blocks. IDLE will help you to do this. Warning: Be very careful with your indentation. Python relies heavily on indentation to know when a block of code starts and ends. Your indentation must be correct. '), Document(metadata={'source': 'data/python book.pdf', 'page': 39}, page_content='Page 40 of 63 You can pass parameters to functions (separated by commas for more than one). The equivalent variable within the function is a different variable. Its value will not affect that in the calling program. In general, use a different name to remind you that they are different. The variable x in the function above is said to be local to the function poly(). If you want to return some value to the calling program, you must use the return statement. We could have used another variable for the value of the polynomial in the function and returned that variable but Python is happy not to bother. It works out the value and returns it. This code would be just as valid: y = x**3 - 7*x**2 + 14*x - 5.0 return y  A function doesn’t have to have a return statement. It’s often handy to have a few lines of code in a function that do something useful but don’t return a result.  This program will run forever! The value of True is always True. To escape from the program, just type <CTRL C>, meaning hold down the CTRL key and type C. This will interrupt your program and return you to the Python prompt. It is better to build in a way to exit cleanly as we saw in exercise 7.3. 8.1.1 Default values for parameters in a function In general, the number of arguments sent to a function should match the number that it is expecting. However, you can pass less than the number it is expecting, provided you give it a default. This is done as follows: The function definition: def myCleverFunction(start, stop, inc=1): You MUST provide start and stop but, if you miss out inc it will be set to 1. In this call, the default value of inc=1 is overridden by the value of 5 provided: start = 1; stop = 10; increment = 5 a = myCleverFunction(start, stop, increment) In this call, no value is provided for inc, so it is set to 1: start = 1; stop = 10 a = myCleverFunction(start, stop) 8.1.2 Documentation Strings As well as using a # for comments, you can just put a string on a line on its own as a comment. It is usual to put a documentation string at the start of every program, at the start of every function and at the top of every module. These documentation strings are important. Python itself will use them to provide help to the programmer. To see this in action, when you type in a call to your own function using IDLE, it will pop up a ‘tip’ to say how the function works. This will be your documentation string. We normally use triple quotes (of either type) for this. This allows there to be carriage returns within the documentation string. Note: In your course work, you will be expected to provide documentation strings. You will lose marks if they are omitted. '), Document(metadata={'source': 'data/python book.pdf', 'page': 40}, page_content='Page 41 of 63 8.2 Returning more than one value You may wish to return more than one value from a function. This is very easy. The return statement will take a list of values, separated by commas. This should match a set of values in the calling program. Alternatively, the returned values can be put in one variable. This will be a tuple. Just treat it like a list to access the results: def cleverFunction(<Parameters>):  return a,b first, second = cleverFunction(<Parameters>)  or: def cleverFunction(<Parameters>):  return a,b results = cleverFunction(<Parameters>) a = results[0] b = results[1]  Note: The above is what is known as ‘pseudo code’. It is not working Python. It just gives an idea of how some real Python might be written. The non-Python bits are often put inside <>. 8.3 Modules and import So, what is a module? It is just a library of useful functions or methods gathered together into a single file. In general, we gather together functions with related functionality into a module. There are very useful standard modules that you can use, and of course you can write your own. Once a module exists, any program can ‘import’ it and make use of its functions. Whether it is a standard module or your own module, the ways of importing a module (making it available to your own program) are the same. Note: A module can import other modules. If you write your own module of clever maths routines, it will probably have to import the numpy module in order to have some maths functionality. Modules are so important they are mentioned in chapter 1 of Hetland. However, the main chapter on modules is his chapter 10 (‘Batteries Included’). This gives a neat introduction to using standard modules in Python. We give just a brief introduction to how to use modules here. There are several different ways in which modules can be imported: 8.3.1 Import This is the default way to import a module. If in doubt, always use this plain import. Every call to a function in the module must use the module name as a prefix to the function name, separated by a dot(.): import myCleverLib # import the module '), Document(metadata={'source': 'data/python book.pdf', 'page': 41}, page_content='Page 42 of 63 # Now call a function in that module theAnswer = myCleverLib.miracle(<parameters>) 8.3.2 from <module> import <function> It is sometimes a bit boring to keep using the full name of a function in a module. There are clear advantages to doing so and it is good practice to do so. The function call, with its module prefix, will be unique which is essential to avoid confusion. However, as always, there are shortcuts and you will see them used. It is poor practice so try not to use them. If you do, be careful. It’s easy to get confused. You can import one or all of the functions from a module so that they look just like functions in your own module: # Import just the one function from a module from myCleverLib import miracle # Now call a function in the module theAnswer = miracle(<parameters>)  #Import all of the functions from a module from myCleverLib import * # Now call two functions from the module theAnswer = miracle(<parameters>) theOtherAnswer = theOtherWay(<parameters>)  If you have a function with the same name in two different modules and import them like this, you won’t know which one you are calling! Even worse, Python will probably decide which one to use and assume that you know. IE It won’t give you an error. So beware. 8.3.3 import <module> as <name> This is just a useful shorthand. If the module has a long name, you can replace that name at import time with a shorter one: #import the module with an alternative name import myCleverLib as lib #Now call a function in the module theAnswer = lib.miracle(<parameters>) # A bit shorter  Advanced topic: Just for fun, there is another shortcut technique. You can use a variable to ‘point to’ a function: import myCleverLib clever = myCleverLib.miracle # A pointer to the function '), Document(metadata={'source': 'data/python book.pdf', 'page': 42}, page_content=\"Page 43 of 63 theAnswer = clever(<parameters>) 8.3.4 Allowing Python to find your Modules Python must know where to find your modules. It keeps a list of where all of the standard modules live so you don’t have to worry about them. The best way to start working is to keep your modules in the same directory from which you are running your main program. This is one of the places where Python will look for modules. Let’s assume you get more organized and put all your well tested modules in the directory: /home/fred/mymodules. You may be developing a program in a different directory. Python can’t find your modules! You need to set what is called an environment variable with the special name PYTHONPATH. This can be done at the Linux prompt with the command: setenv PYTHONPATH /home/fred/mymodules You can check that the variable is set using: echo $PYTHONPATH Now Python will look in that directory as well as all of its usual places. The value of this environment variable will be lost when you log off from Linux. This process is described in more detail in Hetland chapter 10. He also describes the different process used if you are working under MS Windows. Note: If you don’t know if a module exists on your system, try to import it from the Python prompt. If it’s there and Python can find it, it will import ok. 8.3.5 What’s in a Module? Once you find, or are told, about a useful module, how do you know what is in it? What functions does it offer? There is a useful built-in function in Python called dir(). This takes the name of a module and tells you what is in it. You will have to import it first of course! We will soon come across the math module. Try importing it then use dir(math) to find out what it can do: >>> import math >>> dir(math) ['__doc__', '__name__', 'acos', 'asin', 'atan', 'atan2', 'ceil', 'cos', 'cosh', 'degrees', 'e', 'exp', 'fabs', 'floor', 'fmod', 'frexp', 'hypot', 'ldexp', 'log', 'log10', 'modf', 'pi', 'pow', 'radians', 'sin', 'sinh', 'sqrt', 'tan', 'tanh'] >>> print math.__doc__ This module is always available.  It provides access to the mathematical functions defined by the C standard. >>> print math.sin.__doc__ \"), Document(metadata={'source': 'data/python book.pdf', 'page': 43}, page_content='Page 44 of 63 sin(x) Return the sine of x (measured in radians). >>>  Right away, we can see a list of the functions in the module. We have printed the ‘attribute’ called __doc__. This gives you the documentation string or comment that the programmer put (as you should put) at the head of his module.  Every function in the module should also have such a documentation attribute. We have printed the one for the math.sin() function. Python built-in functions also have __doc__ attributes. Try typing this at the command line: print range.__doc__ Note 1: Python uses some special names within modules and functions that begin and end with two underscore characters. Don’t use such names for your own variables! Note 2: IDLE will help you when you are typing, either in the Python shell or in the editor, by putting up ‘tips’, or little pop-up boxes to remind you of the relevant syntax. These are based on attributes of the function or module such as __doc__. 8.3.6 Testing Functions and Modules Testing every function and every module that you write is essential. When you write a new function, test it with some test code then put it into a relevant module. A module file contains one or more functions and is designed to be imported, not run on its own. However, it is useful to the programmer who tests and maintains the module to be able to test the functions by just running the module. There is a standard way to do this. The module will have a (hidden) variable called __name__. If you just run a module, this name will be set to the default which is “__main__”. You can thus detect that the user is running the module, rather than importing it, and run a bit of test code. This bit of code should be at the end of the module. You can change it to cause a call to the function being tested, and to print some results as in this example: ’’’ Module to do some fancy stuff ’’’ def clever(x,y):  ‘ Function that is pretty smart’  < Do clever stuff > # The clever code goes here  return result  if __name__ == ‘__main__’:  print ‘Testing clever()’  a = 1.2  b = 3.4  answer = clever(a,b)  print ‘a=%.3f, b=%.3f, answer = %.3f’ % (a,b,answer) '), Document(metadata={'source': 'data/python book.pdf', 'page': 44}, page_content='Page 45 of 63  This is a standard way of testing a function in a module. It is also fine of course to write a dedicated test program that imports the module and calls the function. If you leave test code in a module that is not subject to this test on __name__, it will be executed on import to your main program; probably NOT what you want. 8.3.7 pyc Files When you import a module, in order to speed up execution, Python will ‘compile’ the module and put the compiled code in a file with the same name but a .pyc extension. ‘But this is an interpreted language isn’t it?’ Well yes. This is not a real compilation but a conversion to something called ‘byte code’. Next time you import the module, Python will use this compiled version. If you change the source code of the compiled module, Python will notice and recompile it next time you use it. The point is that the byte code in the .pyc module will run much faster than if the module is interpreted every time. You don’t have to worry about any of this. Just ignore the .pyc files. If you delete them, Python will re-create them when it needs to 8.4 Exercises 8.4.1 Exercise 8.1 Write a program to evaluate the polynomial x3 – 7x2 +14x -5 as described in section 8.1. You can improve on the code given in that section! For this first function example, just put the function in a file followed by some code to test it. Model solution is in file: exercise8.1.py on DUO. 8.4.2 Exercise 8.2 Improve the program you wrote for exercise 8.1 by moving the function into a separate file; a new module. Put a documentation string in the module and in the function. Add some standard test code as described in section 8.3.6. Then write a separate test program that imports your new module and uses the function. You can use much the same code for testing within the module and in the stand-alone test program. Model solution for the module is in file: poly01.py on DUO. Model solution for the test code is in file: exercise8.2.py on DUO. 8.4.3 Exercise 8.3 Write a function that will evaluate ANY cubic (order 3) polynomial. Add this function to your module. It should take both the value at which to evaluate the polynomial and the 4 coefficients as parameters. Hint: The coefficients could be passed as a list. If the coefficients are not passed to the function, they should take on default values. Write some simple code to test this function inside the module. Model solution for the module is in file: poly02.py on DUO. Now write a program that uses the function. Ask the user to input the coefficients. Enumerate the function from -5 to 5 in steps of 0.5 and print the results to the screen. Test the function for various sets of coefficients.  A model test program is in the file exercise8.3.py on DUO. '), Document(metadata={'source': 'data/python book.pdf', 'page': 45}, page_content='Page 46 of 63 8.4.4 Exercise 8.4 Write a function that takes a list of real numbers and returns both the maximum and the minimum values. Put the function in a module file with some simple test code. Make it work for positive and negative floating point numbers. Model solution for the module is in file: myNumericLib01.py on DUO. Now write a test program for this function. Rather than ask for the numbers from the screen, generate a set of test values in a list. Print the test data and the maximum and minimum to the screen in the test program. Check that the function works as expected. A model test program is in the file exercise8.4.py on DUO. Note: There are of course built-in functions called max() and min() in Python. They are useful but don’t call your own variables ‘max’ or ‘min’. Python may not do what you expect! You should not use variable or function names that already exist in Python. If you don’t know if Python already has a function or keyword, try it at the command line. 8.4.5 Exercise 8.5 Add two further functions to the module you developed in exercise 8.4 to find the mean and the sum of a list of numbers. Hint: Use your sum function in your mean function. Beware: Python has a built-in function called sum(). Model solution for the module is in file: myNumericLib02.py on DUO. Modify the test program of exercise 8.4 to use all three of the functions in your new module. A model test program is in the file exercise8.5.py on DUO. '), Document(metadata={'source': 'data/python book.pdf', 'page': 46}, page_content=\"Page 47 of 63  9 Chapter 9 - Debugging and Exceptions At some point, your program will go wrong! This may be an error that crashed the program and gives you a traceback (the easy ones to find), or it may be an error that means that the program runs to completion but gives you the wrong answer (harder to find). There are various ways to find out where the bug is. We tend to call all errors in programs ‘bugs’ (for historical reasons). In Python, errors are properly referred to as ‘exceptions’. Dealing with exceptions is an important part of programming. Chapter 8 in Hetland covers this. We give a brief introduction here. The handling of exceptions is an advanced topic so you can skip most of this section if you wish. Just note the main debugging methods mentioned in sections 9.1, 9.2 and 9.3. 9.1 Using print for debugging The first and foremost method of debugging is to print out any variables that you are using that might have the wrong values. Just add a print statement to the program, re-run it and check that the variables all have the value that you are expecting: print ‘a=’, a, ‘ b=’, b # This is a debug line 9.2 Use the Command Line If you get an error that suggests you have used the wrong syntax or called a function incorrectly, repeat what the program was doing when it crashed at the command line. All Python programmers, however skilled, do this. Get it to work at the command prompt then use it in your program. 9.3 Module Test Code You can test and debug the functions in your module using test code included in the module. This technique was described in section 8.3.6 9.4 Handling Exceptions: try / except – Advanced Topic You should, of course, try to remove all of the bugs from your program before it is used to solve a problem. However, there are situations where an error could still occur and you don’t want the program to crash when it does. To prevent this happening, there is a construct called try / except that tells Python what to do when the error occurs.  You don’t need to catch exceptions in this course, just fix every bug!! However, exception handling is a powerful advanced technique. 9.4.1 Catching ALL Exceptions The handling of exceptions is covered in chapter 8 of Hetland. There is not time in this course to go into exception handling in any detail. However, let’s just look at one simple example because the try / except construct can be very useful. Try this program: a = input('Enter a number: ') b = input('Enter another number: ') print a/b \"), Document(metadata={'source': 'data/python book.pdf', 'page': 47}, page_content='Page 48 of 63  If you enter 0 for the second number, you should get this output: Enter a number: 2 Enter another number: 0 Traceback (most recent call last):   File \"testException.py\", line 3, in <module>     print a/b ZeroDivisionError: integer division or modulo by zero  You can stop the program from crashing by ‘catching’ the exception: try:     a = input(\\'Enter a number: \\')     b = input(\\'Enter another number: \\')     print a/b except: # Catch ALL exceptions     print \\'The second number must not be zero\\'  Run this as before and you should get: Enter a number: 2 Enter another number: 0 The second number must not be zero  Here, Python has seen the division by zero, but it has also seen your ‘exception handler’. This is the indented code after the except which it has duly executed. The program did not crash! If there is no exception, the exception handling code is just ignored. Note 1: Take note of the colon used after try and except and the fact that, like a conditional block, the code within the try or except block must be indented. 9.4.2 Catching Specific Exceptions The above example will catch ALL exceptions that occur. This is generally bad practice. If there is a syntax error in your ‘try’ block, the exception handler will catch it and give you no traceback!! You should specify the exceptions that are likely to occur. It is possible to catch only one (or more) specific exceptions. This is well described in Hetland, chapter 8. Here is some pseudo code as an example: try:  <Some smart code> except (<exception1>, <exception2>): '), Document(metadata={'source': 'data/python book.pdf', 'page': 48}, page_content='Page 49 of 63  print “An exception occurred”  Only the try block is normally executed. If either of the specified exceptions occurs, the except block is executed. If any non-specified exception occurs, the program will crash. Note 1: If there is more than one specified exception, put them in a tuple, as shown. Note 2: To find out the name of an exception, make it happen at the command line and look in the traceback: >>> a=2/0 Traceback (most recent call last):   File \"<pyshell#1>\", line 1, in <module>     a=2/0 ZeroDivisionError: integer division or modulo by zero  The name of the exception here is ‘ZeroDivisionError’. The code from section 9.4.1 should therefore have been written as below. Note that we have put only the code that might be subject to the error inside the try block: a = input(\\'Enter a number: \\') b = input(\\'Enter another number: \\') try:     print a/b except ZeroDivisionError: # Catch division by zero     print \\'The second number must not be zero\\'  9.5 Exercises  9.5.1 Exercise 9.1 – Advanced Topic Write a routine that requests input of several floating point values from the screen, catches errors on input and returns the values. Put the function in a module file with some simple test code. The function should take a prompt string as its single parameter. It should return all of the valid numbers that are input as a list of floats. If there is some invalid input, it should return an empty list. Test the function with some valid input and some invalid input. The two tests should produce output like: Enter the parameters separated by commas:1,2,3 The parameters are: [1.0, 2.0, 3.0] >>>  Enter the parameters separated by commas:1,2,3,fred The parameters are: [] '), Document(metadata={'source': 'data/python book.pdf', 'page': 49}, page_content='Page 50 of 63  Hint: Use the try/except construct to catch errors where they are likely to occur. A model solution is in the file exercise9.1.py on DUO. '), Document(metadata={'source': 'data/python book.pdf', 'page': 50}, page_content='Page 51 of 63  10 Chapter 10 - Maths Modules: NumPy As yet, we have not really done much mathematics! The Python language does not include built-in support for maths. However, there are of course modules that we can import to provide mathematics functions. The most basic is the math module. This is generally not enough for the Physicist. By all means load the math module and try it. However, we will use the far more powerful NumPy module. This was introduced briefly in section 6.3 but further details are given here. 10.1 The math Module This provides basic maths functionality: sqrt(), sin(), cos() etc. 10.2  The NumPy Module A huge amount of development has gone into the development of a mathematics module called NumPy.  Note 1: It is usually written as NumPy but when you import it use: import numpy  (all lowercase). Note 2: You may come across previous versions called Numeric and NumArray. These are now out of date. Please don’t use them. The NumPy module has been installed for you on the ITS Linux service. It is also bundled with the Enthought version of Python that we recommend for MS Windows. Hetland does not cover the NumPy module at all. However, as stated previously, there is an excellent web page that provides an introduction and a tutorial at: http://www.scipy.org/. This web page refers to another module, SciPy, which provides more complex mathematical functionality, but is heavily dependent on NumPy. Learn to use the facilities of NumPy first. We will give a very brief introduction here. It is worth trying the tutorial which can be found at: http://www.scipy.org/Tentative_NumPy_Tutorial. What NumPy gives you is new numerical array and matrix types and a set of basic operations on them. Unlike Python lists, this new array type is a set of elements, all of the same type. Thus these arrays can be used for vectors and matrices. They provide us with fast matrix and vector manipulation.  One thing to note is that these new types are implemented in pre-compiled code in the C language. The important consequence is that, once you define one, it has some memory allocated to it. You cannot therefore increase its overall size once it has been created.  So, as always, let’s try some things at the command line: 10.2.1 Creating Arrays and Some Examples of Basic Manipulation There are various ways to create an array: You can create an array from a list: >>> import numpy >>> a = numpy.array([1.0,2.0,3.0]) >>> type(a) '), Document(metadata={'source': 'data/python book.pdf', 'page': 51}, page_content=\"Page 52 of 63 <type 'numpy.ndarray'> >>> a array([ 1.,  2.,  3.]) >>> Note 1: The type of an array is actually ndarray for ‘n-dimensional array’. Note 2: We can have matrices of any number of dimensions. You can create an array using the NumPy equivalent of range() which is called arange(): >>> y = numpy.arange(12) >>> y array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]) >>> y.shape = 3,4 # Make it a 3 by 4 matrix >>> y array([[ 0,  1,  2,  3],        [ 4,  5,  6,  7],        [ 8,  9, 10, 11]]) >>> We have created y then changed its shape attribute. In maths terms, we started with a 1d vector and changed it into a 3x4 matrix. You can only do this if the total number of elements remains unchanged. Now we can do some maths on our matrix: >>> a = y*3 >>> a array([[ 0,  3,  6,  9],        [12, 15, 18, 21],        [24, 27, 30, 33]])  Without the matrix capabilities of NumPy, you would have to do this multiply one element at a time in two nested for loops. Try it! It’s a useful exercise to try once. You can slice an array and index an array just as you would a Python list. Use commas to separate the dimensions and the usual colon for the slice: >>> a[2] # The third (numbered from 0!) row array([24, 27, 30, 33])  >>> a[1,2:4] # The third and fourth elements of the second row  array([18, 21]) This can be confusing (mainly because of starting at zero and giving the end index as the one after the last one you want). Try lots of examples at the command line until you get the hang \"), Document(metadata={'source': 'data/python book.pdf', 'page': 52}, page_content=\"Page 53 of 63 of it. We can change the value of any element or slice of the matrix: >>> a[2,3] = -99 # Change the fourth element of the third row >>> a array([[  0,   3,   6,   9],        [ 12,  15,  18,  21],        [ 24,  27,  30, -99]]) >>> a[1] = 0 # Set the second row to all zeros >>> a array([[  0,   3,   6,   9],        [  0,   0,   0,   0],        [ 24,  27,  30, -99]])  Our array has various useful attributes that we can access: >>> a.shape # The shape of a (3, 4) >>> a.ndim # The number of dimensions of a 2 >>> a.size # The overall size of a 12  So far, we used integers; but arange(), unlike range(), can take floats as parameters: >>> numpy.arange(2.0,6.0,0.5) array([ 2. ,  2.5,  3. ,  3.5,  4. ,  4.5,  5. ,  5.5])  There is a function to create an array with all zeros. This can then be filled with the data we want: >>> a = numpy.zeros(10) >>> a array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]) >>> type(a[0]) <type 'int32scalar'>  We didn’t tell Python the type of numbers we wanted in the array. It has given us integers by default. Some versions of NumPy will give you floats by default. To be sure you get what you want, you can specify the type using dtype as follows: \"), Document(metadata={'source': 'data/python book.pdf', 'page': 53}, page_content=\"Page 54 of 63 >>> a = numpy.zeros(10,dtype=numpy.float64) >>> a array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]) >>> type(a[0]) <type 'float64scalar'> There is also a function to fill the new array with ones: >>> numpy.ones(10, dtype=numpy.float64) array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.]) 10.2.2 Linear Algebra NumPy provides a LOT of useful matrix and vector manipulation routines; too many to list here. Let’s just look at one basic linear algebra routine that you might need: If you use the normal * for multiplication, NumPy will multiply all of the elements of two matrices together: >>> a = numpy.arange(1,5,dtype=numpy.float) >>> a array([ 1.,  2.,  3.,  4.]) >>> a.shape = 2,2 >>> a array([[ 1.,  2.],        [ 3.,  4.]]) >>> b = numpy.arange(1,5,dtype=numpy.float64) >>> b array([ 1.,  2.,  3.,  4.]) >>> b.shape = 2,2 >>> c=a*b >>> c array([[  1.,   4.],        [  9.,  16.]])  This may not be what you want. If you want the dot product, use the function numpy.dot(): >>> d = numpy.dot(a,b) >>> d array([[  7.,  10.],        [ 15.,  22.]])  \"), Document(metadata={'source': 'data/python book.pdf', 'page': 54}, page_content=\"Page 55 of 63 Check this is correct by hand. Many of the more advanced linear algebra functions are contained in a sub-module within NumPy called numpy.linalg. Have a look at what is available using: >>> import numpy >>> dir(numpy.linalg) ['LinAlgError', '__builtins__', '__doc__', '__file__', '__name__', '__path__', 'cholesky', 'det', 'eig', 'eigh', 'eigvals', 'eigvalsh', 'info', 'inv', 'lapack_lite', 'linalg', 'lstsq', 'norm', 'pinv', 'qr', 'solve', 'svd', 'tensorinv', 'tensorsolve', 'test'] 10.3 The SciPy Module – Advanced Topic The NumPy module contains a large number of useful functions for numerical calculations. The SciPy module extends these facilities. Many of the functions in SciPy are higher level routines that build on the facilities provided by NumPy. You don’t need to use SciPy in this course but you will find it useful in your future courses on Computational Physics. There are documentation and examples on-line at: www.scipy.org 10.4 Exercises 10.4.1 Exercise 10.1 Write your own routine to find the dot product of two matrices without using numpy.dot(). This can be done using three nested loops. Test it with two off 2x2 matrices and check your result by hand. Test it with 2 off 3x3 matrices and check your result using numpy.dot(). A model solution is in the file exercise10.1.py on DUO. 10.4.2 Exercise 10.2 This is an example of some code that is practical and useful. It is a little harder than previous examples. When we take an astronomical image, the pupil of the telescope is circular due to the circular primary mirror. However, the CCD detector is square. When we come to analyse the CCD data, we don’t want to bother with the pixels that are outside the pupil. We therefore need a mask that has a ‘1’ for every pixel inside the pupil, and a ‘0’ outside it.  Write a program that will make such a mask for any square array of data and any pupil radius. There are many ways of doing this. One way is to generate an array that, for every element or pixel, contains the distance of that pixel from the centre of the array. The mask elements are then just ‘1’ when this value is less than the pupil radius and ‘0’ if it is greater. Hints: There is a useful NumPy method called numpy.where(). This sets elements of an array to one value or another depending on a condition. The 2-d array of distances from the centre can be filled by using two nested loops. This is rather slow. There are various faster ways of doing this without loops using NumPy techniques. To map values from a 1-d array onto a 2-d array, the 1-d array must be extended to 2-d. There is a NumPy facility called ‘newaxis’ that will add a dummy dimension to an \"), Document(metadata={'source': 'data/python book.pdf', 'page': 55}, page_content='Page 56 of 63 array. A model solution is in the file exercise10.2.py on DUO. This module contains two functions. One is based on using loops and the other, faster function uses NumPy facilities.  '), Document(metadata={'source': 'data/python book.pdf', 'page': 56}, page_content=\"Page 57 of 63  11 Chapter 11 - File Input and Output – The Details So far, we have looked mostly at input and output to the screen. You will often want to do some calculation and output the results to a file. Similarly, you may need to read some data from a file in order to analyse it. You don’t have to worry about how the computer stores its data on the hard disk. The operating system will look after that. All you have to do is tell Python which file you want to use, and how you want to use it. We gave a short introduction to doing this in section 4.2 that allowed you to read and write NumPy arrays to/from file. However, you may want to output quite complex mixtures of text and numbers to a file. The details of file IO are given here and there is a self-contained short chapter (11) in Hetland on ‘Files and Stuff’. 11.1 Line Terminators – The \\\\n character It is often useful to read individual lines of text from a file. The end of a line is indicated by a special character or characters. The characters used vary from language to language and for different operating systems. Python uses the character \\\\n (backslash n). This represents a single End of Line (EOL) character.  Try this at the command line: >>> a='This is the first line. This is the second' >>> print a This is the first line. This is the second >>> a='This is the first line.\\\\nThis is the second.' >>> print a This is the first line. This is the second. As you can see, the print function translates the \\\\n into a new line on the screen. Python is very good at dealing with new line characters. Unix also uses \\\\n so no problem there. MS Windows does not. Python on MS windows will do all the translating for you so you don’t have to worry. Just use \\\\n when you want a new line. When writing text to a file, separate the lines with a \\\\n. You can then use the readline() and readlines() functions when you read the file. 11.2 Writing to File Let’s first write some text to a file, then we can try reading it back in various ways. First of all, you must open a file. Not surprisingly, we open a file using the open() function. It will return a file object. We use this object and its methods to access the file. We are again slipping into using objects and methods rather than functions. That is how Python works and you know the syntax so just use the methods provided. Note 1: Before you write to a file, remember that the file will be written in the directory in which you are working unless you give a full path name. To find the name of your working directory under Linux, type pwd (print working directory). To write to a file, try this at the command line: \"), Document(metadata={'source': 'data/python book.pdf', 'page': 57}, page_content=\"Page 58 of 63 >>> f = open('temp.txt', 'w') >>> f <open file 'temp.txt', mode 'w' at 0x0138D260> The variable f is a file pointer object. The second parameter of the open function tells Python how to use the file: ‘r’ => You may only read from this file. This is usually the default. ‘w’ => You may only write to the file ‘a’ => You may append to the file. IE You will write after any data already in the file. Note 2: It is also possible to read and write to a file using ‘r+’ or ‘w+’ mode in combination with the seek() method to read or write to any point within the data in an existing file. You probably won’t need this yet so it is not described here. Now write some lines to the file: >>> f.write('This is the first line.\\\\n') >>> f.write('This is the second line.\\\\n') >>> f.write('This is yet another boring line.\\\\n') >>> f.close()  Notice that we close the file when we have finished with it. You should not leave files open. You will find that you now have a file called temp.txt in your working directory. (Check under Linux with the ls command). Try typing the file – under Linux type: more temp.txt. Note 3: After each write to the file, we have put in a \\\\n to split the file into lines. Note 4: By default, Python will always write from the beginning of a file that is opened in ‘w’ mode. So beware of overwriting any existing contents.  Note 5: All of your data should be written to file as strings. Thus you must convert any numbers to strings before writing them to file. You can write numbers to file in ‘binary’ mode but this is not needed for this course.  11.3 Reading from File Reading from a file is equally easy. You must however tell Python how much of the file you want to read. In general you can: Read a fixed number of bytes from a file using the read(num) method where num is the number of bytes to read. This is not very useful! Read a single line from a file using the readline() method. Read all the lines in a file using the readlines() method. They will be returned as a Python list. Note: After a read, Python keeps a pointer to where you got to in the file. If you do another read, it will start from there. Try reading from the file that you created from the command line: \"), Document(metadata={'source': 'data/python book.pdf', 'page': 58}, page_content=\"Page 59 of 63 >>> f = open('temp.txt') >>> f.readline() 'This is the first line.\\\\n' >>> f.readlines() ['This is the second line.\\\\n', 'This is yet another boring line.\\\\n'] 11.4 Exercises The first two exercises expect you to output some mixed data to a file. We output the number of records as an integer ‘header’ to the file. These exercises are intended to give you some practice at detailed file IO. Do the first two exercises without using the NumPy savetxt() and loadtxt() functions. Then repeat the exercises using the NumPy functions. Refer back to section 4.2 to remind you how to do this. 11.4.1 Exercise 11.1 Write a program to calculate the sine of some angles from 0 to 100 degrees. (Remember to convert to radians before taking the sine). Use numpy arrays for this. Output the data to the screen so that you can see it as it is generated. Output the data to file. On the first line of the file, output the number of records that you are writing to file (100). This acts as a useful ‘header’ so that you can easily find out how much data is in the file. Output the data to file with one angle in degrees and its sin on each line. Separate the data with a comma. You can use any separator, but a comma is usual. You are creating a ‘comma separated variable (CSV) file. Look at the file you have produced and check it is correct. Model solution is in file exercise11.1.py on DUO. 11.4.2 Exercise 11.2 Write a program to read your angle and sin data from the file into numpy arrays and write every 10th angle and its sine to the screen with one angle and its sine on each line in the format: angle =  0.000 degrees      Sin = 0.000 angle = 10.000 degrees      Sin = 0.174 etc Read the ‘header’ to find out how much data is in the file and use this to size your arrays. Try to get the output data to line up as shown using the % format descriptor. Model solution is in file exercise11.2.py on DUO. 11.4.3 Exercise 11.3 Repeat exercise 11.1 using the NumPy savetxt() function but do not write the number of records as a header since this is not needed. Model solution is in file exercise11.3.py on DUO. 11.4.4 Exercise 11.4 Repeat exercise 11.2 using the NumPy loadtxt() function.  Model solution is in file exercise11.4.py on DUO. \"), Document(metadata={'source': 'data/python book.pdf', 'page': 59}, page_content='Page 60 of 63  12 Chapter 12 - Plotting Graphs Python does not have any facilities built in for drawing graphs. However, everyone wants to draw graphs so there are many modules around that do so. The matplotlib package is one of the most common and has been provided for you on the ITS Linux service. It goes by the name of  PyLab so, if you want a graph, just import pylab. The module provides a nice Graphical User Interface (GUI) for you to use to display your graph. Pylab just provides some higher level facilities such as the GUI. It imports matplotlib so just import pylab and you will get both. There is a good introduction to matplotlib on the web at: http://matplotlib.sourceforge.net/ Look at the ‘Gallery’ to see examples of what can be done using this package. As with all things Python, it is easy to draw a simple graph very quickly. The module provides all the extra facilities that you might need such as labels for axes, different coloured symbols, different types of plot etc. You can slowly learn these and use them to produce good looking clear displays of your data. Just a few of the basics will be enough for this course. It is worth gaining some experience with this package as you will be expected to use it to draw graphs in later courses in level 2 and in level 3. We do not explain any of the detail of PyLab here. You should get that from the documentation on the web. We include some general points of interest and an example for you to try. Plotting graphs is great fun but don’t spend too much time on it! 12.1 PyLab: The absolute Basics What do we always do with something new in Python? Try it at the command line. However, as we state in the next section, things like plotters often get confused if you run them in the IDLE environment. So do this at a Python prompt in your terminal window. (IE type python <myprog> at the Linux prompt). The basic plot command is plot(a,b) where a and b are lists (or arrays) of data to plot in x and y respectively.  >>> import pylab >>> a=[1,2,3] >>> b=[2,4,6] >>> pylab.plot(a,b) [<matplotlib.lines.Line2D instance at 0x01A03170>] >>> pylab.show() You should now see a plot popup something like this: '), Document(metadata={'source': 'data/python book.pdf', 'page': 60}, page_content='Page 61 of 63 \\n  This is an example of a Graphical User Interface or GUI. It has been written for you. You are just using it from your program. PyLab has plotted your two lists of data against each other.  Kill the GUI window and you should get your Python prompt back. Note: PyLab has been set up for you to work from a file rather than interactively from the command line so you should develop any real programs in a script file as usual. 12.2 GUIs – How do they work? The last thing you do in your program, if you are going to plot a graph, is to issue the command pylab.show(). This causes pylab to draw the Graphical User Interface (GUI) that is used to display your graph. It then enters an infinite loop waiting for you to use the controls on the GUI. Thus you won’t get your Python prompt back until you kill the GUI window. If you have problems, you can usually cause the program to end with a Control C (Hold down control and hit c). If you can’t get the prompt back in the IDLE Python shell, you will have to close it to stop the program running. As already stated, it is often better not to run such programs inside IDLE. It can get confused!  On Linux, you can run your program ‘in the background’. This is the usual way to run GUIs on Linux. Just add the & character to the end of the line when you run your program: python myprog.py & If you now hit the return key in your terminal window, you will get your prompt back. You '), Document(metadata={'source': 'data/python book.pdf', 'page': 61}, page_content='Page 62 of 63 can go on working with the GUI still up and running. You can’t do this in IDLE! Note: Writing your own GUIs (like PyLab) is great fun but outside the remit of this course. Chapter 12 of Hetland is a good introduction. To write a GUI requires an extra module called a ‘GUI builder’ which we have not installed for you on ITS Linux. wxPython is the current favourite. You can download it to your own computer if you want to try it. Warning: It can be addictive and time consuming! 12.3 Exercises 12.3.1 Exercise 12.1 Modify your program that writes a file of angles and their sines to cover the range 0 to 360 degrees. Modify your program that reads the file to plot this full cycle of the sine function using PyLab. You should give the axes legends and add a title to the graph. Output the graph to a .png file. This could then be added as a figure in a report written in Word or another word processor. Model solution is in file exercise12.1.py on DUO. '), Document(metadata={'source': 'data/python book.pdf', 'page': 62}, page_content='Page 63 of 63  13 Chapter 13 - Random Numbers  There are many problems in Physics where the Physics is well understood but describing an entire system that we wish to model is not possible analytically. In this situation, the Physicist will often resort to a method known as ‘Monte Carlo’. This involves using random numbers to repeatedly test what happens to, for example, a cosmic ray interacting with the Earth’s atmosphere. (It’s like throwing the dice in a casino, hence Monte Carlo). Python (like all languages) incorporates a “random” number generator. Of course such numbers cannot be truly random. Python can however generate a “pseudo-random” sequence of numbers. We will not address the use of random numbers in any detail here, but you should be aware that there is a module, called ‘random’, that provides random numbers in a wide variety of different ways. Import the random module and use dir(random) to see what calls are available. The module is described in detail in the Python documentation in the Module Index at: http://docs.python.org/modindex.html The most basic call is random.random(). This will return a random floating point number between 0.0 and 1.0. However, the most useful is probably random.uniform(<min>, <max>) which returns a random floating point number between min and max. The simplest distribution of random numbers you might want would be a ‘flat’ distribution. One could generate a large number of random numbers using say uniform(0.0, 100.0), bin them in some way and do some tests to see if they are truly random. One of the other most useful distributions of random numbers is a Gaussian distribution. The module random provides the function random.gauss( <mu>, <sigma>) to give you just such a distribution where mu is the mean and sigma is the standard deviation. 13.1 Exercises 13.1.1 Exercise 13.1 Write a program to request a number of Gaussian distributed random numbers. Put them in an array and make a histogram of the array. Use a mean of 100.0 and a standard deviation of 15.0. You can normalise the output so that the integral is 1.0 and the Y axis will represent probability. This also makes the y axis scaling easier for different numbers of random numbers. Hint: There is a pylab.hist() function to plot histograms.  Model solution is in file exercise13.1.py on DUO.   ')]\n",
      "63\n"
     ]
    }
   ],
   "source": [
    "print(docs)\n",
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'data/python book.pdf', 'page': 0}\n",
      "{'source': 'data/python book.pdf', 'page': 1}\n",
      "{'source': 'data/python book.pdf', 'page': 2}\n",
      "{'source': 'data/python book.pdf', 'page': 3}\n",
      "{'source': 'data/python book.pdf', 'page': 4}\n",
      "{'source': 'data/python book.pdf', 'page': 5}\n",
      "{'source': 'data/python book.pdf', 'page': 6}\n",
      "{'source': 'data/python book.pdf', 'page': 7}\n",
      "{'source': 'data/python book.pdf', 'page': 8}\n",
      "{'source': 'data/python book.pdf', 'page': 9}\n",
      "{'source': 'data/python book.pdf', 'page': 10}\n",
      "{'source': 'data/python book.pdf', 'page': 11}\n",
      "{'source': 'data/python book.pdf', 'page': 12}\n",
      "{'source': 'data/python book.pdf', 'page': 13}\n",
      "{'source': 'data/python book.pdf', 'page': 14}\n",
      "{'source': 'data/python book.pdf', 'page': 15}\n",
      "{'source': 'data/python book.pdf', 'page': 16}\n",
      "{'source': 'data/python book.pdf', 'page': 17}\n",
      "{'source': 'data/python book.pdf', 'page': 18}\n",
      "{'source': 'data/python book.pdf', 'page': 19}\n",
      "{'source': 'data/python book.pdf', 'page': 20}\n",
      "{'source': 'data/python book.pdf', 'page': 21}\n",
      "{'source': 'data/python book.pdf', 'page': 22}\n",
      "{'source': 'data/python book.pdf', 'page': 23}\n",
      "{'source': 'data/python book.pdf', 'page': 24}\n",
      "{'source': 'data/python book.pdf', 'page': 25}\n",
      "{'source': 'data/python book.pdf', 'page': 26}\n",
      "{'source': 'data/python book.pdf', 'page': 27}\n",
      "{'source': 'data/python book.pdf', 'page': 28}\n",
      "{'source': 'data/python book.pdf', 'page': 29}\n",
      "{'source': 'data/python book.pdf', 'page': 30}\n",
      "{'source': 'data/python book.pdf', 'page': 31}\n",
      "{'source': 'data/python book.pdf', 'page': 32}\n",
      "{'source': 'data/python book.pdf', 'page': 33}\n",
      "{'source': 'data/python book.pdf', 'page': 34}\n",
      "{'source': 'data/python book.pdf', 'page': 35}\n",
      "{'source': 'data/python book.pdf', 'page': 36}\n",
      "{'source': 'data/python book.pdf', 'page': 37}\n",
      "{'source': 'data/python book.pdf', 'page': 38}\n",
      "{'source': 'data/python book.pdf', 'page': 39}\n",
      "{'source': 'data/python book.pdf', 'page': 40}\n",
      "{'source': 'data/python book.pdf', 'page': 41}\n",
      "{'source': 'data/python book.pdf', 'page': 42}\n",
      "{'source': 'data/python book.pdf', 'page': 43}\n",
      "{'source': 'data/python book.pdf', 'page': 44}\n",
      "{'source': 'data/python book.pdf', 'page': 45}\n",
      "{'source': 'data/python book.pdf', 'page': 46}\n",
      "{'source': 'data/python book.pdf', 'page': 47}\n",
      "{'source': 'data/python book.pdf', 'page': 48}\n",
      "{'source': 'data/python book.pdf', 'page': 49}\n",
      "{'source': 'data/python book.pdf', 'page': 50}\n",
      "{'source': 'data/python book.pdf', 'page': 51}\n",
      "{'source': 'data/python book.pdf', 'page': 52}\n",
      "{'source': 'data/python book.pdf', 'page': 53}\n",
      "{'source': 'data/python book.pdf', 'page': 54}\n",
      "{'source': 'data/python book.pdf', 'page': 55}\n",
      "{'source': 'data/python book.pdf', 'page': 56}\n",
      "{'source': 'data/python book.pdf', 'page': 57}\n",
      "{'source': 'data/python book.pdf', 'page': 58}\n",
      "{'source': 'data/python book.pdf', 'page': 59}\n",
      "{'source': 'data/python book.pdf', 'page': 60}\n",
      "{'source': 'data/python book.pdf', 'page': 61}\n",
      "{'source': 'data/python book.pdf', 'page': 62}\n"
     ]
    }
   ],
   "source": [
    "for doc in docs:\n",
    "    print(doc.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lazy loading refers to a technique where documents or data are loaded only when they are actually needed, rather than loading everything upfront. This is particularly useful when dealing with large datasets or files where loading everything at once would be inefficient or resource-intensive.\n",
    "\n",
    "Lazy loading helps optimize performance by deferring the loading of documents until they are required, which can save both memory and processing time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object PyPDFLoader.lazy_load at 0x0000029F2D9AAAB0>\n"
     ]
    }
   ],
   "source": [
    "lazy_doc = loader.lazy_load()\n",
    "print(lazy_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'data/python book.pdf', 'page': 1}\n",
      "-----------Content----------\n",
      "Page 2 of 63 Table of Contents 1 Chapter 1 - Introduction.....................................................................................................6 2 Chapter 2 - Resources Required for the Course.................................................................8 2.1 Programming Language.............................................................................................8 2.2 Computer Operating Systems....................................................................................8 2.3 Additional Libraries (Modules).................................................................................8 2.4 Editors........................................................................................................................9 2.5 Where to do the Work................................................................................................9 2.6 Books.......................................................................................................................10 3 Chapter 3 - Getting Started..............................................................................................11 3.1 Numbers...................................................................................................................11 3.2 Assignments, Strings and Types..............................................................................12 3.2.1 A First Mention of Functions...........................................................................14 3.2.2 A Brief Mention of Methods............................................................................14 3.3 Complex numbers (Advanced Topic).......................................................................15 3.4 Errors and Exceptions..............................................................................................15 3.5 Precision and Overflow............................................................................................16 3.5.1 Precision...........................................................................................................16 3.5.2 Overflow – Large Numbers.............................................................................17 3.6 Getting Help.............................................................................................................17 4 Chapter 4 - Input and Output (IO)...................................................................................18 4.1 Screen Input/Output.................................................................................................18 4.1.1 Output..............................................................................................................18 4.1.2 The Format Conversion Specifier....................................................................18 4.1.3 Special Characters in Output...........................................................................19 4.1.4 Input.................................................................................................................19 4.2 File Input  and Output..............................................................................................20 4.2.1 Saving an Array to File....................................................................................20 4.2.2 Loading an Array from File.............................................................................21 5 Chapter 5 - Programs (Scripts)........................................................................................22 5.1 My First Program – ‘Hello world’...........................................................................22 5.2 Exercises..................................................................................................................22 5.2.1 Exercise 5.1......................................................................................................22 5.2.2 Exercise 5.2......................................................................................................23 \n"
     ]
    }
   ],
   "source": [
    "for doc in lazy_doc:\n",
    "    print(doc.metadata)\n",
    "    print(\"-----------Content----------\")\n",
    "    print(doc.page_content)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directory Data Loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can pass the parameter silent_errors to the DirectoryLoader to skip the files which could not be loaded and continue the load process.\n",
    "By default this uses the UnstructuredLoader class. To customize the loader, specify the loader class in the loader_cls kwarg. You can use any kind of data loader such as pythonloader, textloader, pdfloader and so on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 1959.96it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'data\\\\README.md'}, page_content='# Generative-AI\\n\\n```python\\nprint(\"Hello world\")\\n\\n```\\n\\n\\nUse document loaders to load data from a source as `Document`\\'s. A `Document` is a piece of text and associated metadata. For example, there are document loaders for loading a simple `.txt` file, for loading the text contents of any web page, or even for loading a transcript of a YouTube video.\\n\\nDocument loaders provide a \"load\" method for loading data as documents from a configured source. They optionally implement a \"lazy load\" as well for lazily loading data into memory.\\n')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "loader = DirectoryLoader(\"data/\", glob=\"**/*.md\", show_progress=True, silent_errors=True,loader_cls=TextLoader)\n",
    "docs = loader.load()\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CSV Data Loading\n",
    "\n",
    "Load a CSV file into a list of Documents.\n",
    "\n",
    "Each document represents one row of the CSV file. Every row is converted into a key/value pair and outputted to a new line in the document’s page_content.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'data/train_and_test2.csv', 'row': 0}, page_content='Passengerid: 1\\nAge: 22\\nFare: 7.25\\nSex: 0\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 1}, page_content='Passengerid: 2\\nAge: 38\\nFare: 71.2833\\nSex: 1\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 0\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 2}, page_content='Passengerid: 3\\nAge: 26\\nFare: 7.925\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 3}, page_content='Passengerid: 4\\nAge: 35\\nFare: 53.1\\nSex: 1\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 4}, page_content='Passengerid: 5\\nAge: 35\\nFare: 8.05\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 5}, page_content='Passengerid: 6\\nAge: 28\\nFare: 8.4583\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 1\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 6}, page_content='Passengerid: 7\\nAge: 54\\nFare: 51.8625\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 7}, page_content='Passengerid: 8\\nAge: 2\\nFare: 21.075\\nSex: 0\\nsibsp: 3\\nzero: 0\\nParch: 1\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 8}, page_content='Passengerid: 9\\nAge: 27\\nFare: 11.1333\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 2\\nPclass: 3\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 9}, page_content='Passengerid: 10\\nAge: 14\\nFare: 30.0708\\nSex: 1\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 0\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 10}, page_content='Passengerid: 11\\nAge: 4\\nFare: 16.7\\nSex: 1\\nsibsp: 1\\nzero: 0\\nParch: 1\\nPclass: 3\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 11}, page_content='Passengerid: 12\\nAge: 58\\nFare: 26.55\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 12}, page_content='Passengerid: 13\\nAge: 20\\nFare: 8.05\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 13}, page_content='Passengerid: 14\\nAge: 39\\nFare: 31.275\\nSex: 0\\nsibsp: 1\\nzero: 0\\nParch: 5\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 14}, page_content='Passengerid: 15\\nAge: 14\\nFare: 7.8542\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 15}, page_content='Passengerid: 16\\nAge: 55\\nFare: 16\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 16}, page_content='Passengerid: 17\\nAge: 2\\nFare: 29.125\\nSex: 0\\nsibsp: 4\\nzero: 0\\nParch: 1\\nPclass: 3\\nEmbarked: 1\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 17}, page_content='Passengerid: 18\\nAge: 28\\nFare: 13\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 18}, page_content='Passengerid: 19\\nAge: 31\\nFare: 18\\nSex: 1\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 19}, page_content='Passengerid: 20\\nAge: 28\\nFare: 7.225\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 0\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 20}, page_content='Passengerid: 21\\nAge: 35\\nFare: 26\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 21}, page_content='Passengerid: 22\\nAge: 34\\nFare: 13\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 22}, page_content='Passengerid: 23\\nAge: 15\\nFare: 8.0292\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 1\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 23}, page_content='Passengerid: 24\\nAge: 28\\nFare: 35.5\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 24}, page_content='Passengerid: 25\\nAge: 8\\nFare: 21.075\\nSex: 1\\nsibsp: 3\\nzero: 0\\nParch: 1\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 25}, page_content='Passengerid: 26\\nAge: 38\\nFare: 31.3875\\nSex: 1\\nsibsp: 1\\nzero: 0\\nParch: 5\\nPclass: 3\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 26}, page_content='Passengerid: 27\\nAge: 28\\nFare: 7.225\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 0\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 27}, page_content='Passengerid: 28\\nAge: 19\\nFare: 263\\nSex: 0\\nsibsp: 3\\nzero: 0\\nParch: 2\\nPclass: 1\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 28}, page_content='Passengerid: 29\\nAge: 28\\nFare: 7.8792\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 1\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 29}, page_content='Passengerid: 30\\nAge: 28\\nFare: 7.8958\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 30}, page_content='Passengerid: 31\\nAge: 40\\nFare: 27.7208\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 0\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 31}, page_content='Passengerid: 32\\nAge: 28\\nFare: 146.5208\\nSex: 1\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 0\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 32}, page_content='Passengerid: 33\\nAge: 28\\nFare: 7.75\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 1\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 33}, page_content='Passengerid: 34\\nAge: 66\\nFare: 10.5\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 34}, page_content='Passengerid: 35\\nAge: 28\\nFare: 82.1708\\nSex: 0\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 0\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 35}, page_content='Passengerid: 36\\nAge: 42\\nFare: 52\\nSex: 0\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 36}, page_content='Passengerid: 37\\nAge: 28\\nFare: 7.2292\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 0\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 37}, page_content='Passengerid: 38\\nAge: 21\\nFare: 8.05\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 38}, page_content='Passengerid: 39\\nAge: 18\\nFare: 18\\nSex: 1\\nsibsp: 2\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 39}, page_content='Passengerid: 40\\nAge: 14\\nFare: 11.2417\\nSex: 1\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 0\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 40}, page_content='Passengerid: 41\\nAge: 40\\nFare: 9.475\\nSex: 1\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 41}, page_content='Passengerid: 42\\nAge: 27\\nFare: 21\\nSex: 1\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 42}, page_content='Passengerid: 43\\nAge: 28\\nFare: 7.8958\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 0\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 43}, page_content='Passengerid: 44\\nAge: 3\\nFare: 41.5792\\nSex: 1\\nsibsp: 1\\nzero: 0\\nParch: 2\\nPclass: 2\\nEmbarked: 0\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 44}, page_content='Passengerid: 45\\nAge: 19\\nFare: 7.8792\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 1\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 45}, page_content='Passengerid: 46\\nAge: 28\\nFare: 8.05\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 46}, page_content='Passengerid: 47\\nAge: 28\\nFare: 15.5\\nSex: 0\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 1\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 47}, page_content='Passengerid: 48\\nAge: 28\\nFare: 7.75\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 1\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 48}, page_content='Passengerid: 49\\nAge: 28\\nFare: 21.6792\\nSex: 0\\nsibsp: 2\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 0\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 49}, page_content='Passengerid: 50\\nAge: 18\\nFare: 17.8\\nSex: 1\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 50}, page_content='Passengerid: 51\\nAge: 7\\nFare: 39.6875\\nSex: 0\\nsibsp: 4\\nzero: 0\\nParch: 1\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 51}, page_content='Passengerid: 52\\nAge: 21\\nFare: 7.8\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 52}, page_content='Passengerid: 53\\nAge: 49\\nFare: 76.7292\\nSex: 1\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 0\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 53}, page_content='Passengerid: 54\\nAge: 29\\nFare: 26\\nSex: 1\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 54}, page_content='Passengerid: 55\\nAge: 65\\nFare: 61.9792\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 1\\nPclass: 1\\nEmbarked: 0\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 55}, page_content='Passengerid: 56\\nAge: 28\\nFare: 35.5\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 56}, page_content='Passengerid: 57\\nAge: 21\\nFare: 10.5\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 57}, page_content='Passengerid: 58\\nAge: 28.5\\nFare: 7.2292\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 0\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 58}, page_content='Passengerid: 59\\nAge: 5\\nFare: 27.75\\nSex: 1\\nsibsp: 1\\nzero: 0\\nParch: 2\\nPclass: 2\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 59}, page_content='Passengerid: 60\\nAge: 11\\nFare: 46.9\\nSex: 0\\nsibsp: 5\\nzero: 0\\nParch: 2\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 60}, page_content='Passengerid: 61\\nAge: 22\\nFare: 7.2292\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 0\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 61}, page_content='Passengerid: 62\\nAge: 38\\nFare: 80\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: \\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 62}, page_content='Passengerid: 63\\nAge: 45\\nFare: 83.475\\nSex: 0\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 63}, page_content='Passengerid: 64\\nAge: 4\\nFare: 27.9\\nSex: 0\\nsibsp: 3\\nzero: 0\\nParch: 2\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 64}, page_content='Passengerid: 65\\nAge: 28\\nFare: 27.7208\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 0\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 65}, page_content='Passengerid: 66\\nAge: 28\\nFare: 15.2458\\nSex: 0\\nsibsp: 1\\nzero: 0\\nParch: 1\\nPclass: 3\\nEmbarked: 0\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 66}, page_content='Passengerid: 67\\nAge: 29\\nFare: 10.5\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 67}, page_content='Passengerid: 68\\nAge: 19\\nFare: 8.1583\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 68}, page_content='Passengerid: 69\\nAge: 17\\nFare: 7.925\\nSex: 1\\nsibsp: 4\\nzero: 0\\nParch: 2\\nPclass: 3\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 69}, page_content='Passengerid: 70\\nAge: 26\\nFare: 8.6625\\nSex: 0\\nsibsp: 2\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 70}, page_content='Passengerid: 71\\nAge: 32\\nFare: 10.5\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 71}, page_content='Passengerid: 72\\nAge: 16\\nFare: 46.9\\nSex: 1\\nsibsp: 5\\nzero: 0\\nParch: 2\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 72}, page_content='Passengerid: 73\\nAge: 21\\nFare: 73.5\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 73}, page_content='Passengerid: 74\\nAge: 26\\nFare: 14.4542\\nSex: 0\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 0\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 74}, page_content='Passengerid: 75\\nAge: 32\\nFare: 56.4958\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 75}, page_content='Passengerid: 76\\nAge: 25\\nFare: 7.65\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 76}, page_content='Passengerid: 77\\nAge: 28\\nFare: 7.8958\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 77}, page_content='Passengerid: 78\\nAge: 28\\nFare: 8.05\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 78}, page_content='Passengerid: 79\\nAge: 0.83\\nFare: 29\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 2\\nPclass: 2\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 79}, page_content='Passengerid: 80\\nAge: 30\\nFare: 12.475\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 80}, page_content='Passengerid: 81\\nAge: 22\\nFare: 9\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 81}, page_content='Passengerid: 82\\nAge: 29\\nFare: 9.5\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 82}, page_content='Passengerid: 83\\nAge: 28\\nFare: 7.7875\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 1\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 83}, page_content='Passengerid: 84\\nAge: 28\\nFare: 47.1\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 84}, page_content='Passengerid: 85\\nAge: 17\\nFare: 10.5\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 85}, page_content='Passengerid: 86\\nAge: 33\\nFare: 15.85\\nSex: 1\\nsibsp: 3\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 86}, page_content='Passengerid: 87\\nAge: 16\\nFare: 34.375\\nSex: 0\\nsibsp: 1\\nzero: 0\\nParch: 3\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 87}, page_content='Passengerid: 88\\nAge: 28\\nFare: 8.05\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 88}, page_content='Passengerid: 89\\nAge: 23\\nFare: 263\\nSex: 1\\nsibsp: 3\\nzero: 0\\nParch: 2\\nPclass: 1\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 89}, page_content='Passengerid: 90\\nAge: 24\\nFare: 8.05\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 90}, page_content='Passengerid: 91\\nAge: 29\\nFare: 8.05\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 91}, page_content='Passengerid: 92\\nAge: 20\\nFare: 7.8542\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 92}, page_content='Passengerid: 93\\nAge: 46\\nFare: 61.175\\nSex: 0\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 93}, page_content='Passengerid: 94\\nAge: 26\\nFare: 20.575\\nSex: 0\\nsibsp: 1\\nzero: 0\\nParch: 2\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 94}, page_content='Passengerid: 95\\nAge: 59\\nFare: 7.25\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 95}, page_content='Passengerid: 96\\nAge: 28\\nFare: 8.05\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 96}, page_content='Passengerid: 97\\nAge: 71\\nFare: 34.6542\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 0\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 97}, page_content='Passengerid: 98\\nAge: 23\\nFare: 63.3583\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 1\\nPclass: 1\\nEmbarked: 0\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 98}, page_content='Passengerid: 99\\nAge: 34\\nFare: 23\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 1\\nPclass: 2\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 99}, page_content='Passengerid: 100\\nAge: 34\\nFare: 26\\nSex: 0\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 100}, page_content='Passengerid: 101\\nAge: 28\\nFare: 7.8958\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 101}, page_content='Passengerid: 102\\nAge: 28\\nFare: 7.8958\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 102}, page_content='Passengerid: 103\\nAge: 21\\nFare: 77.2875\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 1\\nPclass: 1\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 103}, page_content='Passengerid: 104\\nAge: 33\\nFare: 8.6542\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 104}, page_content='Passengerid: 105\\nAge: 37\\nFare: 7.925\\nSex: 0\\nsibsp: 2\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 105}, page_content='Passengerid: 106\\nAge: 28\\nFare: 7.8958\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 106}, page_content='Passengerid: 107\\nAge: 21\\nFare: 7.65\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 107}, page_content='Passengerid: 108\\nAge: 28\\nFare: 7.775\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 108}, page_content='Passengerid: 109\\nAge: 38\\nFare: 7.8958\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 109}, page_content='Passengerid: 110\\nAge: 28\\nFare: 24.15\\nSex: 1\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 1\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 110}, page_content='Passengerid: 111\\nAge: 47\\nFare: 52\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 111}, page_content='Passengerid: 112\\nAge: 14.5\\nFare: 14.4542\\nSex: 1\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 0\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 112}, page_content='Passengerid: 113\\nAge: 22\\nFare: 8.05\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 113}, page_content='Passengerid: 114\\nAge: 20\\nFare: 9.825\\nSex: 1\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 114}, page_content='Passengerid: 115\\nAge: 17\\nFare: 14.4583\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 0\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 115}, page_content='Passengerid: 116\\nAge: 21\\nFare: 7.925\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 116}, page_content='Passengerid: 117\\nAge: 70.5\\nFare: 7.75\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 1\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 117}, page_content='Passengerid: 118\\nAge: 29\\nFare: 21\\nSex: 0\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 118}, page_content='Passengerid: 119\\nAge: 24\\nFare: 247.5208\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 1\\nPclass: 1\\nEmbarked: 0\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 119}, page_content='Passengerid: 120\\nAge: 2\\nFare: 31.275\\nSex: 1\\nsibsp: 4\\nzero: 0\\nParch: 2\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 120}, page_content='Passengerid: 121\\nAge: 21\\nFare: 73.5\\nSex: 0\\nsibsp: 2\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 121}, page_content='Passengerid: 122\\nAge: 28\\nFare: 8.05\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 122}, page_content='Passengerid: 123\\nAge: 32.5\\nFare: 30.0708\\nSex: 0\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 0\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 123}, page_content='Passengerid: 124\\nAge: 32.5\\nFare: 13\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 124}, page_content='Passengerid: 125\\nAge: 54\\nFare: 77.2875\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 1\\nPclass: 1\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 125}, page_content='Passengerid: 126\\nAge: 12\\nFare: 11.2417\\nSex: 0\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 0\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 126}, page_content='Passengerid: 127\\nAge: 28\\nFare: 7.75\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 1\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 127}, page_content='Passengerid: 128\\nAge: 24\\nFare: 7.1417\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 128}, page_content='Passengerid: 129\\nAge: 28\\nFare: 22.3583\\nSex: 1\\nsibsp: 1\\nzero: 0\\nParch: 1\\nPclass: 3\\nEmbarked: 0\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 129}, page_content='Passengerid: 130\\nAge: 45\\nFare: 6.975\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 130}, page_content='Passengerid: 131\\nAge: 33\\nFare: 7.8958\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 0\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 131}, page_content='Passengerid: 132\\nAge: 20\\nFare: 7.05\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 132}, page_content='Passengerid: 133\\nAge: 47\\nFare: 14.5\\nSex: 1\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 133}, page_content='Passengerid: 134\\nAge: 29\\nFare: 26\\nSex: 1\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 134}, page_content='Passengerid: 135\\nAge: 25\\nFare: 13\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 135}, page_content='Passengerid: 136\\nAge: 23\\nFare: 15.0458\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 0\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 136}, page_content='Passengerid: 137\\nAge: 19\\nFare: 26.2833\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 2\\nPclass: 1\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 137}, page_content='Passengerid: 138\\nAge: 37\\nFare: 53.1\\nSex: 0\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 138}, page_content='Passengerid: 139\\nAge: 16\\nFare: 9.2167\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 139}, page_content='Passengerid: 140\\nAge: 24\\nFare: 79.2\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 0\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 140}, page_content='Passengerid: 141\\nAge: 28\\nFare: 15.2458\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 2\\nPclass: 3\\nEmbarked: 0\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 141}, page_content='Passengerid: 142\\nAge: 22\\nFare: 7.75\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 142}, page_content='Passengerid: 143\\nAge: 24\\nFare: 15.85\\nSex: 1\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 143}, page_content='Passengerid: 144\\nAge: 19\\nFare: 6.75\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 1\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 144}, page_content='Passengerid: 145\\nAge: 18\\nFare: 11.5\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 145}, page_content='Passengerid: 146\\nAge: 19\\nFare: 36.75\\nSex: 0\\nsibsp: 1\\nzero: 0\\nParch: 1\\nPclass: 2\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 146}, page_content='Passengerid: 147\\nAge: 27\\nFare: 7.7958\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 147}, page_content='Passengerid: 148\\nAge: 9\\nFare: 34.375\\nSex: 1\\nsibsp: 2\\nzero: 0\\nParch: 2\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 148}, page_content='Passengerid: 149\\nAge: 36.5\\nFare: 26\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 2\\nPclass: 2\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 149}, page_content='Passengerid: 150\\nAge: 42\\nFare: 13\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 150}, page_content='Passengerid: 151\\nAge: 51\\nFare: 12.525\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 151}, page_content='Passengerid: 152\\nAge: 22\\nFare: 66.6\\nSex: 1\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 152}, page_content='Passengerid: 153\\nAge: 55.5\\nFare: 8.05\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 153}, page_content='Passengerid: 154\\nAge: 40.5\\nFare: 14.5\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 2\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 154}, page_content='Passengerid: 155\\nAge: 28\\nFare: 7.3125\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 155}, page_content='Passengerid: 156\\nAge: 51\\nFare: 61.3792\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 1\\nPclass: 1\\nEmbarked: 0\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 156}, page_content='Passengerid: 157\\nAge: 16\\nFare: 7.7333\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 1\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 157}, page_content='Passengerid: 158\\nAge: 30\\nFare: 8.05\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 158}, page_content='Passengerid: 159\\nAge: 28\\nFare: 8.6625\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 159}, page_content='Passengerid: 160\\nAge: 28\\nFare: 69.55\\nSex: 0\\nsibsp: 8\\nzero: 0\\nParch: 2\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 160}, page_content='Passengerid: 161\\nAge: 44\\nFare: 16.1\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 1\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 161}, page_content='Passengerid: 162\\nAge: 40\\nFare: 15.75\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 162}, page_content='Passengerid: 163\\nAge: 26\\nFare: 7.775\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 163}, page_content='Passengerid: 164\\nAge: 17\\nFare: 8.6625\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 164}, page_content='Passengerid: 165\\nAge: 1\\nFare: 39.6875\\nSex: 0\\nsibsp: 4\\nzero: 0\\nParch: 1\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 165}, page_content='Passengerid: 166\\nAge: 9\\nFare: 20.525\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 2\\nPclass: 3\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 166}, page_content='Passengerid: 167\\nAge: 28\\nFare: 55\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 1\\nPclass: 1\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 167}, page_content='Passengerid: 168\\nAge: 45\\nFare: 27.9\\nSex: 1\\nsibsp: 1\\nzero: 0\\nParch: 4\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 168}, page_content='Passengerid: 169\\nAge: 28\\nFare: 25.925\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 169}, page_content='Passengerid: 170\\nAge: 28\\nFare: 56.4958\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 170}, page_content='Passengerid: 171\\nAge: 61\\nFare: 33.5\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 171}, page_content='Passengerid: 172\\nAge: 4\\nFare: 29.125\\nSex: 0\\nsibsp: 4\\nzero: 0\\nParch: 1\\nPclass: 3\\nEmbarked: 1\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 172}, page_content='Passengerid: 173\\nAge: 1\\nFare: 11.1333\\nSex: 1\\nsibsp: 1\\nzero: 0\\nParch: 1\\nPclass: 3\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 173}, page_content='Passengerid: 174\\nAge: 21\\nFare: 7.925\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 174}, page_content='Passengerid: 175\\nAge: 56\\nFare: 30.6958\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 0\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 175}, page_content='Passengerid: 176\\nAge: 18\\nFare: 7.8542\\nSex: 0\\nsibsp: 1\\nzero: 0\\nParch: 1\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 176}, page_content='Passengerid: 177\\nAge: 28\\nFare: 25.4667\\nSex: 0\\nsibsp: 3\\nzero: 0\\nParch: 1\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 177}, page_content='Passengerid: 178\\nAge: 50\\nFare: 28.7125\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 0\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 178}, page_content='Passengerid: 179\\nAge: 30\\nFare: 13\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 179}, page_content='Passengerid: 180\\nAge: 36\\nFare: 0\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 180}, page_content='Passengerid: 181\\nAge: 28\\nFare: 69.55\\nSex: 1\\nsibsp: 8\\nzero: 0\\nParch: 2\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 181}, page_content='Passengerid: 182\\nAge: 28\\nFare: 15.05\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 0\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 182}, page_content='Passengerid: 183\\nAge: 9\\nFare: 31.3875\\nSex: 0\\nsibsp: 4\\nzero: 0\\nParch: 2\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 183}, page_content='Passengerid: 184\\nAge: 1\\nFare: 39\\nSex: 0\\nsibsp: 2\\nzero: 0\\nParch: 1\\nPclass: 2\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 184}, page_content='Passengerid: 185\\nAge: 4\\nFare: 22.025\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 2\\nPclass: 3\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 185}, page_content='Passengerid: 186\\nAge: 28\\nFare: 50\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 186}, page_content='Passengerid: 187\\nAge: 28\\nFare: 15.5\\nSex: 1\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 1\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 187}, page_content='Passengerid: 188\\nAge: 45\\nFare: 26.55\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 188}, page_content='Passengerid: 189\\nAge: 40\\nFare: 15.5\\nSex: 0\\nsibsp: 1\\nzero: 0\\nParch: 1\\nPclass: 3\\nEmbarked: 1\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 189}, page_content='Passengerid: 190\\nAge: 36\\nFare: 7.8958\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 190}, page_content='Passengerid: 191\\nAge: 32\\nFare: 13\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 191}, page_content='Passengerid: 192\\nAge: 19\\nFare: 13\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 192}, page_content='Passengerid: 193\\nAge: 19\\nFare: 7.8542\\nSex: 1\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 193}, page_content='Passengerid: 194\\nAge: 3\\nFare: 26\\nSex: 0\\nsibsp: 1\\nzero: 0\\nParch: 1\\nPclass: 2\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 194}, page_content='Passengerid: 195\\nAge: 44\\nFare: 27.7208\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 0\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 195}, page_content='Passengerid: 196\\nAge: 58\\nFare: 146.5208\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 0\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 196}, page_content='Passengerid: 197\\nAge: 28\\nFare: 7.75\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 1\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 197}, page_content='Passengerid: 198\\nAge: 42\\nFare: 8.4042\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 1\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 198}, page_content='Passengerid: 199\\nAge: 28\\nFare: 7.75\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 1\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 199}, page_content='Passengerid: 200\\nAge: 24\\nFare: 13\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 200}, page_content='Passengerid: 201\\nAge: 28\\nFare: 9.5\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 201}, page_content='Passengerid: 202\\nAge: 28\\nFare: 69.55\\nSex: 0\\nsibsp: 8\\nzero: 0\\nParch: 2\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 202}, page_content='Passengerid: 203\\nAge: 34\\nFare: 6.4958\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 203}, page_content='Passengerid: 204\\nAge: 45.5\\nFare: 7.225\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 0\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 204}, page_content='Passengerid: 205\\nAge: 18\\nFare: 8.05\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 205}, page_content='Passengerid: 206\\nAge: 2\\nFare: 10.4625\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 1\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 206}, page_content='Passengerid: 207\\nAge: 32\\nFare: 15.85\\nSex: 0\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 207}, page_content='Passengerid: 208\\nAge: 26\\nFare: 18.7875\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 0\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 208}, page_content='Passengerid: 209\\nAge: 16\\nFare: 7.75\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 1\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 209}, page_content='Passengerid: 210\\nAge: 40\\nFare: 31\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 0\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 210}, page_content='Passengerid: 211\\nAge: 24\\nFare: 7.05\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 211}, page_content='Passengerid: 212\\nAge: 35\\nFare: 21\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 212}, page_content='Passengerid: 213\\nAge: 22\\nFare: 7.25\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 213}, page_content='Passengerid: 214\\nAge: 30\\nFare: 13\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 214}, page_content='Passengerid: 215\\nAge: 28\\nFare: 7.75\\nSex: 0\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 1\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 215}, page_content='Passengerid: 216\\nAge: 31\\nFare: 113.275\\nSex: 1\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 0\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 216}, page_content='Passengerid: 217\\nAge: 27\\nFare: 7.925\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 217}, page_content='Passengerid: 218\\nAge: 42\\nFare: 27\\nSex: 0\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 218}, page_content='Passengerid: 219\\nAge: 32\\nFare: 76.2917\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 0\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 219}, page_content='Passengerid: 220\\nAge: 30\\nFare: 10.5\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 220}, page_content='Passengerid: 221\\nAge: 16\\nFare: 8.05\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 221}, page_content='Passengerid: 222\\nAge: 27\\nFare: 13\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 222}, page_content='Passengerid: 223\\nAge: 51\\nFare: 8.05\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 223}, page_content='Passengerid: 224\\nAge: 28\\nFare: 7.8958\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 224}, page_content='Passengerid: 225\\nAge: 38\\nFare: 90\\nSex: 0\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 225}, page_content='Passengerid: 226\\nAge: 22\\nFare: 9.35\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 226}, page_content='Passengerid: 227\\nAge: 19\\nFare: 10.5\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 227}, page_content='Passengerid: 228\\nAge: 20.5\\nFare: 7.25\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 228}, page_content='Passengerid: 229\\nAge: 18\\nFare: 13\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 229}, page_content='Passengerid: 230\\nAge: 28\\nFare: 25.4667\\nSex: 1\\nsibsp: 3\\nzero: 0\\nParch: 1\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 230}, page_content='Passengerid: 231\\nAge: 35\\nFare: 83.475\\nSex: 1\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 231}, page_content='Passengerid: 232\\nAge: 29\\nFare: 7.775\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 232}, page_content='Passengerid: 233\\nAge: 59\\nFare: 13.5\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 233}, page_content='Passengerid: 234\\nAge: 5\\nFare: 31.3875\\nSex: 1\\nsibsp: 4\\nzero: 0\\nParch: 2\\nPclass: 3\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 234}, page_content='Passengerid: 235\\nAge: 24\\nFare: 10.5\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 235}, page_content='Passengerid: 236\\nAge: 28\\nFare: 7.55\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 236}, page_content='Passengerid: 237\\nAge: 44\\nFare: 26\\nSex: 0\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 237}, page_content='Passengerid: 238\\nAge: 8\\nFare: 26.25\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 2\\nPclass: 2\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 238}, page_content='Passengerid: 239\\nAge: 19\\nFare: 10.5\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 239}, page_content='Passengerid: 240\\nAge: 33\\nFare: 12.275\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 240}, page_content='Passengerid: 241\\nAge: 28\\nFare: 14.4542\\nSex: 1\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 0\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 241}, page_content='Passengerid: 242\\nAge: 28\\nFare: 15.5\\nSex: 1\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 1\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 242}, page_content='Passengerid: 243\\nAge: 29\\nFare: 10.5\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 243}, page_content='Passengerid: 244\\nAge: 22\\nFare: 7.125\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 244}, page_content='Passengerid: 245\\nAge: 30\\nFare: 7.225\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 0\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 245}, page_content='Passengerid: 246\\nAge: 44\\nFare: 90\\nSex: 0\\nsibsp: 2\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 1\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 246}, page_content='Passengerid: 247\\nAge: 25\\nFare: 7.775\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 247}, page_content='Passengerid: 248\\nAge: 24\\nFare: 14.5\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 2\\nPclass: 2\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 248}, page_content='Passengerid: 249\\nAge: 37\\nFare: 52.5542\\nSex: 0\\nsibsp: 1\\nzero: 0\\nParch: 1\\nPclass: 1\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 249}, page_content='Passengerid: 250\\nAge: 54\\nFare: 26\\nSex: 0\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 250}, page_content='Passengerid: 251\\nAge: 28\\nFare: 7.25\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 251}, page_content='Passengerid: 252\\nAge: 29\\nFare: 10.4625\\nSex: 1\\nsibsp: 1\\nzero: 0\\nParch: 1\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 252}, page_content='Passengerid: 253\\nAge: 62\\nFare: 26.55\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 253}, page_content='Passengerid: 254\\nAge: 30\\nFare: 16.1\\nSex: 0\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 254}, page_content='Passengerid: 255\\nAge: 41\\nFare: 20.2125\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 2\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 255}, page_content='Passengerid: 256\\nAge: 29\\nFare: 15.2458\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 2\\nPclass: 3\\nEmbarked: 0\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 256}, page_content='Passengerid: 257\\nAge: 28\\nFare: 79.2\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 0\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 257}, page_content='Passengerid: 258\\nAge: 30\\nFare: 86.5\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 258}, page_content='Passengerid: 259\\nAge: 35\\nFare: 512.3292\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 0\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 259}, page_content='Passengerid: 260\\nAge: 50\\nFare: 26\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 1\\nPclass: 2\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 260}, page_content='Passengerid: 261\\nAge: 28\\nFare: 7.75\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 1\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 261}, page_content='Passengerid: 262\\nAge: 3\\nFare: 31.3875\\nSex: 0\\nsibsp: 4\\nzero: 0\\nParch: 2\\nPclass: 3\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 262}, page_content='Passengerid: 263\\nAge: 52\\nFare: 79.65\\nSex: 0\\nsibsp: 1\\nzero: 0\\nParch: 1\\nPclass: 1\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 263}, page_content='Passengerid: 264\\nAge: 40\\nFare: 0\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 264}, page_content='Passengerid: 265\\nAge: 28\\nFare: 7.75\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 1\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 265}, page_content='Passengerid: 266\\nAge: 36\\nFare: 10.5\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 266}, page_content='Passengerid: 267\\nAge: 16\\nFare: 39.6875\\nSex: 0\\nsibsp: 4\\nzero: 0\\nParch: 1\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 267}, page_content='Passengerid: 268\\nAge: 25\\nFare: 7.775\\nSex: 0\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 268}, page_content='Passengerid: 269\\nAge: 58\\nFare: 153.4625\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 1\\nPclass: 1\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 269}, page_content='Passengerid: 270\\nAge: 35\\nFare: 135.6333\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 270}, page_content='Passengerid: 271\\nAge: 28\\nFare: 31\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 271}, page_content='Passengerid: 272\\nAge: 25\\nFare: 0\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 272}, page_content='Passengerid: 273\\nAge: 41\\nFare: 19.5\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 1\\nPclass: 2\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 273}, page_content='Passengerid: 274\\nAge: 37\\nFare: 29.7\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 1\\nPclass: 1\\nEmbarked: 0\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 274}, page_content='Passengerid: 275\\nAge: 28\\nFare: 7.75\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 1\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 275}, page_content='Passengerid: 276\\nAge: 63\\nFare: 77.9583\\nSex: 1\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 276}, page_content='Passengerid: 277\\nAge: 45\\nFare: 7.75\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 277}, page_content='Passengerid: 278\\nAge: 28\\nFare: 0\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 278}, page_content='Passengerid: 279\\nAge: 7\\nFare: 29.125\\nSex: 0\\nsibsp: 4\\nzero: 0\\nParch: 1\\nPclass: 3\\nEmbarked: 1\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 279}, page_content='Passengerid: 280\\nAge: 35\\nFare: 20.25\\nSex: 1\\nsibsp: 1\\nzero: 0\\nParch: 1\\nPclass: 3\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 280}, page_content='Passengerid: 281\\nAge: 65\\nFare: 7.75\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 1\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 281}, page_content='Passengerid: 282\\nAge: 28\\nFare: 7.8542\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 282}, page_content='Passengerid: 283\\nAge: 16\\nFare: 9.5\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 283}, page_content='Passengerid: 284\\nAge: 19\\nFare: 8.05\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 284}, page_content='Passengerid: 285\\nAge: 28\\nFare: 26\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 285}, page_content='Passengerid: 286\\nAge: 33\\nFare: 8.6625\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 0\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 286}, page_content='Passengerid: 287\\nAge: 30\\nFare: 9.5\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 287}, page_content='Passengerid: 288\\nAge: 22\\nFare: 7.8958\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 288}, page_content='Passengerid: 289\\nAge: 42\\nFare: 13\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 289}, page_content='Passengerid: 290\\nAge: 22\\nFare: 7.75\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 1\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 290}, page_content='Passengerid: 291\\nAge: 26\\nFare: 78.85\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 291}, page_content='Passengerid: 292\\nAge: 19\\nFare: 91.0792\\nSex: 1\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 0\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 292}, page_content='Passengerid: 293\\nAge: 36\\nFare: 12.875\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 0\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 293}, page_content='Passengerid: 294\\nAge: 24\\nFare: 8.85\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 294}, page_content='Passengerid: 295\\nAge: 24\\nFare: 7.8958\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 295}, page_content='Passengerid: 296\\nAge: 28\\nFare: 27.7208\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 0\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 296}, page_content='Passengerid: 297\\nAge: 23.5\\nFare: 7.2292\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 0\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 297}, page_content='Passengerid: 298\\nAge: 2\\nFare: 151.55\\nSex: 1\\nsibsp: 1\\nzero: 0\\nParch: 2\\nPclass: 1\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 298}, page_content='Passengerid: 299\\nAge: 28\\nFare: 30.5\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 299}, page_content='Passengerid: 300\\nAge: 50\\nFare: 247.5208\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 1\\nPclass: 1\\nEmbarked: 0\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 300}, page_content='Passengerid: 301\\nAge: 28\\nFare: 7.75\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 1\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 301}, page_content='Passengerid: 302\\nAge: 28\\nFare: 23.25\\nSex: 0\\nsibsp: 2\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 1\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 302}, page_content='Passengerid: 303\\nAge: 19\\nFare: 0\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 303}, page_content='Passengerid: 304\\nAge: 28\\nFare: 12.35\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 1\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 304}, page_content='Passengerid: 305\\nAge: 28\\nFare: 8.05\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 305}, page_content='Passengerid: 306\\nAge: 0.92\\nFare: 151.55\\nSex: 0\\nsibsp: 1\\nzero: 0\\nParch: 2\\nPclass: 1\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 306}, page_content='Passengerid: 307\\nAge: 28\\nFare: 110.8833\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 0\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 307}, page_content='Passengerid: 308\\nAge: 17\\nFare: 108.9\\nSex: 1\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 0\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 308}, page_content='Passengerid: 309\\nAge: 30\\nFare: 24\\nSex: 0\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 0\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 309}, page_content='Passengerid: 310\\nAge: 30\\nFare: 56.9292\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 0\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 310}, page_content='Passengerid: 311\\nAge: 24\\nFare: 83.1583\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 0\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 311}, page_content='Passengerid: 312\\nAge: 18\\nFare: 262.375\\nSex: 1\\nsibsp: 2\\nzero: 0\\nParch: 2\\nPclass: 1\\nEmbarked: 0\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 312}, page_content='Passengerid: 313\\nAge: 26\\nFare: 26\\nSex: 1\\nsibsp: 1\\nzero: 0\\nParch: 1\\nPclass: 2\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 313}, page_content='Passengerid: 314\\nAge: 28\\nFare: 7.8958\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 314}, page_content='Passengerid: 315\\nAge: 43\\nFare: 26.25\\nSex: 0\\nsibsp: 1\\nzero: 0\\nParch: 1\\nPclass: 2\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 315}, page_content='Passengerid: 316\\nAge: 26\\nFare: 7.8542\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 316}, page_content='Passengerid: 317\\nAge: 24\\nFare: 26\\nSex: 1\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 317}, page_content='Passengerid: 318\\nAge: 54\\nFare: 14\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 318}, page_content='Passengerid: 319\\nAge: 31\\nFare: 164.8667\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 2\\nPclass: 1\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 319}, page_content='Passengerid: 320\\nAge: 40\\nFare: 134.5\\nSex: 1\\nsibsp: 1\\nzero: 0\\nParch: 1\\nPclass: 1\\nEmbarked: 0\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 320}, page_content='Passengerid: 321\\nAge: 22\\nFare: 7.25\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 321}, page_content='Passengerid: 322\\nAge: 27\\nFare: 7.8958\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 322}, page_content='Passengerid: 323\\nAge: 30\\nFare: 12.35\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 1\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 323}, page_content='Passengerid: 324\\nAge: 22\\nFare: 29\\nSex: 1\\nsibsp: 1\\nzero: 0\\nParch: 1\\nPclass: 2\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 324}, page_content='Passengerid: 325\\nAge: 28\\nFare: 69.55\\nSex: 0\\nsibsp: 8\\nzero: 0\\nParch: 2\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 325}, page_content='Passengerid: 326\\nAge: 36\\nFare: 135.6333\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 0\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 326}, page_content='Passengerid: 327\\nAge: 61\\nFare: 6.2375\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 327}, page_content='Passengerid: 328\\nAge: 36\\nFare: 13\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 328}, page_content='Passengerid: 329\\nAge: 31\\nFare: 20.525\\nSex: 1\\nsibsp: 1\\nzero: 0\\nParch: 1\\nPclass: 3\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 329}, page_content='Passengerid: 330\\nAge: 16\\nFare: 57.9792\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 1\\nPclass: 1\\nEmbarked: 0\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 330}, page_content='Passengerid: 331\\nAge: 28\\nFare: 23.25\\nSex: 1\\nsibsp: 2\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 1\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 331}, page_content='Passengerid: 332\\nAge: 45.5\\nFare: 28.5\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 332}, page_content='Passengerid: 333\\nAge: 38\\nFare: 153.4625\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 1\\nPclass: 1\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 333}, page_content='Passengerid: 334\\nAge: 16\\nFare: 18\\nSex: 0\\nsibsp: 2\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 334}, page_content='Passengerid: 335\\nAge: 28\\nFare: 133.65\\nSex: 1\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 335}, page_content='Passengerid: 336\\nAge: 28\\nFare: 7.8958\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 336}, page_content='Passengerid: 337\\nAge: 29\\nFare: 66.6\\nSex: 0\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 337}, page_content='Passengerid: 338\\nAge: 41\\nFare: 134.5\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 0\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 338}, page_content='Passengerid: 339\\nAge: 45\\nFare: 8.05\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 339}, page_content='Passengerid: 340\\nAge: 45\\nFare: 35.5\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 340}, page_content='Passengerid: 341\\nAge: 2\\nFare: 26\\nSex: 0\\nsibsp: 1\\nzero: 0\\nParch: 1\\nPclass: 2\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 341}, page_content='Passengerid: 342\\nAge: 24\\nFare: 263\\nSex: 1\\nsibsp: 3\\nzero: 0\\nParch: 2\\nPclass: 1\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 342}, page_content='Passengerid: 343\\nAge: 28\\nFare: 13\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 343}, page_content='Passengerid: 344\\nAge: 25\\nFare: 13\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 344}, page_content='Passengerid: 345\\nAge: 36\\nFare: 13\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 345}, page_content='Passengerid: 346\\nAge: 24\\nFare: 13\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 346}, page_content='Passengerid: 347\\nAge: 40\\nFare: 13\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 347}, page_content='Passengerid: 348\\nAge: 28\\nFare: 16.1\\nSex: 1\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 348}, page_content='Passengerid: 349\\nAge: 3\\nFare: 15.9\\nSex: 0\\nsibsp: 1\\nzero: 0\\nParch: 1\\nPclass: 3\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 349}, page_content='Passengerid: 350\\nAge: 42\\nFare: 8.6625\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 350}, page_content='Passengerid: 351\\nAge: 23\\nFare: 9.225\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 351}, page_content='Passengerid: 352\\nAge: 28\\nFare: 35\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 352}, page_content='Passengerid: 353\\nAge: 15\\nFare: 7.2292\\nSex: 0\\nsibsp: 1\\nzero: 0\\nParch: 1\\nPclass: 3\\nEmbarked: 0\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 353}, page_content='Passengerid: 354\\nAge: 25\\nFare: 17.8\\nSex: 0\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 354}, page_content='Passengerid: 355\\nAge: 28\\nFare: 7.225\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 0\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 355}, page_content='Passengerid: 356\\nAge: 28\\nFare: 9.5\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 356}, page_content='Passengerid: 357\\nAge: 22\\nFare: 55\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 1\\nPclass: 1\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 357}, page_content='Passengerid: 358\\nAge: 38\\nFare: 13\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 358}, page_content='Passengerid: 359\\nAge: 28\\nFare: 7.8792\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 1\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 359}, page_content='Passengerid: 360\\nAge: 28\\nFare: 7.8792\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 1\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 360}, page_content='Passengerid: 361\\nAge: 40\\nFare: 27.9\\nSex: 0\\nsibsp: 1\\nzero: 0\\nParch: 4\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 361}, page_content='Passengerid: 362\\nAge: 29\\nFare: 27.7208\\nSex: 0\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 0\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 362}, page_content='Passengerid: 363\\nAge: 45\\nFare: 14.4542\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 1\\nPclass: 3\\nEmbarked: 0\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 363}, page_content='Passengerid: 364\\nAge: 35\\nFare: 7.05\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 364}, page_content='Passengerid: 365\\nAge: 28\\nFare: 15.5\\nSex: 0\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 1\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 365}, page_content='Passengerid: 366\\nAge: 30\\nFare: 7.25\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 366}, page_content='Passengerid: 367\\nAge: 60\\nFare: 75.25\\nSex: 1\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 0\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 367}, page_content='Passengerid: 368\\nAge: 28\\nFare: 7.2292\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 0\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 368}, page_content='Passengerid: 369\\nAge: 28\\nFare: 7.75\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 1\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 369}, page_content='Passengerid: 370\\nAge: 24\\nFare: 69.3\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 0\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 370}, page_content='Passengerid: 371\\nAge: 25\\nFare: 55.4417\\nSex: 0\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 0\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 371}, page_content='Passengerid: 372\\nAge: 18\\nFare: 6.4958\\nSex: 0\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 372}, page_content='Passengerid: 373\\nAge: 19\\nFare: 8.05\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 373}, page_content='Passengerid: 374\\nAge: 22\\nFare: 135.6333\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 0\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 374}, page_content='Passengerid: 375\\nAge: 3\\nFare: 21.075\\nSex: 1\\nsibsp: 3\\nzero: 0\\nParch: 1\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 375}, page_content='Passengerid: 376\\nAge: 28\\nFare: 82.1708\\nSex: 1\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 0\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 376}, page_content='Passengerid: 377\\nAge: 22\\nFare: 7.25\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 377}, page_content='Passengerid: 378\\nAge: 27\\nFare: 211.5\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 2\\nPclass: 1\\nEmbarked: 0\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 378}, page_content='Passengerid: 379\\nAge: 20\\nFare: 4.0125\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 0\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 379}, page_content='Passengerid: 380\\nAge: 19\\nFare: 7.775\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 380}, page_content='Passengerid: 381\\nAge: 42\\nFare: 227.525\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 0\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 381}, page_content='Passengerid: 382\\nAge: 1\\nFare: 15.7417\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 2\\nPclass: 3\\nEmbarked: 0\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 382}, page_content='Passengerid: 383\\nAge: 32\\nFare: 7.925\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 383}, page_content='Passengerid: 384\\nAge: 35\\nFare: 52\\nSex: 1\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 384}, page_content='Passengerid: 385\\nAge: 28\\nFare: 7.8958\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 385}, page_content='Passengerid: 386\\nAge: 18\\nFare: 73.5\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 386}, page_content='Passengerid: 387\\nAge: 1\\nFare: 46.9\\nSex: 0\\nsibsp: 5\\nzero: 0\\nParch: 2\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 387}, page_content='Passengerid: 388\\nAge: 36\\nFare: 13\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 388}, page_content='Passengerid: 389\\nAge: 28\\nFare: 7.7292\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 1\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 389}, page_content='Passengerid: 390\\nAge: 17\\nFare: 12\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 0\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 390}, page_content='Passengerid: 391\\nAge: 36\\nFare: 120\\nSex: 0\\nsibsp: 1\\nzero: 0\\nParch: 2\\nPclass: 1\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 391}, page_content='Passengerid: 392\\nAge: 21\\nFare: 7.7958\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 392}, page_content='Passengerid: 393\\nAge: 28\\nFare: 7.925\\nSex: 0\\nsibsp: 2\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 393}, page_content='Passengerid: 394\\nAge: 23\\nFare: 113.275\\nSex: 1\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 0\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 394}, page_content='Passengerid: 395\\nAge: 24\\nFare: 16.7\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 2\\nPclass: 3\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 395}, page_content='Passengerid: 396\\nAge: 22\\nFare: 7.7958\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 396}, page_content='Passengerid: 397\\nAge: 31\\nFare: 7.8542\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 397}, page_content='Passengerid: 398\\nAge: 46\\nFare: 26\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 398}, page_content='Passengerid: 399\\nAge: 23\\nFare: 10.5\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 399}, page_content='Passengerid: 400\\nAge: 28\\nFare: 12.65\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 400}, page_content='Passengerid: 401\\nAge: 39\\nFare: 7.925\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 401}, page_content='Passengerid: 402\\nAge: 26\\nFare: 8.05\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 402}, page_content='Passengerid: 403\\nAge: 21\\nFare: 9.825\\nSex: 1\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 403}, page_content='Passengerid: 404\\nAge: 28\\nFare: 15.85\\nSex: 0\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 404}, page_content='Passengerid: 405\\nAge: 20\\nFare: 8.6625\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 405}, page_content='Passengerid: 406\\nAge: 34\\nFare: 21\\nSex: 0\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 406}, page_content='Passengerid: 407\\nAge: 51\\nFare: 7.75\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 407}, page_content='Passengerid: 408\\nAge: 3\\nFare: 18.75\\nSex: 0\\nsibsp: 1\\nzero: 0\\nParch: 1\\nPclass: 2\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 408}, page_content='Passengerid: 409\\nAge: 21\\nFare: 7.775\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 409}, page_content='Passengerid: 410\\nAge: 28\\nFare: 25.4667\\nSex: 1\\nsibsp: 3\\nzero: 0\\nParch: 1\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 410}, page_content='Passengerid: 411\\nAge: 28\\nFare: 7.8958\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 411}, page_content='Passengerid: 412\\nAge: 28\\nFare: 6.8583\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 1\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 412}, page_content='Passengerid: 413\\nAge: 33\\nFare: 90\\nSex: 1\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 1\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 413}, page_content='Passengerid: 414\\nAge: 28\\nFare: 0\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 414}, page_content='Passengerid: 415\\nAge: 44\\nFare: 7.925\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 415}, page_content='Passengerid: 416\\nAge: 28\\nFare: 8.05\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 416}, page_content='Passengerid: 417\\nAge: 34\\nFare: 32.5\\nSex: 1\\nsibsp: 1\\nzero: 0\\nParch: 1\\nPclass: 2\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 417}, page_content='Passengerid: 418\\nAge: 18\\nFare: 13\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 2\\nPclass: 2\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 418}, page_content='Passengerid: 419\\nAge: 30\\nFare: 13\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 419}, page_content='Passengerid: 420\\nAge: 10\\nFare: 24.15\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 2\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 420}, page_content='Passengerid: 421\\nAge: 28\\nFare: 7.8958\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 0\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 421}, page_content='Passengerid: 422\\nAge: 21\\nFare: 7.7333\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 1\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 422}, page_content='Passengerid: 423\\nAge: 29\\nFare: 7.875\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 423}, page_content='Passengerid: 424\\nAge: 28\\nFare: 14.4\\nSex: 1\\nsibsp: 1\\nzero: 0\\nParch: 1\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 424}, page_content='Passengerid: 425\\nAge: 18\\nFare: 20.2125\\nSex: 0\\nsibsp: 1\\nzero: 0\\nParch: 1\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 425}, page_content='Passengerid: 426\\nAge: 28\\nFare: 7.25\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 426}, page_content='Passengerid: 427\\nAge: 28\\nFare: 26\\nSex: 1\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 427}, page_content='Passengerid: 428\\nAge: 19\\nFare: 26\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 428}, page_content='Passengerid: 429\\nAge: 28\\nFare: 7.75\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 1\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 429}, page_content='Passengerid: 430\\nAge: 32\\nFare: 8.05\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 430}, page_content='Passengerid: 431\\nAge: 28\\nFare: 26.55\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 431}, page_content='Passengerid: 432\\nAge: 28\\nFare: 16.1\\nSex: 1\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 432}, page_content='Passengerid: 433\\nAge: 42\\nFare: 26\\nSex: 1\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 433}, page_content='Passengerid: 434\\nAge: 17\\nFare: 7.125\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 434}, page_content='Passengerid: 435\\nAge: 50\\nFare: 55.9\\nSex: 0\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 435}, page_content='Passengerid: 436\\nAge: 14\\nFare: 120\\nSex: 1\\nsibsp: 1\\nzero: 0\\nParch: 2\\nPclass: 1\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 436}, page_content='Passengerid: 437\\nAge: 21\\nFare: 34.375\\nSex: 1\\nsibsp: 2\\nzero: 0\\nParch: 2\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 437}, page_content='Passengerid: 438\\nAge: 24\\nFare: 18.75\\nSex: 1\\nsibsp: 2\\nzero: 0\\nParch: 3\\nPclass: 2\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 438}, page_content='Passengerid: 439\\nAge: 64\\nFare: 263\\nSex: 0\\nsibsp: 1\\nzero: 0\\nParch: 4\\nPclass: 1\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 439}, page_content='Passengerid: 440\\nAge: 31\\nFare: 10.5\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 440}, page_content='Passengerid: 441\\nAge: 45\\nFare: 26.25\\nSex: 1\\nsibsp: 1\\nzero: 0\\nParch: 1\\nPclass: 2\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 441}, page_content='Passengerid: 442\\nAge: 20\\nFare: 9.5\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 442}, page_content='Passengerid: 443\\nAge: 25\\nFare: 7.775\\nSex: 0\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 443}, page_content='Passengerid: 444\\nAge: 28\\nFare: 13\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 444}, page_content='Passengerid: 445\\nAge: 28\\nFare: 8.1125\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 445}, page_content='Passengerid: 446\\nAge: 4\\nFare: 81.8583\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 2\\nPclass: 1\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 446}, page_content='Passengerid: 447\\nAge: 13\\nFare: 19.5\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 1\\nPclass: 2\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 447}, page_content='Passengerid: 448\\nAge: 34\\nFare: 26.55\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 448}, page_content='Passengerid: 449\\nAge: 5\\nFare: 19.2583\\nSex: 1\\nsibsp: 2\\nzero: 0\\nParch: 1\\nPclass: 3\\nEmbarked: 0\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 449}, page_content='Passengerid: 450\\nAge: 52\\nFare: 30.5\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 450}, page_content='Passengerid: 451\\nAge: 36\\nFare: 27.75\\nSex: 0\\nsibsp: 1\\nzero: 0\\nParch: 2\\nPclass: 2\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 451}, page_content='Passengerid: 452\\nAge: 28\\nFare: 19.9667\\nSex: 0\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 452}, page_content='Passengerid: 453\\nAge: 30\\nFare: 27.75\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 0\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 453}, page_content='Passengerid: 454\\nAge: 49\\nFare: 89.1042\\nSex: 0\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 0\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 454}, page_content='Passengerid: 455\\nAge: 28\\nFare: 8.05\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 455}, page_content='Passengerid: 456\\nAge: 29\\nFare: 7.8958\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 0\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 456}, page_content='Passengerid: 457\\nAge: 65\\nFare: 26.55\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 457}, page_content='Passengerid: 458\\nAge: 28\\nFare: 51.8625\\nSex: 1\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 458}, page_content='Passengerid: 459\\nAge: 50\\nFare: 10.5\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 459}, page_content='Passengerid: 460\\nAge: 28\\nFare: 7.75\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 1\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 460}, page_content='Passengerid: 461\\nAge: 48\\nFare: 26.55\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 461}, page_content='Passengerid: 462\\nAge: 34\\nFare: 8.05\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 462}, page_content='Passengerid: 463\\nAge: 47\\nFare: 38.5\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 463}, page_content='Passengerid: 464\\nAge: 48\\nFare: 13\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 464}, page_content='Passengerid: 465\\nAge: 28\\nFare: 8.05\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 465}, page_content='Passengerid: 466\\nAge: 38\\nFare: 7.05\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 466}, page_content='Passengerid: 467\\nAge: 28\\nFare: 0\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 467}, page_content='Passengerid: 468\\nAge: 56\\nFare: 26.55\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 468}, page_content='Passengerid: 469\\nAge: 28\\nFare: 7.725\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 1\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 469}, page_content='Passengerid: 470\\nAge: 0.75\\nFare: 19.2583\\nSex: 1\\nsibsp: 2\\nzero: 0\\nParch: 1\\nPclass: 3\\nEmbarked: 0\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 470}, page_content='Passengerid: 471\\nAge: 28\\nFare: 7.25\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 471}, page_content='Passengerid: 472\\nAge: 38\\nFare: 8.6625\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 472}, page_content='Passengerid: 473\\nAge: 33\\nFare: 27.75\\nSex: 1\\nsibsp: 1\\nzero: 0\\nParch: 2\\nPclass: 2\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 473}, page_content='Passengerid: 474\\nAge: 23\\nFare: 13.7917\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 0\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 474}, page_content='Passengerid: 475\\nAge: 22\\nFare: 9.8375\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 475}, page_content='Passengerid: 476\\nAge: 28\\nFare: 52\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 476}, page_content='Passengerid: 477\\nAge: 34\\nFare: 21\\nSex: 0\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 477}, page_content='Passengerid: 478\\nAge: 29\\nFare: 7.0458\\nSex: 0\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 478}, page_content='Passengerid: 479\\nAge: 22\\nFare: 7.5208\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 479}, page_content='Passengerid: 480\\nAge: 2\\nFare: 12.2875\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 1\\nPclass: 3\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 480}, page_content='Passengerid: 481\\nAge: 9\\nFare: 46.9\\nSex: 0\\nsibsp: 5\\nzero: 0\\nParch: 2\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 481}, page_content='Passengerid: 482\\nAge: 28\\nFare: 0\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 482}, page_content='Passengerid: 483\\nAge: 50\\nFare: 8.05\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 483}, page_content='Passengerid: 484\\nAge: 63\\nFare: 9.5875\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 484}, page_content='Passengerid: 485\\nAge: 25\\nFare: 91.0792\\nSex: 0\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 0\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 485}, page_content='Passengerid: 486\\nAge: 28\\nFare: 25.4667\\nSex: 1\\nsibsp: 3\\nzero: 0\\nParch: 1\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 486}, page_content='Passengerid: 487\\nAge: 35\\nFare: 90\\nSex: 1\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 487}, page_content='Passengerid: 488\\nAge: 58\\nFare: 29.7\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 0\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 488}, page_content='Passengerid: 489\\nAge: 30\\nFare: 8.05\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 489}, page_content='Passengerid: 490\\nAge: 9\\nFare: 15.9\\nSex: 0\\nsibsp: 1\\nzero: 0\\nParch: 1\\nPclass: 3\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 490}, page_content='Passengerid: 491\\nAge: 28\\nFare: 19.9667\\nSex: 0\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 491}, page_content='Passengerid: 492\\nAge: 21\\nFare: 7.25\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 492}, page_content='Passengerid: 493\\nAge: 55\\nFare: 30.5\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 493}, page_content='Passengerid: 494\\nAge: 71\\nFare: 49.5042\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 0\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 494}, page_content='Passengerid: 495\\nAge: 21\\nFare: 8.05\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 495}, page_content='Passengerid: 496\\nAge: 28\\nFare: 14.4583\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 0\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 496}, page_content='Passengerid: 497\\nAge: 54\\nFare: 78.2667\\nSex: 1\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 0\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 497}, page_content='Passengerid: 498\\nAge: 28\\nFare: 15.1\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 498}, page_content='Passengerid: 499\\nAge: 25\\nFare: 151.55\\nSex: 1\\nsibsp: 1\\nzero: 0\\nParch: 2\\nPclass: 1\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 499}, page_content='Passengerid: 500\\nAge: 24\\nFare: 7.7958\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 500}, page_content='Passengerid: 501\\nAge: 17\\nFare: 8.6625\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 501}, page_content='Passengerid: 502\\nAge: 21\\nFare: 7.75\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 1\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 502}, page_content='Passengerid: 503\\nAge: 28\\nFare: 7.6292\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 1\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 503}, page_content='Passengerid: 504\\nAge: 37\\nFare: 9.5875\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 504}, page_content='Passengerid: 505\\nAge: 16\\nFare: 86.5\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 505}, page_content='Passengerid: 506\\nAge: 18\\nFare: 108.9\\nSex: 0\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 0\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 506}, page_content='Passengerid: 507\\nAge: 33\\nFare: 26\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 2\\nPclass: 2\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 507}, page_content='Passengerid: 508\\nAge: 28\\nFare: 26.55\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 508}, page_content='Passengerid: 509\\nAge: 28\\nFare: 22.525\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 509}, page_content='Passengerid: 510\\nAge: 26\\nFare: 56.4958\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 510}, page_content='Passengerid: 511\\nAge: 29\\nFare: 7.75\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 1\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 511}, page_content='Passengerid: 512\\nAge: 28\\nFare: 8.05\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 512}, page_content='Passengerid: 513\\nAge: 36\\nFare: 26.2875\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 513}, page_content='Passengerid: 514\\nAge: 54\\nFare: 59.4\\nSex: 1\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 0\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 514}, page_content='Passengerid: 515\\nAge: 24\\nFare: 7.4958\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 515}, page_content='Passengerid: 516\\nAge: 47\\nFare: 34.0208\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 516}, page_content='Passengerid: 517\\nAge: 34\\nFare: 10.5\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 517}, page_content='Passengerid: 518\\nAge: 28\\nFare: 24.15\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 1\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 518}, page_content='Passengerid: 519\\nAge: 36\\nFare: 26\\nSex: 1\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 519}, page_content='Passengerid: 520\\nAge: 32\\nFare: 7.8958\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 520}, page_content='Passengerid: 521\\nAge: 30\\nFare: 93.5\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 521}, page_content='Passengerid: 522\\nAge: 22\\nFare: 7.8958\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 522}, page_content='Passengerid: 523\\nAge: 28\\nFare: 7.225\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 0\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 523}, page_content='Passengerid: 524\\nAge: 44\\nFare: 57.9792\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 1\\nPclass: 1\\nEmbarked: 0\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 524}, page_content='Passengerid: 525\\nAge: 28\\nFare: 7.2292\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 0\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 525}, page_content='Passengerid: 526\\nAge: 40.5\\nFare: 7.75\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 1\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 526}, page_content='Passengerid: 527\\nAge: 50\\nFare: 10.5\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 527}, page_content='Passengerid: 528\\nAge: 28\\nFare: 221.7792\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 528}, page_content='Passengerid: 529\\nAge: 39\\nFare: 7.925\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 529}, page_content='Passengerid: 530\\nAge: 23\\nFare: 11.5\\nSex: 0\\nsibsp: 2\\nzero: 0\\nParch: 1\\nPclass: 2\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 530}, page_content='Passengerid: 531\\nAge: 2\\nFare: 26\\nSex: 1\\nsibsp: 1\\nzero: 0\\nParch: 1\\nPclass: 2\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 531}, page_content='Passengerid: 532\\nAge: 28\\nFare: 7.2292\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 0\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 532}, page_content='Passengerid: 533\\nAge: 17\\nFare: 7.2292\\nSex: 0\\nsibsp: 1\\nzero: 0\\nParch: 1\\nPclass: 3\\nEmbarked: 0\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 533}, page_content='Passengerid: 534\\nAge: 28\\nFare: 22.3583\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 2\\nPclass: 3\\nEmbarked: 0\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 534}, page_content='Passengerid: 535\\nAge: 30\\nFare: 8.6625\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 535}, page_content='Passengerid: 536\\nAge: 7\\nFare: 26.25\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 2\\nPclass: 2\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 536}, page_content='Passengerid: 537\\nAge: 45\\nFare: 26.55\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 537}, page_content='Passengerid: 538\\nAge: 30\\nFare: 106.425\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 0\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 538}, page_content='Passengerid: 539\\nAge: 28\\nFare: 14.5\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 539}, page_content='Passengerid: 540\\nAge: 22\\nFare: 49.5\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 2\\nPclass: 1\\nEmbarked: 0\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 540}, page_content='Passengerid: 541\\nAge: 36\\nFare: 71\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 2\\nPclass: 1\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 541}, page_content='Passengerid: 542\\nAge: 9\\nFare: 31.275\\nSex: 1\\nsibsp: 4\\nzero: 0\\nParch: 2\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 542}, page_content='Passengerid: 543\\nAge: 11\\nFare: 31.275\\nSex: 1\\nsibsp: 4\\nzero: 0\\nParch: 2\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 543}, page_content='Passengerid: 544\\nAge: 32\\nFare: 26\\nSex: 0\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 544}, page_content='Passengerid: 545\\nAge: 50\\nFare: 106.425\\nSex: 0\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 0\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 545}, page_content='Passengerid: 546\\nAge: 64\\nFare: 26\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 546}, page_content='Passengerid: 547\\nAge: 19\\nFare: 26\\nSex: 1\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 547}, page_content='Passengerid: 548\\nAge: 28\\nFare: 13.8625\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 0\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 548}, page_content='Passengerid: 549\\nAge: 33\\nFare: 20.525\\nSex: 0\\nsibsp: 1\\nzero: 0\\nParch: 1\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 549}, page_content='Passengerid: 550\\nAge: 8\\nFare: 36.75\\nSex: 0\\nsibsp: 1\\nzero: 0\\nParch: 1\\nPclass: 2\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 550}, page_content='Passengerid: 551\\nAge: 17\\nFare: 110.8833\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 2\\nPclass: 1\\nEmbarked: 0\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 551}, page_content='Passengerid: 552\\nAge: 27\\nFare: 26\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 552}, page_content='Passengerid: 553\\nAge: 28\\nFare: 7.8292\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 1\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 553}, page_content='Passengerid: 554\\nAge: 22\\nFare: 7.225\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 0\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 554}, page_content='Passengerid: 555\\nAge: 22\\nFare: 7.775\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 555}, page_content='Passengerid: 556\\nAge: 62\\nFare: 26.55\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 556}, page_content='Passengerid: 557\\nAge: 48\\nFare: 39.6\\nSex: 1\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 0\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 557}, page_content='Passengerid: 558\\nAge: 28\\nFare: 227.525\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 0\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 558}, page_content='Passengerid: 559\\nAge: 39\\nFare: 79.65\\nSex: 1\\nsibsp: 1\\nzero: 0\\nParch: 1\\nPclass: 1\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 559}, page_content='Passengerid: 560\\nAge: 36\\nFare: 17.4\\nSex: 1\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 560}, page_content='Passengerid: 561\\nAge: 28\\nFare: 7.75\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 1\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 561}, page_content='Passengerid: 562\\nAge: 40\\nFare: 7.8958\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 562}, page_content='Passengerid: 563\\nAge: 28\\nFare: 13.5\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 563}, page_content='Passengerid: 564\\nAge: 28\\nFare: 8.05\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 564}, page_content='Passengerid: 565\\nAge: 28\\nFare: 8.05\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 565}, page_content='Passengerid: 566\\nAge: 24\\nFare: 24.15\\nSex: 0\\nsibsp: 2\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 566}, page_content='Passengerid: 567\\nAge: 19\\nFare: 7.8958\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 567}, page_content='Passengerid: 568\\nAge: 29\\nFare: 21.075\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 4\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 568}, page_content='Passengerid: 569\\nAge: 28\\nFare: 7.2292\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 0\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 569}, page_content='Passengerid: 570\\nAge: 32\\nFare: 7.8542\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 570}, page_content='Passengerid: 571\\nAge: 62\\nFare: 10.5\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 571}, page_content='Passengerid: 572\\nAge: 53\\nFare: 51.4792\\nSex: 1\\nsibsp: 2\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 572}, page_content='Passengerid: 573\\nAge: 36\\nFare: 26.3875\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 573}, page_content='Passengerid: 574\\nAge: 28\\nFare: 7.75\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 1\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 574}, page_content='Passengerid: 575\\nAge: 16\\nFare: 8.05\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 575}, page_content='Passengerid: 576\\nAge: 19\\nFare: 14.5\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 576}, page_content='Passengerid: 577\\nAge: 34\\nFare: 13\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 577}, page_content='Passengerid: 578\\nAge: 39\\nFare: 55.9\\nSex: 1\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 578}, page_content='Passengerid: 579\\nAge: 28\\nFare: 14.4583\\nSex: 1\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 0\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 579}, page_content='Passengerid: 580\\nAge: 32\\nFare: 7.925\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 580}, page_content='Passengerid: 581\\nAge: 25\\nFare: 30\\nSex: 1\\nsibsp: 1\\nzero: 0\\nParch: 1\\nPclass: 2\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 581}, page_content='Passengerid: 582\\nAge: 39\\nFare: 110.8833\\nSex: 1\\nsibsp: 1\\nzero: 0\\nParch: 1\\nPclass: 1\\nEmbarked: 0\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 582}, page_content='Passengerid: 583\\nAge: 54\\nFare: 26\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 583}, page_content='Passengerid: 584\\nAge: 36\\nFare: 40.125\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 0\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 584}, page_content='Passengerid: 585\\nAge: 28\\nFare: 8.7125\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 0\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 585}, page_content='Passengerid: 586\\nAge: 18\\nFare: 79.65\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 2\\nPclass: 1\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 586}, page_content='Passengerid: 587\\nAge: 47\\nFare: 15\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 587}, page_content='Passengerid: 588\\nAge: 60\\nFare: 79.2\\nSex: 0\\nsibsp: 1\\nzero: 0\\nParch: 1\\nPclass: 1\\nEmbarked: 0\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 588}, page_content='Passengerid: 589\\nAge: 22\\nFare: 8.05\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 589}, page_content='Passengerid: 590\\nAge: 28\\nFare: 8.05\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 590}, page_content='Passengerid: 591\\nAge: 35\\nFare: 7.125\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 591}, page_content='Passengerid: 592\\nAge: 52\\nFare: 78.2667\\nSex: 1\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 0\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 592}, page_content='Passengerid: 593\\nAge: 47\\nFare: 7.25\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 593}, page_content='Passengerid: 594\\nAge: 28\\nFare: 7.75\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 2\\nPclass: 3\\nEmbarked: 1\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 594}, page_content='Passengerid: 595\\nAge: 37\\nFare: 26\\nSex: 0\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 595}, page_content='Passengerid: 596\\nAge: 36\\nFare: 24.15\\nSex: 0\\nsibsp: 1\\nzero: 0\\nParch: 1\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 596}, page_content='Passengerid: 597\\nAge: 28\\nFare: 33\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 597}, page_content='Passengerid: 598\\nAge: 49\\nFare: 0\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 598}, page_content='Passengerid: 599\\nAge: 28\\nFare: 7.225\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 0\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 599}, page_content='Passengerid: 600\\nAge: 49\\nFare: 56.9292\\nSex: 0\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 0\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 600}, page_content='Passengerid: 601\\nAge: 24\\nFare: 27\\nSex: 1\\nsibsp: 2\\nzero: 0\\nParch: 1\\nPclass: 2\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 601}, page_content='Passengerid: 602\\nAge: 28\\nFare: 7.8958\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 602}, page_content='Passengerid: 603\\nAge: 28\\nFare: 42.4\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 603}, page_content='Passengerid: 604\\nAge: 44\\nFare: 8.05\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 604}, page_content='Passengerid: 605\\nAge: 35\\nFare: 26.55\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 0\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 605}, page_content='Passengerid: 606\\nAge: 36\\nFare: 15.55\\nSex: 0\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 606}, page_content='Passengerid: 607\\nAge: 30\\nFare: 7.8958\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 607}, page_content='Passengerid: 608\\nAge: 27\\nFare: 30.5\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 608}, page_content='Passengerid: 609\\nAge: 22\\nFare: 41.5792\\nSex: 1\\nsibsp: 1\\nzero: 0\\nParch: 2\\nPclass: 2\\nEmbarked: 0\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 609}, page_content='Passengerid: 610\\nAge: 40\\nFare: 153.4625\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 610}, page_content='Passengerid: 611\\nAge: 39\\nFare: 31.275\\nSex: 1\\nsibsp: 1\\nzero: 0\\nParch: 5\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 611}, page_content='Passengerid: 612\\nAge: 28\\nFare: 7.05\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 612}, page_content='Passengerid: 613\\nAge: 28\\nFare: 15.5\\nSex: 1\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 1\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 613}, page_content='Passengerid: 614\\nAge: 28\\nFare: 7.75\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 1\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 614}, page_content='Passengerid: 615\\nAge: 35\\nFare: 8.05\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 615}, page_content='Passengerid: 616\\nAge: 24\\nFare: 65\\nSex: 1\\nsibsp: 1\\nzero: 0\\nParch: 2\\nPclass: 2\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 616}, page_content='Passengerid: 617\\nAge: 34\\nFare: 14.4\\nSex: 0\\nsibsp: 1\\nzero: 0\\nParch: 1\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 617}, page_content='Passengerid: 618\\nAge: 26\\nFare: 16.1\\nSex: 1\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 618}, page_content='Passengerid: 619\\nAge: 4\\nFare: 39\\nSex: 1\\nsibsp: 2\\nzero: 0\\nParch: 1\\nPclass: 2\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 619}, page_content='Passengerid: 620\\nAge: 26\\nFare: 10.5\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 620}, page_content='Passengerid: 621\\nAge: 27\\nFare: 14.4542\\nSex: 0\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 0\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 621}, page_content='Passengerid: 622\\nAge: 42\\nFare: 52.5542\\nSex: 0\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 622}, page_content='Passengerid: 623\\nAge: 20\\nFare: 15.7417\\nSex: 0\\nsibsp: 1\\nzero: 0\\nParch: 1\\nPclass: 3\\nEmbarked: 0\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 623}, page_content='Passengerid: 624\\nAge: 21\\nFare: 7.8542\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 624}, page_content='Passengerid: 625\\nAge: 21\\nFare: 16.1\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 625}, page_content='Passengerid: 626\\nAge: 61\\nFare: 32.3208\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 626}, page_content='Passengerid: 627\\nAge: 57\\nFare: 12.35\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 1\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 627}, page_content='Passengerid: 628\\nAge: 21\\nFare: 77.9583\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 628}, page_content='Passengerid: 629\\nAge: 26\\nFare: 7.8958\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 629}, page_content='Passengerid: 630\\nAge: 28\\nFare: 7.7333\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 1\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 630}, page_content='Passengerid: 631\\nAge: 80\\nFare: 30\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 631}, page_content='Passengerid: 632\\nAge: 51\\nFare: 7.0542\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 632}, page_content='Passengerid: 633\\nAge: 32\\nFare: 30.5\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 0\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 633}, page_content='Passengerid: 634\\nAge: 28\\nFare: 0\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 634}, page_content='Passengerid: 635\\nAge: 9\\nFare: 27.9\\nSex: 1\\nsibsp: 3\\nzero: 0\\nParch: 2\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 635}, page_content='Passengerid: 636\\nAge: 28\\nFare: 13\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 636}, page_content='Passengerid: 637\\nAge: 32\\nFare: 7.925\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 637}, page_content='Passengerid: 638\\nAge: 31\\nFare: 26.25\\nSex: 0\\nsibsp: 1\\nzero: 0\\nParch: 1\\nPclass: 2\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 638}, page_content='Passengerid: 639\\nAge: 41\\nFare: 39.6875\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 5\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 639}, page_content='Passengerid: 640\\nAge: 28\\nFare: 16.1\\nSex: 0\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 640}, page_content='Passengerid: 641\\nAge: 20\\nFare: 7.8542\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 641}, page_content='Passengerid: 642\\nAge: 24\\nFare: 69.3\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 0\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 642}, page_content='Passengerid: 643\\nAge: 2\\nFare: 27.9\\nSex: 1\\nsibsp: 3\\nzero: 0\\nParch: 2\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 643}, page_content='Passengerid: 644\\nAge: 28\\nFare: 56.4958\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 644}, page_content='Passengerid: 645\\nAge: 0.75\\nFare: 19.2583\\nSex: 1\\nsibsp: 2\\nzero: 0\\nParch: 1\\nPclass: 3\\nEmbarked: 0\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 645}, page_content='Passengerid: 646\\nAge: 48\\nFare: 76.7292\\nSex: 0\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 0\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 646}, page_content='Passengerid: 647\\nAge: 19\\nFare: 7.8958\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 647}, page_content='Passengerid: 648\\nAge: 56\\nFare: 35.5\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 0\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 648}, page_content='Passengerid: 649\\nAge: 28\\nFare: 7.55\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 649}, page_content='Passengerid: 650\\nAge: 23\\nFare: 7.55\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 650}, page_content='Passengerid: 651\\nAge: 28\\nFare: 7.8958\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 651}, page_content='Passengerid: 652\\nAge: 18\\nFare: 23\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 1\\nPclass: 2\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 652}, page_content='Passengerid: 653\\nAge: 21\\nFare: 8.4333\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 653}, page_content='Passengerid: 654\\nAge: 28\\nFare: 7.8292\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 1\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 654}, page_content='Passengerid: 655\\nAge: 18\\nFare: 6.75\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 1\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 655}, page_content='Passengerid: 656\\nAge: 24\\nFare: 73.5\\nSex: 0\\nsibsp: 2\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 656}, page_content='Passengerid: 657\\nAge: 28\\nFare: 7.8958\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 657}, page_content='Passengerid: 658\\nAge: 32\\nFare: 15.5\\nSex: 1\\nsibsp: 1\\nzero: 0\\nParch: 1\\nPclass: 3\\nEmbarked: 1\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 658}, page_content='Passengerid: 659\\nAge: 23\\nFare: 13\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 659}, page_content='Passengerid: 660\\nAge: 58\\nFare: 113.275\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 2\\nPclass: 1\\nEmbarked: 0\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 660}, page_content='Passengerid: 661\\nAge: 50\\nFare: 133.65\\nSex: 0\\nsibsp: 2\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 661}, page_content='Passengerid: 662\\nAge: 40\\nFare: 7.225\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 0\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 662}, page_content='Passengerid: 663\\nAge: 47\\nFare: 25.5875\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 663}, page_content='Passengerid: 664\\nAge: 36\\nFare: 7.4958\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 664}, page_content='Passengerid: 665\\nAge: 20\\nFare: 7.925\\nSex: 0\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 665}, page_content='Passengerid: 666\\nAge: 32\\nFare: 73.5\\nSex: 0\\nsibsp: 2\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 666}, page_content='Passengerid: 667\\nAge: 25\\nFare: 13\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 667}, page_content='Passengerid: 668\\nAge: 28\\nFare: 7.775\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 668}, page_content='Passengerid: 669\\nAge: 43\\nFare: 8.05\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 669}, page_content='Passengerid: 670\\nAge: 28\\nFare: 52\\nSex: 1\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 670}, page_content='Passengerid: 671\\nAge: 40\\nFare: 39\\nSex: 1\\nsibsp: 1\\nzero: 0\\nParch: 1\\nPclass: 2\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 671}, page_content='Passengerid: 672\\nAge: 31\\nFare: 52\\nSex: 0\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 672}, page_content='Passengerid: 673\\nAge: 70\\nFare: 10.5\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 673}, page_content='Passengerid: 674\\nAge: 31\\nFare: 13\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 674}, page_content='Passengerid: 675\\nAge: 28\\nFare: 0\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 675}, page_content='Passengerid: 676\\nAge: 18\\nFare: 7.775\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 676}, page_content='Passengerid: 677\\nAge: 24.5\\nFare: 8.05\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 677}, page_content='Passengerid: 678\\nAge: 18\\nFare: 9.8417\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 678}, page_content='Passengerid: 679\\nAge: 43\\nFare: 46.9\\nSex: 1\\nsibsp: 1\\nzero: 0\\nParch: 6\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 679}, page_content='Passengerid: 680\\nAge: 36\\nFare: 512.3292\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 1\\nPclass: 1\\nEmbarked: 0\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 680}, page_content='Passengerid: 681\\nAge: 28\\nFare: 8.1375\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 1\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 681}, page_content='Passengerid: 682\\nAge: 27\\nFare: 76.7292\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 0\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 682}, page_content='Passengerid: 683\\nAge: 20\\nFare: 9.225\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 683}, page_content='Passengerid: 684\\nAge: 14\\nFare: 46.9\\nSex: 0\\nsibsp: 5\\nzero: 0\\nParch: 2\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 684}, page_content='Passengerid: 685\\nAge: 60\\nFare: 39\\nSex: 0\\nsibsp: 1\\nzero: 0\\nParch: 1\\nPclass: 2\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 685}, page_content='Passengerid: 686\\nAge: 25\\nFare: 41.5792\\nSex: 0\\nsibsp: 1\\nzero: 0\\nParch: 2\\nPclass: 2\\nEmbarked: 0\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 686}, page_content='Passengerid: 687\\nAge: 14\\nFare: 39.6875\\nSex: 0\\nsibsp: 4\\nzero: 0\\nParch: 1\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 687}, page_content='Passengerid: 688\\nAge: 19\\nFare: 10.1708\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 688}, page_content='Passengerid: 689\\nAge: 18\\nFare: 7.7958\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 689}, page_content='Passengerid: 690\\nAge: 15\\nFare: 211.3375\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 1\\nPclass: 1\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 690}, page_content='Passengerid: 691\\nAge: 31\\nFare: 57\\nSex: 0\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 691}, page_content='Passengerid: 692\\nAge: 4\\nFare: 13.4167\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 1\\nPclass: 3\\nEmbarked: 0\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 692}, page_content='Passengerid: 693\\nAge: 28\\nFare: 56.4958\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 693}, page_content='Passengerid: 694\\nAge: 25\\nFare: 7.225\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 0\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 694}, page_content='Passengerid: 695\\nAge: 60\\nFare: 26.55\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 695}, page_content='Passengerid: 696\\nAge: 52\\nFare: 13.5\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 696}, page_content='Passengerid: 697\\nAge: 44\\nFare: 8.05\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 697}, page_content='Passengerid: 698\\nAge: 28\\nFare: 7.7333\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 1\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 698}, page_content='Passengerid: 699\\nAge: 49\\nFare: 110.8833\\nSex: 0\\nsibsp: 1\\nzero: 0\\nParch: 1\\nPclass: 1\\nEmbarked: 0\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 699}, page_content='Passengerid: 700\\nAge: 42\\nFare: 7.65\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 700}, page_content='Passengerid: 701\\nAge: 18\\nFare: 227.525\\nSex: 1\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 0\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 701}, page_content='Passengerid: 702\\nAge: 35\\nFare: 26.2875\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 702}, page_content='Passengerid: 703\\nAge: 18\\nFare: 14.4542\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 1\\nPclass: 3\\nEmbarked: 0\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 703}, page_content='Passengerid: 704\\nAge: 25\\nFare: 7.7417\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 1\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 704}, page_content='Passengerid: 705\\nAge: 26\\nFare: 7.8542\\nSex: 0\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 705}, page_content='Passengerid: 706\\nAge: 39\\nFare: 26\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 706}, page_content='Passengerid: 707\\nAge: 45\\nFare: 13.5\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 707}, page_content='Passengerid: 708\\nAge: 42\\nFare: 26.2875\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 708}, page_content='Passengerid: 709\\nAge: 22\\nFare: 151.55\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 709}, page_content='Passengerid: 710\\nAge: 28\\nFare: 15.2458\\nSex: 0\\nsibsp: 1\\nzero: 0\\nParch: 1\\nPclass: 3\\nEmbarked: 0\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 710}, page_content='Passengerid: 711\\nAge: 24\\nFare: 49.5042\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 0\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 711}, page_content='Passengerid: 712\\nAge: 28\\nFare: 26.55\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 712}, page_content='Passengerid: 713\\nAge: 48\\nFare: 52\\nSex: 0\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 713}, page_content='Passengerid: 714\\nAge: 29\\nFare: 9.4833\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 714}, page_content='Passengerid: 715\\nAge: 52\\nFare: 13\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 715}, page_content='Passengerid: 716\\nAge: 19\\nFare: 7.65\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 716}, page_content='Passengerid: 717\\nAge: 38\\nFare: 227.525\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 0\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 717}, page_content='Passengerid: 718\\nAge: 27\\nFare: 10.5\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 718}, page_content='Passengerid: 719\\nAge: 28\\nFare: 15.5\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 1\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 719}, page_content='Passengerid: 720\\nAge: 33\\nFare: 7.775\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 720}, page_content='Passengerid: 721\\nAge: 6\\nFare: 33\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 1\\nPclass: 2\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 721}, page_content='Passengerid: 722\\nAge: 17\\nFare: 7.0542\\nSex: 0\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 722}, page_content='Passengerid: 723\\nAge: 34\\nFare: 13\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 723}, page_content='Passengerid: 724\\nAge: 50\\nFare: 13\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 724}, page_content='Passengerid: 725\\nAge: 27\\nFare: 53.1\\nSex: 0\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 725}, page_content='Passengerid: 726\\nAge: 20\\nFare: 8.6625\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 726}, page_content='Passengerid: 727\\nAge: 30\\nFare: 21\\nSex: 1\\nsibsp: 3\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 727}, page_content='Passengerid: 728\\nAge: 28\\nFare: 7.7375\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 1\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 728}, page_content='Passengerid: 729\\nAge: 25\\nFare: 26\\nSex: 0\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 729}, page_content='Passengerid: 730\\nAge: 25\\nFare: 7.925\\nSex: 1\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 730}, page_content='Passengerid: 731\\nAge: 29\\nFare: 211.3375\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 731}, page_content='Passengerid: 732\\nAge: 11\\nFare: 18.7875\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 0\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 732}, page_content='Passengerid: 733\\nAge: 28\\nFare: 0\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 733}, page_content='Passengerid: 734\\nAge: 23\\nFare: 13\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 734}, page_content='Passengerid: 735\\nAge: 23\\nFare: 13\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 735}, page_content='Passengerid: 736\\nAge: 28.5\\nFare: 16.1\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 736}, page_content='Passengerid: 737\\nAge: 48\\nFare: 34.375\\nSex: 1\\nsibsp: 1\\nzero: 0\\nParch: 3\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 737}, page_content='Passengerid: 738\\nAge: 35\\nFare: 512.3292\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 0\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 738}, page_content='Passengerid: 739\\nAge: 28\\nFare: 7.8958\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 739}, page_content='Passengerid: 740\\nAge: 28\\nFare: 7.8958\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 740}, page_content='Passengerid: 741\\nAge: 28\\nFare: 30\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 741}, page_content='Passengerid: 742\\nAge: 36\\nFare: 78.85\\nSex: 0\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 742}, page_content='Passengerid: 743\\nAge: 21\\nFare: 262.375\\nSex: 1\\nsibsp: 2\\nzero: 0\\nParch: 2\\nPclass: 1\\nEmbarked: 0\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 743}, page_content='Passengerid: 744\\nAge: 24\\nFare: 16.1\\nSex: 0\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 744}, page_content='Passengerid: 745\\nAge: 31\\nFare: 7.925\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 745}, page_content='Passengerid: 746\\nAge: 70\\nFare: 71\\nSex: 0\\nsibsp: 1\\nzero: 0\\nParch: 1\\nPclass: 1\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 746}, page_content='Passengerid: 747\\nAge: 16\\nFare: 20.25\\nSex: 0\\nsibsp: 1\\nzero: 0\\nParch: 1\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 747}, page_content='Passengerid: 748\\nAge: 30\\nFare: 13\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 748}, page_content='Passengerid: 749\\nAge: 19\\nFare: 53.1\\nSex: 0\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 749}, page_content='Passengerid: 750\\nAge: 31\\nFare: 7.75\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 1\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 750}, page_content='Passengerid: 751\\nAge: 4\\nFare: 23\\nSex: 1\\nsibsp: 1\\nzero: 0\\nParch: 1\\nPclass: 2\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 751}, page_content='Passengerid: 752\\nAge: 6\\nFare: 12.475\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 1\\nPclass: 3\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 752}, page_content='Passengerid: 753\\nAge: 33\\nFare: 9.5\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 753}, page_content='Passengerid: 754\\nAge: 23\\nFare: 7.8958\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 754}, page_content='Passengerid: 755\\nAge: 48\\nFare: 65\\nSex: 1\\nsibsp: 1\\nzero: 0\\nParch: 2\\nPclass: 2\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 755}, page_content='Passengerid: 756\\nAge: 0.67\\nFare: 14.5\\nSex: 0\\nsibsp: 1\\nzero: 0\\nParch: 1\\nPclass: 2\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 756}, page_content='Passengerid: 757\\nAge: 28\\nFare: 7.7958\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 757}, page_content='Passengerid: 758\\nAge: 18\\nFare: 11.5\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 758}, page_content='Passengerid: 759\\nAge: 34\\nFare: 8.05\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 759}, page_content='Passengerid: 760\\nAge: 33\\nFare: 86.5\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 760}, page_content='Passengerid: 761\\nAge: 28\\nFare: 14.5\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 761}, page_content='Passengerid: 762\\nAge: 41\\nFare: 7.125\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 762}, page_content='Passengerid: 763\\nAge: 20\\nFare: 7.2292\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 0\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 763}, page_content='Passengerid: 764\\nAge: 36\\nFare: 120\\nSex: 1\\nsibsp: 1\\nzero: 0\\nParch: 2\\nPclass: 1\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 764}, page_content='Passengerid: 765\\nAge: 16\\nFare: 7.775\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 765}, page_content='Passengerid: 766\\nAge: 51\\nFare: 77.9583\\nSex: 1\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 766}, page_content='Passengerid: 767\\nAge: 28\\nFare: 39.6\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 0\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 767}, page_content='Passengerid: 768\\nAge: 30.5\\nFare: 7.75\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 1\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 768}, page_content='Passengerid: 769\\nAge: 28\\nFare: 24.15\\nSex: 0\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 1\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 769}, page_content='Passengerid: 770\\nAge: 32\\nFare: 8.3625\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 770}, page_content='Passengerid: 771\\nAge: 24\\nFare: 9.5\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 771}, page_content='Passengerid: 772\\nAge: 48\\nFare: 7.8542\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 772}, page_content='Passengerid: 773\\nAge: 57\\nFare: 10.5\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 773}, page_content='Passengerid: 774\\nAge: 28\\nFare: 7.225\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 0\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 774}, page_content='Passengerid: 775\\nAge: 54\\nFare: 23\\nSex: 1\\nsibsp: 1\\nzero: 0\\nParch: 3\\nPclass: 2\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 775}, page_content='Passengerid: 776\\nAge: 18\\nFare: 7.75\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 776}, page_content='Passengerid: 777\\nAge: 28\\nFare: 7.75\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 1\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 777}, page_content='Passengerid: 778\\nAge: 5\\nFare: 12.475\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 778}, page_content='Passengerid: 779\\nAge: 28\\nFare: 7.7375\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 1\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 779}, page_content='Passengerid: 780\\nAge: 43\\nFare: 211.3375\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 1\\nPclass: 1\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 780}, page_content='Passengerid: 781\\nAge: 13\\nFare: 7.2292\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 0\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 781}, page_content='Passengerid: 782\\nAge: 17\\nFare: 57\\nSex: 1\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 782}, page_content='Passengerid: 783\\nAge: 29\\nFare: 30\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 783}, page_content='Passengerid: 784\\nAge: 28\\nFare: 23.45\\nSex: 0\\nsibsp: 1\\nzero: 0\\nParch: 2\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 784}, page_content='Passengerid: 785\\nAge: 25\\nFare: 7.05\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 785}, page_content='Passengerid: 786\\nAge: 25\\nFare: 7.25\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 786}, page_content='Passengerid: 787\\nAge: 18\\nFare: 7.4958\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 787}, page_content='Passengerid: 788\\nAge: 8\\nFare: 29.125\\nSex: 0\\nsibsp: 4\\nzero: 0\\nParch: 1\\nPclass: 3\\nEmbarked: 1\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 788}, page_content='Passengerid: 789\\nAge: 1\\nFare: 20.575\\nSex: 0\\nsibsp: 1\\nzero: 0\\nParch: 2\\nPclass: 3\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 789}, page_content='Passengerid: 790\\nAge: 46\\nFare: 79.2\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 0\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 790}, page_content='Passengerid: 791\\nAge: 28\\nFare: 7.75\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 1\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 791}, page_content='Passengerid: 792\\nAge: 16\\nFare: 26\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 792}, page_content='Passengerid: 793\\nAge: 28\\nFare: 69.55\\nSex: 1\\nsibsp: 8\\nzero: 0\\nParch: 2\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 793}, page_content='Passengerid: 794\\nAge: 28\\nFare: 30.6958\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 0\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 794}, page_content='Passengerid: 795\\nAge: 25\\nFare: 7.8958\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 795}, page_content='Passengerid: 796\\nAge: 39\\nFare: 13\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 796}, page_content='Passengerid: 797\\nAge: 49\\nFare: 25.9292\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 797}, page_content='Passengerid: 798\\nAge: 31\\nFare: 8.6833\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 798}, page_content='Passengerid: 799\\nAge: 30\\nFare: 7.2292\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 0\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 799}, page_content='Passengerid: 800\\nAge: 30\\nFare: 24.15\\nSex: 1\\nsibsp: 1\\nzero: 0\\nParch: 1\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 800}, page_content='Passengerid: 801\\nAge: 34\\nFare: 13\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 801}, page_content='Passengerid: 802\\nAge: 31\\nFare: 26.25\\nSex: 1\\nsibsp: 1\\nzero: 0\\nParch: 1\\nPclass: 2\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 802}, page_content='Passengerid: 803\\nAge: 11\\nFare: 120\\nSex: 0\\nsibsp: 1\\nzero: 0\\nParch: 2\\nPclass: 1\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 803}, page_content='Passengerid: 804\\nAge: 0.42\\nFare: 8.5167\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 1\\nPclass: 3\\nEmbarked: 0\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 804}, page_content='Passengerid: 805\\nAge: 27\\nFare: 6.975\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 805}, page_content='Passengerid: 806\\nAge: 31\\nFare: 7.775\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 806}, page_content='Passengerid: 807\\nAge: 39\\nFare: 0\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 807}, page_content='Passengerid: 808\\nAge: 18\\nFare: 7.775\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 808}, page_content='Passengerid: 809\\nAge: 39\\nFare: 13\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 809}, page_content='Passengerid: 810\\nAge: 33\\nFare: 53.1\\nSex: 1\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 810}, page_content='Passengerid: 811\\nAge: 26\\nFare: 7.8875\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 811}, page_content='Passengerid: 812\\nAge: 39\\nFare: 24.15\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 812}, page_content='Passengerid: 813\\nAge: 35\\nFare: 10.5\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 813}, page_content='Passengerid: 814\\nAge: 6\\nFare: 31.275\\nSex: 1\\nsibsp: 4\\nzero: 0\\nParch: 2\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 814}, page_content='Passengerid: 815\\nAge: 30.5\\nFare: 8.05\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 815}, page_content='Passengerid: 816\\nAge: 28\\nFare: 0\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 816}, page_content='Passengerid: 817\\nAge: 23\\nFare: 7.925\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 817}, page_content='Passengerid: 818\\nAge: 31\\nFare: 37.0042\\nSex: 0\\nsibsp: 1\\nzero: 0\\nParch: 1\\nPclass: 2\\nEmbarked: 0\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 818}, page_content='Passengerid: 819\\nAge: 43\\nFare: 6.45\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 819}, page_content='Passengerid: 820\\nAge: 10\\nFare: 27.9\\nSex: 0\\nsibsp: 3\\nzero: 0\\nParch: 2\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 820}, page_content='Passengerid: 821\\nAge: 52\\nFare: 93.5\\nSex: 1\\nsibsp: 1\\nzero: 0\\nParch: 1\\nPclass: 1\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 821}, page_content='Passengerid: 822\\nAge: 27\\nFare: 8.6625\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 822}, page_content='Passengerid: 823\\nAge: 38\\nFare: 0\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 823}, page_content='Passengerid: 824\\nAge: 27\\nFare: 12.475\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 1\\nPclass: 3\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 824}, page_content='Passengerid: 825\\nAge: 2\\nFare: 39.6875\\nSex: 0\\nsibsp: 4\\nzero: 0\\nParch: 1\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 825}, page_content='Passengerid: 826\\nAge: 28\\nFare: 6.95\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 1\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 826}, page_content='Passengerid: 827\\nAge: 28\\nFare: 56.4958\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 827}, page_content='Passengerid: 828\\nAge: 1\\nFare: 37.0042\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 2\\nPclass: 2\\nEmbarked: 0\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 828}, page_content='Passengerid: 829\\nAge: 28\\nFare: 7.75\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 1\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 829}, page_content='Passengerid: 830\\nAge: 62\\nFare: 80\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: \\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 830}, page_content='Passengerid: 831\\nAge: 15\\nFare: 14.4542\\nSex: 1\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 0\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 831}, page_content='Passengerid: 832\\nAge: 0.83\\nFare: 18.75\\nSex: 0\\nsibsp: 1\\nzero: 0\\nParch: 1\\nPclass: 2\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 832}, page_content='Passengerid: 833\\nAge: 28\\nFare: 7.2292\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 0\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 833}, page_content='Passengerid: 834\\nAge: 23\\nFare: 7.8542\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 834}, page_content='Passengerid: 835\\nAge: 18\\nFare: 8.3\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 835}, page_content='Passengerid: 836\\nAge: 39\\nFare: 83.1583\\nSex: 1\\nsibsp: 1\\nzero: 0\\nParch: 1\\nPclass: 1\\nEmbarked: 0\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 836}, page_content='Passengerid: 837\\nAge: 21\\nFare: 8.6625\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 837}, page_content='Passengerid: 838\\nAge: 28\\nFare: 8.05\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 838}, page_content='Passengerid: 839\\nAge: 32\\nFare: 56.4958\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 839}, page_content='Passengerid: 840\\nAge: 28\\nFare: 29.7\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 0\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 840}, page_content='Passengerid: 841\\nAge: 20\\nFare: 7.925\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 841}, page_content='Passengerid: 842\\nAge: 16\\nFare: 10.5\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 842}, page_content='Passengerid: 843\\nAge: 30\\nFare: 31\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 0\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 843}, page_content='Passengerid: 844\\nAge: 34.5\\nFare: 6.4375\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 0\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 844}, page_content='Passengerid: 845\\nAge: 17\\nFare: 8.6625\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 845}, page_content='Passengerid: 846\\nAge: 42\\nFare: 7.55\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 846}, page_content='Passengerid: 847\\nAge: 28\\nFare: 69.55\\nSex: 0\\nsibsp: 8\\nzero: 0\\nParch: 2\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 847}, page_content='Passengerid: 848\\nAge: 35\\nFare: 7.8958\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 0\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 848}, page_content='Passengerid: 849\\nAge: 28\\nFare: 33\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 1\\nPclass: 2\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 849}, page_content='Passengerid: 850\\nAge: 28\\nFare: 89.1042\\nSex: 1\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 0\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 850}, page_content='Passengerid: 851\\nAge: 4\\nFare: 31.275\\nSex: 0\\nsibsp: 4\\nzero: 0\\nParch: 2\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 851}, page_content='Passengerid: 852\\nAge: 74\\nFare: 7.775\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 852}, page_content='Passengerid: 853\\nAge: 9\\nFare: 15.2458\\nSex: 1\\nsibsp: 1\\nzero: 0\\nParch: 1\\nPclass: 3\\nEmbarked: 0\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 853}, page_content='Passengerid: 854\\nAge: 16\\nFare: 39.4\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 1\\nPclass: 1\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 854}, page_content='Passengerid: 855\\nAge: 44\\nFare: 26\\nSex: 1\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 855}, page_content='Passengerid: 856\\nAge: 18\\nFare: 9.35\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 1\\nPclass: 3\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 856}, page_content='Passengerid: 857\\nAge: 45\\nFare: 164.8667\\nSex: 1\\nsibsp: 1\\nzero: 0\\nParch: 1\\nPclass: 1\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 857}, page_content='Passengerid: 858\\nAge: 51\\nFare: 26.55\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 858}, page_content='Passengerid: 859\\nAge: 24\\nFare: 19.2583\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 3\\nPclass: 3\\nEmbarked: 0\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 859}, page_content='Passengerid: 860\\nAge: 28\\nFare: 7.2292\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 0\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 860}, page_content='Passengerid: 861\\nAge: 41\\nFare: 14.1083\\nSex: 0\\nsibsp: 2\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 861}, page_content='Passengerid: 862\\nAge: 21\\nFare: 11.5\\nSex: 0\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 862}, page_content='Passengerid: 863\\nAge: 48\\nFare: 25.9292\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 863}, page_content='Passengerid: 864\\nAge: 28\\nFare: 69.55\\nSex: 1\\nsibsp: 8\\nzero: 0\\nParch: 2\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 864}, page_content='Passengerid: 865\\nAge: 24\\nFare: 13\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 865}, page_content='Passengerid: 866\\nAge: 42\\nFare: 13\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 866}, page_content='Passengerid: 867\\nAge: 27\\nFare: 13.8583\\nSex: 1\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 0\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 867}, page_content='Passengerid: 868\\nAge: 31\\nFare: 50.4958\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 868}, page_content='Passengerid: 869\\nAge: 28\\nFare: 9.5\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 869}, page_content='Passengerid: 870\\nAge: 4\\nFare: 11.1333\\nSex: 0\\nsibsp: 1\\nzero: 0\\nParch: 1\\nPclass: 3\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 870}, page_content='Passengerid: 871\\nAge: 26\\nFare: 7.8958\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 871}, page_content='Passengerid: 872\\nAge: 47\\nFare: 52.5542\\nSex: 1\\nsibsp: 1\\nzero: 0\\nParch: 1\\nPclass: 1\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 872}, page_content='Passengerid: 873\\nAge: 33\\nFare: 5\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 873}, page_content='Passengerid: 874\\nAge: 47\\nFare: 9\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 874}, page_content='Passengerid: 875\\nAge: 28\\nFare: 24\\nSex: 1\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 0\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 875}, page_content='Passengerid: 876\\nAge: 15\\nFare: 7.225\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 0\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 876}, page_content='Passengerid: 877\\nAge: 20\\nFare: 9.8458\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 877}, page_content='Passengerid: 878\\nAge: 19\\nFare: 7.8958\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 878}, page_content='Passengerid: 879\\nAge: 28\\nFare: 7.8958\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 879}, page_content='Passengerid: 880\\nAge: 56\\nFare: 83.1583\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 1\\nPclass: 1\\nEmbarked: 0\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 880}, page_content='Passengerid: 881\\nAge: 25\\nFare: 26\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 1\\nPclass: 2\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 881}, page_content='Passengerid: 882\\nAge: 33\\nFare: 7.8958\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 882}, page_content='Passengerid: 883\\nAge: 22\\nFare: 10.5167\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 883}, page_content='Passengerid: 884\\nAge: 28\\nFare: 10.5\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 884}, page_content='Passengerid: 885\\nAge: 25\\nFare: 7.05\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 885}, page_content='Passengerid: 886\\nAge: 39\\nFare: 29.125\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 5\\nPclass: 3\\nEmbarked: 1\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 886}, page_content='Passengerid: 887\\nAge: 27\\nFare: 13\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 887}, page_content='Passengerid: 888\\nAge: 19\\nFare: 30\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 2\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 888}, page_content='Passengerid: 889\\nAge: 28\\nFare: 23.45\\nSex: 1\\nsibsp: 1\\nzero: 0\\nParch: 2\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 889}, page_content='Passengerid: 890\\nAge: 26\\nFare: 30\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 0\\n2urvived: 1'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 890}, page_content='Passengerid: 891\\nAge: 32\\nFare: 7.75\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 1\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 891}, page_content='Passengerid: 892\\nAge: 34.5\\nFare: 7.8292\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 1\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 892}, page_content='Passengerid: 893\\nAge: 47\\nFare: 7\\nSex: 1\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 893}, page_content='Passengerid: 894\\nAge: 62\\nFare: 9.6875\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 1\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 894}, page_content='Passengerid: 895\\nAge: 27\\nFare: 8.6625\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 895}, page_content='Passengerid: 896\\nAge: 22\\nFare: 12.2875\\nSex: 1\\nsibsp: 1\\nzero: 0\\nParch: 1\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 896}, page_content='Passengerid: 897\\nAge: 14\\nFare: 9.225\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 897}, page_content='Passengerid: 898\\nAge: 30\\nFare: 7.6292\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 1\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 898}, page_content='Passengerid: 899\\nAge: 26\\nFare: 29\\nSex: 0\\nsibsp: 1\\nzero: 0\\nParch: 1\\nPclass: 2\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 899}, page_content='Passengerid: 900\\nAge: 18\\nFare: 7.2292\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 0\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 900}, page_content='Passengerid: 901\\nAge: 21\\nFare: 24.15\\nSex: 0\\nsibsp: 2\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 901}, page_content='Passengerid: 902\\nAge: 28\\nFare: 7.8958\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 902}, page_content='Passengerid: 903\\nAge: 46\\nFare: 26\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 903}, page_content='Passengerid: 904\\nAge: 23\\nFare: 82.2667\\nSex: 1\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 904}, page_content='Passengerid: 905\\nAge: 63\\nFare: 26\\nSex: 0\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 905}, page_content='Passengerid: 906\\nAge: 47\\nFare: 61.175\\nSex: 1\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 906}, page_content='Passengerid: 907\\nAge: 24\\nFare: 27.7208\\nSex: 1\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 0\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 907}, page_content='Passengerid: 908\\nAge: 35\\nFare: 12.35\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 1\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 908}, page_content='Passengerid: 909\\nAge: 21\\nFare: 7.225\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 0\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 909}, page_content='Passengerid: 910\\nAge: 27\\nFare: 7.925\\nSex: 1\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 910}, page_content='Passengerid: 911\\nAge: 45\\nFare: 7.225\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 0\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 911}, page_content='Passengerid: 912\\nAge: 55\\nFare: 59.4\\nSex: 0\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 0\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 912}, page_content='Passengerid: 913\\nAge: 9\\nFare: 3.1708\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 1\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 913}, page_content='Passengerid: 914\\nAge: 28\\nFare: 31.6833\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 914}, page_content='Passengerid: 915\\nAge: 21\\nFare: 61.3792\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 1\\nPclass: 1\\nEmbarked: 0\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 915}, page_content='Passengerid: 916\\nAge: 48\\nFare: 262.375\\nSex: 1\\nsibsp: 1\\nzero: 0\\nParch: 3\\nPclass: 1\\nEmbarked: 0\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 916}, page_content='Passengerid: 917\\nAge: 50\\nFare: 14.5\\nSex: 0\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 917}, page_content='Passengerid: 918\\nAge: 22\\nFare: 61.9792\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 1\\nPclass: 1\\nEmbarked: 0\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 918}, page_content='Passengerid: 919\\nAge: 22.5\\nFare: 7.225\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 0\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 919}, page_content='Passengerid: 920\\nAge: 41\\nFare: 30.5\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 920}, page_content='Passengerid: 921\\nAge: 28\\nFare: 21.6792\\nSex: 0\\nsibsp: 2\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 0\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 921}, page_content='Passengerid: 922\\nAge: 50\\nFare: 26\\nSex: 0\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 922}, page_content='Passengerid: 923\\nAge: 24\\nFare: 31.5\\nSex: 0\\nsibsp: 2\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 923}, page_content='Passengerid: 924\\nAge: 33\\nFare: 20.575\\nSex: 1\\nsibsp: 1\\nzero: 0\\nParch: 2\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 924}, page_content='Passengerid: 925\\nAge: 28\\nFare: 23.45\\nSex: 1\\nsibsp: 1\\nzero: 0\\nParch: 2\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 925}, page_content='Passengerid: 926\\nAge: 30\\nFare: 57.75\\nSex: 0\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 0\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 926}, page_content='Passengerid: 927\\nAge: 18.5\\nFare: 7.2292\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 0\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 927}, page_content='Passengerid: 928\\nAge: 28\\nFare: 8.05\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 928}, page_content='Passengerid: 929\\nAge: 21\\nFare: 8.6625\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 929}, page_content='Passengerid: 930\\nAge: 25\\nFare: 9.5\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 930}, page_content='Passengerid: 931\\nAge: 28\\nFare: 56.4958\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 931}, page_content='Passengerid: 932\\nAge: 39\\nFare: 13.4167\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 1\\nPclass: 3\\nEmbarked: 0\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 932}, page_content='Passengerid: 933\\nAge: 28\\nFare: 26.55\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 933}, page_content='Passengerid: 934\\nAge: 41\\nFare: 7.85\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 934}, page_content='Passengerid: 935\\nAge: 30\\nFare: 13\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 935}, page_content='Passengerid: 936\\nAge: 45\\nFare: 52.5542\\nSex: 1\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 936}, page_content='Passengerid: 937\\nAge: 25\\nFare: 7.925\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 937}, page_content='Passengerid: 938\\nAge: 45\\nFare: 29.7\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 0\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 938}, page_content='Passengerid: 939\\nAge: 28\\nFare: 7.75\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 1\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 939}, page_content='Passengerid: 940\\nAge: 60\\nFare: 76.2917\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 0\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 940}, page_content='Passengerid: 941\\nAge: 36\\nFare: 15.9\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 2\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 941}, page_content='Passengerid: 942\\nAge: 24\\nFare: 60\\nSex: 0\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 942}, page_content='Passengerid: 943\\nAge: 27\\nFare: 15.0333\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 0\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 943}, page_content='Passengerid: 944\\nAge: 20\\nFare: 23\\nSex: 1\\nsibsp: 2\\nzero: 0\\nParch: 1\\nPclass: 2\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 944}, page_content='Passengerid: 945\\nAge: 28\\nFare: 263\\nSex: 1\\nsibsp: 3\\nzero: 0\\nParch: 2\\nPclass: 1\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 945}, page_content='Passengerid: 946\\nAge: 28\\nFare: 15.5792\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 0\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 946}, page_content='Passengerid: 947\\nAge: 10\\nFare: 29.125\\nSex: 0\\nsibsp: 4\\nzero: 0\\nParch: 1\\nPclass: 3\\nEmbarked: 1\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 947}, page_content='Passengerid: 948\\nAge: 35\\nFare: 7.8958\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 948}, page_content='Passengerid: 949\\nAge: 25\\nFare: 7.65\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 949}, page_content='Passengerid: 950\\nAge: 28\\nFare: 16.1\\nSex: 0\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 950}, page_content='Passengerid: 951\\nAge: 36\\nFare: 262.375\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 0\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 951}, page_content='Passengerid: 952\\nAge: 17\\nFare: 7.8958\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 952}, page_content='Passengerid: 953\\nAge: 32\\nFare: 13.5\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 953}, page_content='Passengerid: 954\\nAge: 18\\nFare: 7.75\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 954}, page_content='Passengerid: 955\\nAge: 22\\nFare: 7.725\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 1\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 955}, page_content='Passengerid: 956\\nAge: 13\\nFare: 262.375\\nSex: 0\\nsibsp: 2\\nzero: 0\\nParch: 2\\nPclass: 1\\nEmbarked: 0\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 956}, page_content='Passengerid: 957\\nAge: 28\\nFare: 21\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 957}, page_content='Passengerid: 958\\nAge: 18\\nFare: 7.8792\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 1\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 958}, page_content='Passengerid: 959\\nAge: 47\\nFare: 42.4\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 959}, page_content='Passengerid: 960\\nAge: 31\\nFare: 28.5375\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 0\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 960}, page_content='Passengerid: 961\\nAge: 60\\nFare: 263\\nSex: 1\\nsibsp: 1\\nzero: 0\\nParch: 4\\nPclass: 1\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 961}, page_content='Passengerid: 962\\nAge: 24\\nFare: 7.75\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 1\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 962}, page_content='Passengerid: 963\\nAge: 21\\nFare: 7.8958\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 963}, page_content='Passengerid: 964\\nAge: 29\\nFare: 7.925\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 964}, page_content='Passengerid: 965\\nAge: 28.5\\nFare: 27.7208\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 0\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 965}, page_content='Passengerid: 966\\nAge: 35\\nFare: 211.5\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 0\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 966}, page_content='Passengerid: 967\\nAge: 32.5\\nFare: 211.5\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 0\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 967}, page_content='Passengerid: 968\\nAge: 28\\nFare: 8.05\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 968}, page_content='Passengerid: 969\\nAge: 55\\nFare: 25.7\\nSex: 1\\nsibsp: 2\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 969}, page_content='Passengerid: 970\\nAge: 30\\nFare: 13\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 970}, page_content='Passengerid: 971\\nAge: 24\\nFare: 7.75\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 1\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 971}, page_content='Passengerid: 972\\nAge: 6\\nFare: 15.2458\\nSex: 0\\nsibsp: 1\\nzero: 0\\nParch: 1\\nPclass: 3\\nEmbarked: 0\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 972}, page_content='Passengerid: 973\\nAge: 67\\nFare: 221.7792\\nSex: 0\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 973}, page_content='Passengerid: 974\\nAge: 49\\nFare: 26\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 974}, page_content='Passengerid: 975\\nAge: 28\\nFare: 7.8958\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 975}, page_content='Passengerid: 976\\nAge: 28\\nFare: 10.7083\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 1\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 976}, page_content='Passengerid: 977\\nAge: 28\\nFare: 14.4542\\nSex: 0\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 0\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 977}, page_content='Passengerid: 978\\nAge: 27\\nFare: 7.8792\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 1\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 978}, page_content='Passengerid: 979\\nAge: 18\\nFare: 8.05\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 979}, page_content='Passengerid: 980\\nAge: 28\\nFare: 7.75\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 1\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 980}, page_content='Passengerid: 981\\nAge: 2\\nFare: 23\\nSex: 0\\nsibsp: 1\\nzero: 0\\nParch: 1\\nPclass: 2\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 981}, page_content='Passengerid: 982\\nAge: 22\\nFare: 13.9\\nSex: 1\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 982}, page_content='Passengerid: 983\\nAge: 28\\nFare: 7.775\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 983}, page_content='Passengerid: 984\\nAge: 27\\nFare: 52\\nSex: 1\\nsibsp: 1\\nzero: 0\\nParch: 2\\nPclass: 1\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 984}, page_content='Passengerid: 985\\nAge: 28\\nFare: 8.05\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 985}, page_content='Passengerid: 986\\nAge: 25\\nFare: 26\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 0\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 986}, page_content='Passengerid: 987\\nAge: 25\\nFare: 7.7958\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 987}, page_content='Passengerid: 988\\nAge: 76\\nFare: 78.85\\nSex: 1\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 988}, page_content='Passengerid: 989\\nAge: 29\\nFare: 7.925\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 989}, page_content='Passengerid: 990\\nAge: 20\\nFare: 7.8542\\nSex: 1\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 990}, page_content='Passengerid: 991\\nAge: 33\\nFare: 8.05\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 991}, page_content='Passengerid: 992\\nAge: 43\\nFare: 55.4417\\nSex: 1\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 1\\nEmbarked: 0\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 992}, page_content='Passengerid: 993\\nAge: 27\\nFare: 26\\nSex: 0\\nsibsp: 1\\nzero: 0\\nParch: 0\\nPclass: 2\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 993}, page_content='Passengerid: 994\\nAge: 28\\nFare: 7.75\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 1\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 994}, page_content='Passengerid: 995\\nAge: 26\\nFare: 7.775\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 995}, page_content='Passengerid: 996\\nAge: 16\\nFare: 8.5167\\nSex: 1\\nsibsp: 1\\nzero: 0\\nParch: 1\\nPclass: 3\\nEmbarked: 0\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 996}, page_content='Passengerid: 997\\nAge: 28\\nFare: 22.525\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 997}, page_content='Passengerid: 998\\nAge: 21\\nFare: 7.8208\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 1\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 998}, page_content='Passengerid: 999\\nAge: 28\\nFare: 7.75\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 1\\n2urvived: 0'),\n",
       " Document(metadata={'source': 'data/train_and_test2.csv', 'row': 999}, page_content='Passengerid: 1000\\nAge: 28\\nFare: 8.7125\\nSex: 0\\nsibsp: 0\\nzero: 0\\nParch: 0\\nPclass: 3\\nEmbarked: 2\\n2urvived: 0'),\n",
       " ...]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import CSVLoader\n",
    "file_path = \"data/train_and_test2.csv\"\n",
    "loader = CSVLoader(file_path=file_path)\n",
    "# loader = CSVLoader(\n",
    "#     file_path=file_path,\n",
    "#     csv_args={\n",
    "#         \"delimiter\": \",\",\n",
    "#         \"quotechar\": '\"',\n",
    "#         \"fieldnames\": [\"Id\", \"email\", \"class\"],\n",
    "#     },\n",
    "# )\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YouTube Transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'LAfrShnpVIk'}, page_content=\"when using chat GPT you most probably have encountered responses like I'm sorry but as of my last knowledge update in January 2022 Etc or even responses that are not true at all this is where rag comes into play and says let me help you by injecting more knowledge or content into your interactions with an llm and help it answer the unknown and upcoming questions we hear llms prompts and rag Everywhere by now I think most of us know what an llm and the prompt is but did you know that right now rag is just as important as both of these and Powers most applications you may use involving a chatbot I recently did a poll on the learn AI together Discord Community to find out if people had already studied created or used rag applications and most voted to understand what rag is used for rag is as important as your coursebook for success in a class so understanding what it is is highly relevant in AI an llm or a large language mode model is just an AI model trained on language to talk with humans like GPT 4 used in ch GPT a prompt is simply your interaction with it it's the question you ask it but if you are experiencing issues like hallucinations or biases using such a language model or llm then rag or retrieval augmented Generations comes into play Let's quickly clarify hallucinations first it's when the model returns random things that seems true but aren't simply because it doesn't know the answer in fact a language model is constantly hallucinating it only predicts words in a statistical way it turns out that when they are trained with the entire internet there are so many examples that they manage to accurately predict the next logical words to answer most questions despite this it hallucinates it doesn't really understand what it's talking about and just outputs one word at a time that is probable what is incredible is that most of these hallucinations are actually true and answer our questions however some of them are real hallucinations of fabricated facts or scenarios and that can cause quite a few problems if they are not sufficiently controlled while there are several reasons why llms hallucinate it is mostly because they lack relevant context either because they cannot find the relevant data or don't know which data to refer to for a particular question this is because they were trained to answer and not to say I don't know rag solves this by automatically adding more knowledge or content into your interactions with an llm put simply you have a data set which is required and you use it to help the llm answer the unknown and upcoming user questions this is the simplest form and requires a few steps to make it work but this is the gist of a rag based system you have a user question that is sent to an automatic search in the database for finding relevant information which is then used back along with the question to give back an answer to the user as you can see with frag we use context from the user question question and our knowledge base to answer it this helps with grounding our model to the knowledge we control making it safer and aligned the disadvantage is limiting our answers to our knowledge base which is finite and probably not as big as the Internet it's just like an open book exam you would have in school you already have access to most answers and simply need to know where it is in your knowledge base if you find the answer in the manual it's quite hard to fail the question and write something wrong Jerry Leu CEO of L index gave a very interesting view on how to see rag in my most recent podcast with him if you think about it rag is basically prompt engineering because you're basically figuring out a way to put context into the prompt uh it's just a programmatic way of prompt engineering it's a way of prompting so that you actually get back um some contacts he also said to subscribe to the channel to learn more about AI okay maybe that's just a hallucination actually but you should still do it honestly in rag you first need data or knowledge which can be in the form of documentation but books articles Etc and only allow the llm to search and respond if the answer to the question is inside this knowledge base you have anyways if you have access to accurate information in your school manual why would you try to come up with something different Instead This is currently the best way to control your outputs and make your model safer and aligned basically the best way to ensure you will give the right answer and get your best grade for example we recently built an AI tutor to answer AI related questions we wanted accurate responses for our students both in terms of accuracy to give the right answer and in terms of relevancy so upto-date information with rag you can simply update your database if things have changed there's no big deal if the whole pytorch Library had a big update yesterday scrape it again and update your data set in a second and voila you don't have to retrain the whole model or wait for a gp4 to finally update the noledge card update the overall process of the butt is quite straightforward we validate the question answering it is related to Ai and that our chatbot should answer it then we search in our database to find good and relevant sources and finally use chat GPT to digest those sources and give a good answer for the student if you need safe information from an AI chat but like a medical assistant a tutor a lawyer or an accent you will be using rag for sure well maybe not if you're listening in 2030 but as of now rag is by far the best and safest approach to using a chatbot where you need to give factual and accurate information to build a rag based chatbot or application like our AI tutor we start by ingesting all our data into memory this is done by splitting all the content into chunks of text so split our textual data into fixed or flexible parts for example 500 character parts and processing it to an embedding model like open AI text embedding Adda model this will produce embeddings that are just vectors of numbers repres representing your text it will facilitate your life and allow you to compare text together easily you can save those vectors in a memory then for a new question from a particular user you can repeat this process and answer this means embedding the question using the same approach and compare it with all your current embeddings in your memory here you are basically looking for the most probable answer for this question searching in your memory just like you do for an exam looking through the chapters to find a title that seems relevant to the current exam question once it finds the most similar embedding chat GPT is asked to understand the user's question and intent and only use the retrieved sources of knowledge to answer the question this is how rag reduces hallucination risks and allows you to have upto-date information since you can update your knowledge base as much as you want and chat gbt or your current language model simply picks information from it to answer plus as you see it cites all sources it found on question for you to dive in and learn more which is also a plus when you're trying to learn and understand a new topic then there are still many things to consider like how to determine when to answer a question or not if it is relevant or in your documentation understand new terms or acronyms not in chat gpt's knowledge base find the relevant information more efficiently and accurately etc those concerns are all things we've improved through using various techniques like better chunking methods rank ERS query expansion agents and more that you can learn about in our free Advanced rag course we've built together with toi and active Loop that I Linked In the description below before some of you may ask yes an alternative to rag would be to fine-tune your model on your specific task basically to further train a model on your own data to make it more specific and ingest the knowledge you have rather than always searching in it like memorizing the book before the exam instead of bringing it with you I have a video comparing fine tuning and rag to teach you when you should consider each but in short rag stays relevant with or without fine tuning as it is much cheaper to build and is better to reduce undesired hallucinations as you force the model to give answers based on documentation you control and not simply things it ingested and hopefully will regurgitate correctly as in fine tune models coming back to our open book exams is just like professors making you focus on understanding the core matter and logic and not the knowledge itself as you can always find it in manuals or on Google same here for llms and complimenting them with rag plus even though those models have much better memories than us they are not perfect and they will not retain all the information you give them thus even with a fine tune model on hyp specific data rag remains something worth leveraging before we end this video I just wanted to mention that we discussed both these Topics in depth with coding examples in our llm and rag courses if you want to put this knowledge into practice the link is in the description below and they are completely free I hope you've enjoyed this video and that it helps you understand the goals and principles of rag better if you did please share it with a friend or your network to spread the know Edge and help the channel grow thank you for [Music] watching [Music]\")]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %pip install --upgrade --quiet  youtube-transcript-api\n",
    "# %pip install --upgrade --quiet  pytube\n",
    "\n",
    "from langchain_community.document_loaders import YoutubeLoader\n",
    "\n",
    "loader = YoutubeLoader.from_youtube_url(\n",
    "    \"https://www.youtube.com/watch?v=LAfrShnpVIk\",\n",
    "    add_video_info=False,\n",
    ")\n",
    "\n",
    "result = loader.load()\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'LAfrShnpVIk', 'title': 'What is Retrieval Augmented Generation (RAG) - Augmenting LLMs with a memory', 'description': 'Unknown', 'view_count': 34435, 'thumbnail_url': 'https://i.ytimg.com/vi/LAfrShnpVIk/hq720.jpg', 'publish_date': '2024-01-09 00:00:00', 'length': 580, 'author': \"What's AI by Louis-François Bouchard\"}, page_content=\"when using chat GPT you most probably have encountered responses like I'm sorry but as of my last knowledge update in January 2022 Etc or even responses that are not true at all this is where rag comes into play and says let me help you by injecting more knowledge or content into your interactions with an llm and help it answer the unknown and upcoming questions we hear llms prompts and rag Everywhere by now I think most of us know what an llm and the prompt is but did you know that right now rag is just as important as both of these and Powers most applications you may use involving a chatbot I recently did a poll on the learn AI together Discord Community to find out if people had already studied created or used rag applications and most voted to understand what rag is used for rag is as important as your coursebook for success in a class so understanding what it is is highly relevant in AI an llm or a large language mode model is just an AI model trained on language to talk with humans like GPT 4 used in ch GPT a prompt is simply your interaction with it it's the question you ask it but if you are experiencing issues like hallucinations or biases using such a language model or llm then rag or retrieval augmented Generations comes into play Let's quickly clarify hallucinations first it's when the model returns random things that seems true but aren't simply because it doesn't know the answer in fact a language model is constantly hallucinating it only predicts words in a statistical way it turns out that when they are trained with the entire internet there are so many examples that they manage to accurately predict the next logical words to answer most questions despite this it hallucinates it doesn't really understand what it's talking about and just outputs one word at a time that is probable what is incredible is that most of these hallucinations are actually true and answer our questions however some of them are real hallucinations of fabricated facts or scenarios and that can cause quite a few problems if they are not sufficiently controlled while there are several reasons why llms hallucinate it is mostly because they lack relevant context either because they cannot find the relevant data or don't know which data to refer to for a particular question this is because they were trained to answer and not to say I don't know rag solves this by automatically adding more knowledge or content into your interactions with an llm put simply you have a data set which is required and you use it to help the llm answer the unknown and upcoming user questions this is the simplest form and requires a few steps to make it work but this is the gist of a rag based system you have a user question that is sent to an automatic search in the database for finding relevant information which is then used back along with the question to give back an answer to the user as you can see with frag we use context from the user question question and our knowledge base to answer it this helps with grounding our model to the knowledge we control making it safer and aligned the disadvantage is limiting our answers to our knowledge base which is finite and probably not as big as the Internet it's just like an open book exam you would have in school you already have access to most answers and simply need to know where it is in your knowledge base if you find the answer in the manual it's quite hard to fail the question and write something wrong Jerry Leu CEO of L index gave a very interesting view on how to see rag in my most recent podcast with him if you think about it rag is basically prompt engineering because you're basically figuring out a way to put context into the prompt uh it's just a programmatic way of prompt engineering it's a way of prompting so that you actually get back um some contacts he also said to subscribe to the channel to learn more about AI okay maybe that's just a hallucination actually but you should still do it honestly in rag you first need data or knowledge which can be in the form of documentation but books articles Etc and only allow the llm to search and respond if the answer to the question is inside this knowledge base you have anyways if you have access to accurate information in your school manual why would you try to come up with something different Instead This is currently the best way to control your outputs and make your model safer and aligned basically the best way to ensure you will give the right answer and get your best grade for example we recently built an AI tutor to answer AI related questions we wanted accurate responses for our students both in terms of accuracy to give the right answer and in terms of relevancy so upto-date information with rag you can simply update your database if things have changed there's no big deal if the whole pytorch Library had a big update yesterday scrape it again and update your data set in a second and voila you don't have to retrain the whole model or wait for a gp4 to finally update the noledge card update the overall process of the butt is quite straightforward we validate the question answering it is related to Ai and that our chatbot should answer it then we search in our database to find good and relevant sources and finally use chat GPT to digest those sources and give a good answer for the student if you need safe information from an AI chat but like a medical assistant a tutor a lawyer or an accent you will be using rag for sure well maybe not if you're listening in 2030 but as of now rag is by far the best and safest approach to using a chatbot where you need to give factual and accurate information to build a rag based chatbot or application like our AI tutor we start by ingesting all our data into memory this is done by splitting all the content into chunks of text so split our textual data into fixed or flexible parts for example 500 character parts and processing it to an embedding model like open AI text embedding Adda model this will produce embeddings that are just vectors of numbers repres representing your text it will facilitate your life and allow you to compare text together easily you can save those vectors in a memory then for a new question from a particular user you can repeat this process and answer this means embedding the question using the same approach and compare it with all your current embeddings in your memory here you are basically looking for the most probable answer for this question searching in your memory just like you do for an exam looking through the chapters to find a title that seems relevant to the current exam question once it finds the most similar embedding chat GPT is asked to understand the user's question and intent and only use the retrieved sources of knowledge to answer the question this is how rag reduces hallucination risks and allows you to have upto-date information since you can update your knowledge base as much as you want and chat gbt or your current language model simply picks information from it to answer plus as you see it cites all sources it found on question for you to dive in and learn more which is also a plus when you're trying to learn and understand a new topic then there are still many things to consider like how to determine when to answer a question or not if it is relevant or in your documentation understand new terms or acronyms not in chat gpt's knowledge base find the relevant information more efficiently and accurately etc those concerns are all things we've improved through using various techniques like better chunking methods rank ERS query expansion agents and more that you can learn about in our free Advanced rag course we've built together with toi and active Loop that I Linked In the description below before some of you may ask yes an alternative to rag would be to fine-tune your model on your specific task basically to further train a model on your own data to make it more specific and ingest the knowledge you have rather than always searching in it like memorizing the book before the exam instead of bringing it with you I have a video comparing fine tuning and rag to teach you when you should consider each but in short rag stays relevant with or without fine tuning as it is much cheaper to build and is better to reduce undesired hallucinations as you force the model to give answers based on documentation you control and not simply things it ingested and hopefully will regurgitate correctly as in fine tune models coming back to our open book exams is just like professors making you focus on understanding the core matter and logic and not the knowledge itself as you can always find it in manuals or on Google same here for llms and complimenting them with rag plus even though those models have much better memories than us they are not perfect and they will not retain all the information you give them thus even with a fine tune model on hyp specific data rag remains something worth leveraging before we end this video I just wanted to mention that we discussed both these Topics in depth with coding examples in our llm and rag courses if you want to put this knowledge into practice the link is in the description below and they are completely free I hope you've enjoyed this video and that it helps you understand the goals and principles of rag better if you did please share it with a friend or your network to spread the know Edge and help the channel grow thank you for [Music] watching [Music]\")]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = YoutubeLoader.from_youtube_url(\n",
    "    \"https://www.youtube.com/watch?v=LAfrShnpVIk\",\n",
    "    add_video_info=True,\n",
    ")\n",
    "loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document(metadata={'source': 'https://www.youtube.com/watch?v=TKCMw0utiak&t=0s', 'start_seconds': 0, 'start_timestamp': '00:00:00'}, page_content='♪ Hail to the victors valiant ♪')\n",
      "\n",
      "Document(metadata={'source': 'https://www.youtube.com/watch?v=TKCMw0utiak&t=10s', 'start_seconds': 10, 'start_timestamp': '00:00:10'}, page_content='♪ Hail to the conquering heroes ♪ ♪ Hail, hail to Michigan ♪')\n",
      "\n",
      "Document(metadata={'source': 'https://www.youtube.com/watch?v=TKCMw0utiak&t=20s', 'start_seconds': 20, 'start_timestamp': '00:00:20'}, page_content='♪ The leaders and best ♪')\n",
      "\n",
      "Document(metadata={'source': 'https://www.youtube.com/watch?v=TKCMw0utiak&t=30s', 'start_seconds': 30, 'start_timestamp': '00:00:30'}, page_content='♪ Hail to the victors valiant ♪ ♪ Hail to the conquering heroes ♪')\n",
      "\n",
      "Document(metadata={'source': 'https://www.youtube.com/watch?v=TKCMw0utiak&t=40s', 'start_seconds': 40, 'start_timestamp': '00:00:40'}, page_content='♪ Hail, hail to Michigan ♪ ♪ The champions of the west ♪ ♪ Hail to the victors valiant ♪')\n",
      "\n",
      "Document(metadata={'source': 'https://www.youtube.com/watch?v=TKCMw0utiak&t=50s', 'start_seconds': 50, 'start_timestamp': '00:00:50'}, page_content='♪ Hail to the conquering heroes ♪ ♪ Hail, hail to Michigan ♪')\n",
      "\n",
      "Document(metadata={'source': 'https://www.youtube.com/watch?v=TKCMw0utiak&t=60s', 'start_seconds': 60, 'start_timestamp': '00:01:00'}, page_content='♪ The leaders and best ♪ ♪ Hail to victors valiant ♪ ♪ Hail to the conquering heroes ♪')\n",
      "\n",
      "Document(metadata={'source': 'https://www.youtube.com/watch?v=TKCMw0utiak&t=70s', 'start_seconds': 70, 'start_timestamp': '00:01:10'}, page_content='♪ Hail, hail to Michigan ♪ ♪ The champions of the west ♪')\n",
      "\n",
      "Document(metadata={'source': 'https://www.youtube.com/watch?v=TKCMw0utiak&t=80s', 'start_seconds': 80, 'start_timestamp': '00:01:20'}, page_content=\"(choir clapping rhythmically) - Go blue! (choir clapping rhythmically) Go blue! ♪ It's great to be ♪ ♪ A Michigan Wolverine ♪\\n- Go blue!\")\n",
      "\n",
      "Document(metadata={'source': 'https://www.youtube.com/watch?v=TKCMw0utiak&t=90s', 'start_seconds': 90, 'start_timestamp': '00:01:30'}, page_content=\"♪ It's great to be ♪ ♪ A Michigan Wolverine ♪\\n- Go blue! ♪ It's great to be ♪\\n(choir scatting) ♪ a Michigan Wolverine ♪\\n(choir scatting) ♪ It's great to be ♪\\n(choir scatting)\")\n",
      "\n",
      "Document(metadata={'source': 'https://www.youtube.com/watch?v=TKCMw0utiak&t=100s', 'start_seconds': 100, 'start_timestamp': '00:01:40'}, page_content=\"♪ A Michigan ♪\\n(choir scatting) - Let's go blue! ♪ Hail to the victors valiant ♪ ♪ Hail to the conquering heroes ♪ ♪ Hail, hail to Michigan ♪ ♪ The leaders and best ♪\")\n",
      "\n",
      "Document(metadata={'source': 'https://www.youtube.com/watch?v=TKCMw0utiak&t=110s', 'start_seconds': 110, 'start_timestamp': '00:01:50'}, page_content='♪ Hail to the victors valiant ♪ ♪ Hail to the conquering heroes ♪ ♪ Hail, hail to Michigan ♪ ♪ The champions of the west ♪')\n",
      "\n",
      "Document(metadata={'source': 'https://www.youtube.com/watch?v=TKCMw0utiak&t=120s', 'start_seconds': 120, 'start_timestamp': '00:02:00'}, page_content='♪ Hail to the victors valiant ♪ ♪ Hail to the conquering heroes ♪ ♪ Hail to the blue, hail to the blue ♪ ♪ Hail to the blue, hail to the blue ♪')\n",
      "\n",
      "Document(metadata={'source': 'https://www.youtube.com/watch?v=TKCMw0utiak&t=130s', 'start_seconds': 130, 'start_timestamp': '00:02:10'}, page_content='♪ Hail to the blue, hail to the blue ♪ ♪ Hail to the blue, hail to the blue ♪ ♪ Hail to the blue, hail to the blue ♪ ♪ Hail to the blue, hail ♪')\n",
      "\n",
      "Document(metadata={'source': 'https://www.youtube.com/watch?v=TKCMw0utiak&t=140s', 'start_seconds': 140, 'start_timestamp': '00:02:20'}, page_content='♪ To Michigan ♪ ♪ The champions of the west ♪')\n",
      "\n",
      "Document(metadata={'source': 'https://www.youtube.com/watch?v=TKCMw0utiak&t=150s', 'start_seconds': 150, 'start_timestamp': '00:02:30'}, page_content='♪ Go blue ♪')\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders.youtube import TranscriptFormat\n",
    "\n",
    "loader = YoutubeLoader.from_youtube_url(\n",
    "    \"https://www.youtube.com/watch?v=TKCMw0utiak\",\n",
    "    add_video_info=False,\n",
    "    transcript_format=TranscriptFormat.CHUNKS,\n",
    "    chunk_size_seconds=10,\n",
    ")\n",
    "print(\"\\n\\n\".join(map(repr, loader.load())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recursive URL\n",
    "\n",
    "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abdullah\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\html\\parser.py:171: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  k = self.parse_starttag(i)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %pip install -qU langchain-community beautifulsoup4\n",
    "\n",
    "from langchain_community.document_loaders import RecursiveUrlLoader\n",
    "\n",
    "loader = RecursiveUrlLoader(\n",
    "    \"https://docs.python.org/3.9/\",\n",
    "    timeout=10,\n",
    "    max_depth=2,\n",
    "\n",
    ")\n",
    "\n",
    "docs = loader.load()\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'https://docs.python.org/3.9/',\n",
       " 'content_type': 'text/html',\n",
       " 'title': '3.9.20 Documentation',\n",
       " 'language': None}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<!DOCTYPE html>\n",
      "\n",
      "<html xmlns=\"http://www.w3.org/1999/xhtml\">\n",
      "  <head>\n",
      "    <meta charset=\"utf-8\" /><title>3.9.20 Documentation</title><meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
      "    \n",
      "    <link rel=\"stylesheet\" href=\"_static/pydoctheme.css\" type=\"text/css\" />\n",
      "    <link rel=\"stylesheet\" href=\"_static/pygments.css\" type=\"text/css\" />\n",
      "    \n",
      "    <script id=\"documentation_options\" data-url_root=\"./\" src=\"_static/documentation_options.js\"></script>\n",
      "    <script src=\"_static/jquery.js\"></script>\n",
      "    <script src=\"_static/underscore.js\"></script>\n",
      "    <script src=\"_static/doctools.js\"></script>\n",
      "    <script src=\"_static/language_data.js\"></script>\n",
      "    \n",
      "    <script src=\"_static/sidebar.js\"></script>\n",
      "    \n",
      "    <link rel=\"search\" type=\"application/opensearchdescription+xml\"\n",
      "          title=\"Search within Python 3.9.20 documentation\"\n",
      "          href=\"_static/opensearch.xml\"/>\n",
      "    <link rel=\"author\" title=\"About these documents\" href=\"about.html\" />\n",
      "    <link rel=\"index\" title=\"Index\" href=\"genindex.html\" />\n",
      "    <link rel=\"search\" title=\"Search\" href=\"search.html\" />\n",
      "    <link rel=\"copyright\" title=\"Copyright\" href=\"copyright.html\" />\n",
      "    <link rel=\"canonical\" href=\"https://docs.python.org/3/index.html\" />\n",
      "    \n",
      "      \n",
      "    \n",
      "\n",
      "    \n",
      "    <style>\n",
      "      @media only screen {\n",
      "        table.full-width-table {\n",
      "            width: 100%;\n",
      "        }\n",
      "      }\n",
      "    </style>\n",
      "<link rel=\"shortcut icon\" type=\"image/png\" href=\"_static/py.svg\" />\n",
      "            <script type=\"text/javascript\" src=\"_static/copybutton.js\"></script>\n",
      "            <script type=\"text/javascript\" src=\"_static/menu.js\"></script> \n",
      "\n",
      "  </head>\n",
      "<body>\n",
      "<div class=\"mobile-nav\">\n",
      "    <input type=\"checkbox\" id=\"menuToggler\" class=\"toggler__input\" aria-controls=\"navigation\"\n",
      "           aria-pressed=\"false\" aria-expanded=\"false\" role=\"button\" aria-label=\"Menu\" />\n",
      "    <label for=\"menuToggler\" class=\"toggler__label\">\n",
      "        <span></span>\n",
      "    </label>\n",
      "    <nav class=\"nav-content\" role=\"navigation\">\n",
      "         <a href=\"https://www.python.org/\" class=\"nav-logo\">\n",
      "             <img src=\"_static/py.svg\" alt=\"Logo\"/>\n",
      "         </a>\n",
      "        <div class=\"version_switcher_placeholder\"></div>\n",
      "        <form role=\"search\" class=\"search\" action=\"search.html\" method=\"get\">\n",
      "            <svg xmlns=\"http://www.w3.org/2000/svg\" width=\"20\" height=\"20\" viewBox=\"0 0 24 24\" class=\"search-icon\">\n",
      "                <path fill-rule=\"nonzero\"\n",
      "                        d=\"M15.5 14h-.79l-.28-.27a6.5 6.5 0 001.48-5.34c-.47-2.78-2.79-5-5.59-5.34a6.505 6.505 0 00-7.27 7.27c.34 2.8 2.56 5.12 5.34 5.59a6.5 6.5 0 005.34-1.48l.27.28v.79l4.25 4.25c.41.41 1.08.41 1.49 0 .41-.41.41-1.08 0-1.49L15.5 14zm-6 0C7.01 14 5 11.99 5 9.5S7.01 5 9.5 5 14 7.01 14 9.5 11.99 14 9.5 14z\" fill=\"#444\"></path>\n",
      "            </svg>\n",
      "            <input type=\"text\" name=\"q\" aria-label=\"Quick search\"/>\n",
      "            <input type=\"submit\" value=\"Go\"/>\n",
      "        </form>\n",
      "    </nav>\n",
      "    <div class=\"menu-wrapper\">\n",
      "        <nav class=\"menu\" role=\"navigation\" aria-label=\"main navigation\">\n",
      "            <div class=\"language_switcher_placeholder\"></div>\n",
      "\n",
      "\n",
      "<h3>Download</h3>\n",
      "<p><a href=\"download.html\">Download these documents</a></p>\n",
      "\n",
      "\n",
      "<h3>Docs by version</h3>\n",
      "<ul>\n",
      "  \n",
      "  <li><a href=\"https://docs.python.org/3.14/\">Python 3.14 (in development)</a></li>\n",
      "  \n",
      "  <li><a href=\"https://docs.python.org/3.13/\">Python 3.13 (pre-release)</a></li>\n",
      "  \n",
      "  <li><a href=\"https://docs.python.org/3.12/\">Python 3.12 (stable)</a></li>\n",
      "  \n",
      "  <li><a href=\"https://docs.python.org/3.11/\">Python 3.11 (security-fixes)</a></li>\n",
      "  \n",
      "  <li><a href=\"https://docs.python.org/3.10/\">Python 3.10 (security-fixes)</a></li>\n",
      "  \n",
      "  <li><a href=\"https://docs.python.org/3.9/\">Python 3.9 (security-fixes)</a></li>\n",
      "  \n",
      "  <li><a href=\"https://docs.python.org/3.8/\">Python 3.8 (security-fixes)</a></li>\n",
      "  \n",
      "  <li><a href=\"https://docs.python.org/3.7/\">Python 3.7 (EOL)</a></li>\n",
      "  \n",
      "  <li><a href=\"https://docs.python.org/3.6/\">Python 3.6 (EOL)</a></li>\n",
      "  \n",
      "  <li><a href=\"https://docs.python.org/3.5/\">Python 3.5 (EOL)</a></li>\n",
      "  \n",
      "  <li><a href=\"https://docs.python.org/3.4/\">Python 3.4 (EOL)</a></li>\n",
      "  \n",
      "  <li><a href=\"https://docs.python.org/3.3/\">Python 3.3 (EOL)</a></li>\n",
      "  \n",
      "  <li><a href=\"https://docs.python.org/3.2/\">Python 3.2 (EOL)</a></li>\n",
      "  \n",
      "  <li><a href=\"https://docs.python.org/3.1/\">Python 3.1 (EOL)</a></li>\n",
      "  \n",
      "  <li><a href=\"https://docs.python.org/3.0/\">Python 3.0 (EOL)</a></li>\n",
      "  \n",
      "  <li><a href=\"https://docs.python.org/2.7/\">Python 2.7 (EOL)</a></li>\n",
      "  \n",
      "  <li><a href=\"https://docs.python.org/2.6/\">Python 2.6 (EOL)</a></li>\n",
      "  \n",
      "  <li><a href=\"https://www.python.org/doc/versions/\">All versions</a></li>\n",
      "</ul>\n",
      "\n",
      "\n",
      "<h3>Other resources</h3>\n",
      "<ul>\n",
      "  \n",
      "  <li><a href=\"https://peps.python.org\">PEP Index</a></li>\n",
      "  <li><a href=\"https://wiki.python.org/moin/BeginnersGuide\">Beginner's Guide</a></li>\n",
      "  <li><a href=\"https://wiki.python.org/moin/PythonBooks\">Book List</a></li>\n",
      "  <li><a href=\"https://www.python.org/doc/av/\">Audio/Visual Talks</a></li>\n",
      "  <li><a href=\"https://devguide.python.org/\">Python Developer’s Guide</a></li>\n",
      "</ul>\n",
      "        </nav>\n",
      "    </div>\n",
      "</div>\n",
      "\n",
      "  \n",
      "    <div class=\"related\" role=\"navigation\" aria-label=\"related navigation\">\n",
      "      <h3>Navigation</h3>\n",
      "      <ul>\n",
      "        <li class=\"right\" style=\"margin-right: 10px\">\n",
      "          <a href=\"genindex.html\" title=\"General Index\"\n",
      "             accesskey=\"I\">index</a></li>\n",
      "        <li class=\"right\" >\n",
      "          <a href=\"py-modindex.html\" title=\"Python Module Index\"\n",
      "             >modules</a> |</li>\n",
      "\n",
      "          <li><img src=\"_static/py.svg\" alt=\"python logo\" style=\"vertical-align: middle; margin-top: -1px\"/></li>\n",
      "          <li><a href=\"https://www.python.org/\">Python</a> &#187;</li>\n",
      "          <li class=\"switchers\">\n",
      "            <div class=\"language_switcher_placeholder\"></div>\n",
      "            <div class=\"version_switcher_placeholder\"></div>\n",
      "          </li>\n",
      "          <li>\n",
      "              \n",
      "          </li>\n",
      "    <li id=\"cpython-language-and-version\">\n",
      "      <a href=\"#\">3.9.20 Documentation</a> &#187;\n",
      "    </li>\n",
      "\n",
      "                <li class=\"right\">\n",
      "                    \n",
      "\n",
      "    <div class=\"inline-search\" role=\"search\">\n",
      "        <form class=\"inline-search\" action=\"search.html\" method=\"get\">\n",
      "          <input placeholder=\"Quick search\" aria-label=\"Quick search\" type=\"text\" name=\"q\" />\n",
      "          <input type=\"submit\" value=\"Go\" />\n",
      "          <input type=\"hidden\" name=\"check_keywords\" value=\"yes\" />\n",
      "          <input type=\"hidden\" name=\"area\" value=\"default\" />\n",
      "        </form>\n",
      "    </div>\n",
      "                     |\n",
      "                </li>\n",
      "            \n",
      "      </ul>\n",
      "    </div>    \n",
      "\n",
      "    <div class=\"document\">\n",
      "      <div class=\"documentwrapper\">\n",
      "        <div class=\"bodywrapper\">\n",
      "          <div class=\"body\" role=\"main\">\n",
      "            \n",
      "  <h1>Python 3.9.20 documentation</h1>\n",
      "  <p>\n",
      "  Welcome! This is the official documentation for Python 3.9.20.\n",
      "  </p>\n",
      "  <p><strong>Parts of the documentation:</strong></p>\n",
      "  <table class=\"contentstable\" align=\"center\"><tr>\n",
      "    <td width=\"50%\">\n",
      "      <p class=\"biglink\"><a class=\"biglink\" href=\"whatsnew/3.9.html\">What's new in Python 3.9?</a><br/>\n",
      "        <span class=\"linkdescr\"> or <a href=\"whatsnew/index.html\">all \"What's new\" documents</a> since 2.0</span></p>\n",
      "      <p class=\"biglink\"><a class=\"biglink\" href=\"tutorial/index.html\">Tutorial</a><br/>\n",
      "         <span class=\"linkdescr\">start here</span></p>\n",
      "      <p class=\"biglink\"><a class=\"biglink\" href=\"library/index.html\">Library Reference</a><br/>\n",
      "         <span class=\"linkdescr\">keep this under your pillow</span></p>\n",
      "      <p class=\"biglink\"><a class=\"biglink\" href=\"reference/index.html\">Language Reference</a><br/>\n",
      "         <span class=\"linkdescr\">describes syntax and language elements</span></p>\n",
      "      <p class=\"biglink\"><a class=\"biglink\" href=\"using/index.html\">Python Setup and Usage</a><br/>\n",
      "         <span class=\"linkdescr\">how to use Python on different platforms</span></p>\n",
      "      <p class=\"biglink\"><a class=\"biglink\" href=\"howto/index.html\">Python HOWTOs</a><br/>\n",
      "         <span class=\"linkdescr\">in-depth documents on specific topics</span></p>\n",
      "    </td><td width=\"50%\">\n",
      "      <p class=\"biglink\"><a class=\"biglink\" href=\"installing/index.html\">Installing Python Modules</a><br/>\n",
      "         <span class=\"linkdescr\">installing from the Python Package Index &amp; other sources</span></p>\n",
      "      <p class=\"biglink\"><a class=\"biglink\" href=\"distributing/index.html\">Distributing Python Modules</a><br/>\n",
      "         <span class=\"linkdescr\">publishing modules for installation by others</span></p>\n",
      "      <p class=\"biglink\"><a class=\"biglink\" href=\"extending/index.html\">Extending and Embedding</a><br/>\n",
      "         <span class=\"linkdescr\">tutorial for C/C++ programmers</span></p>\n",
      "      <p class=\"biglink\"><a class=\"biglink\" href=\"c-api/index.html\">Python/C API</a><br/>\n",
      "         <span class=\"linkdescr\">reference for C/C++ programmers</span></p>\n",
      "      <p class=\"biglink\"><a class=\"biglink\" href=\"faq/index.html\">FAQs</a><br/>\n",
      "         <span class=\"linkdescr\">frequently asked questions (with answers!)</span></p>\n",
      "    </td></tr>\n",
      "  </table>\n",
      "\n",
      "  <p><strong>Indices and tables:</strong></p>\n",
      "  <table class=\"contentstable\" align=\"center\"><tr>\n",
      "    <td width=\"50%\">\n",
      "      <p class=\"biglink\"><a class=\"biglink\" href=\"py-modindex.html\">Global Module Index</a><br/>\n",
      "         <span class=\"linkdescr\">quick access to all modules</span></p>\n",
      "      <p class=\"biglink\"><a class=\"biglink\" href=\"genindex.html\">General Index</a><br/>\n",
      "         <span class=\"linkdescr\">all functions, classes, terms</span></p>\n",
      "      <p class=\"biglink\"><a class=\"biglink\" href=\"glossary.html\">Glossary</a><br/>\n",
      "         <span class=\"linkdescr\">the most important terms explained</span></p>\n",
      "    </td><td width=\"50%\">\n",
      "      <p class=\"biglink\"><a class=\"biglink\" href=\"search.html\">Search page</a><br/>\n",
      "         <span class=\"linkdescr\">search this documentation</span></p>\n",
      "      <p class=\"biglink\"><a class=\"biglink\" href=\"contents.html\">Complete Table of Contents</a><br/>\n",
      "         <span class=\"linkdescr\">lists all sections and subsections</span></p>\n",
      "    </td></tr>\n",
      "  </table>\n",
      "\n",
      "  <p><strong>Meta information:</strong></p>\n",
      "  <table class=\"contentstable\" align=\"center\"><tr>\n",
      "    <td width=\"50%\">\n",
      "      <p class=\"biglink\"><a class=\"biglink\" href=\"bugs.html\">Reporting bugs</a></p>\n",
      "      <p class=\"biglink\"><a class=\"biglink\" href=\"https://devguide.python.org/docquality/#helping-with-documentation\">Contributing to Docs</a></p>\n",
      "      <p class=\"biglink\"><a class=\"biglink\" href=\"about.html\">About the documentation</a></p>\n",
      "    </td><td width=\"50%\">\n",
      "      <p class=\"biglink\"><a class=\"biglink\" href=\"license.html\">History and License of Python</a></p>\n",
      "      <p class=\"biglink\"><a class=\"biglink\" href=\"copyright.html\">Copyright</a></p>\n",
      "    </td></tr>\n",
      "  </table>\n",
      "\n",
      "          </div>\n",
      "        </div>\n",
      "      </div>\n",
      "      <div class=\"sphinxsidebar\" role=\"navigation\" aria-label=\"main navigation\">\n",
      "        <div class=\"sphinxsidebarwrapper\">\n",
      "\n",
      "\n",
      "<h3>Download</h3>\n",
      "<p><a href=\"download.html\">Download these documents</a></p>\n",
      "\n",
      "\n",
      "<h3>Docs by version</h3>\n",
      "<ul>\n",
      "  \n",
      "  <li><a href=\"https://docs.python.org/3.14/\">Python 3.14 (in development)</a></li>\n",
      "  \n",
      "  <li><a href=\"https://docs.python.org/3.13/\">Python 3.13 (pre-release)</a></li>\n",
      "  \n",
      "  <li><a href=\"https://docs.python.org/3.12/\">Python 3.12 (stable)</a></li>\n",
      "  \n",
      "  <li><a href=\"https://docs.python.org/3.11/\">Python 3.11 (security-fixes)</a></li>\n",
      "  \n",
      "  <li><a href=\"https://docs.python.org/3.10/\">Python 3.10 (security-fixes)</a></li>\n",
      "  \n",
      "  <li><a href=\"https://docs.python.org/3.9/\">Python 3.9 (security-fixes)</a></li>\n",
      "  \n",
      "  <li><a href=\"https://docs.python.org/3.8/\">Python 3.8 (security-fixes)</a></li>\n",
      "  \n",
      "  <li><a href=\"https://docs.python.org/3.7/\">Python 3.7 (EOL)</a></li>\n",
      "  \n",
      "  <li><a href=\"https://docs.python.org/3.6/\">Python 3.6 (EOL)</a></li>\n",
      "  \n",
      "  <li><a href=\"https://docs.python.org/3.5/\">Python 3.5 (EOL)</a></li>\n",
      "  \n",
      "  <li><a href=\"https://docs.python.org/3.4/\">Python 3.4 (EOL)</a></li>\n",
      "  \n",
      "  <li><a href=\"https://docs.python.org/3.3/\">Python 3.3 (EOL)</a></li>\n",
      "  \n",
      "  <li><a href=\"https://docs.python.org/3.2/\">Python 3.2 (EOL)</a></li>\n",
      "  \n",
      "  <li><a href=\"https://docs.python.org/3.1/\">Python 3.1 (EOL)</a></li>\n",
      "  \n",
      "  <li><a href=\"https://docs.python.org/3.0/\">Python 3.0 (EOL)</a></li>\n",
      "  \n",
      "  <li><a href=\"https://docs.python.org/2.7/\">Python 2.7 (EOL)</a></li>\n",
      "  \n",
      "  <li><a href=\"https://docs.python.org/2.6/\">Python 2.6 (EOL)</a></li>\n",
      "  \n",
      "  <li><a href=\"https://www.python.org/doc/versions/\">All versions</a></li>\n",
      "</ul>\n",
      "\n",
      "\n",
      "<h3>Other resources</h3>\n",
      "<ul>\n",
      "  \n",
      "  <li><a href=\"https://peps.python.org\">PEP Index</a></li>\n",
      "  <li><a href=\"https://wiki.python.org/moin/BeginnersGuide\">Beginner's Guide</a></li>\n",
      "  <li><a href=\"https://wiki.python.org/moin/PythonBooks\">Book List</a></li>\n",
      "  <li><a href=\"https://www.python.org/doc/av/\">Audio/Visual Talks</a></li>\n",
      "  <li><a href=\"https://devguide.python.org/\">Python Developer’s Guide</a></li>\n",
      "</ul>\n",
      "        </div>\n",
      "      </div>\n",
      "      <div class=\"clearer\"></div>\n",
      "    </div>  \n",
      "    <div class=\"related\" role=\"navigation\" aria-label=\"related navigation\">\n",
      "      <h3>Navigation</h3>\n",
      "      <ul>\n",
      "        <li class=\"right\" style=\"margin-right: 10px\">\n",
      "          <a href=\"genindex.html\" title=\"General Index\"\n",
      "             >index</a></li>\n",
      "        <li class=\"right\" >\n",
      "          <a href=\"py-modindex.html\" title=\"Python Module Index\"\n",
      "             >modules</a> |</li>\n",
      "\n",
      "          <li><img src=\"_static/py.svg\" alt=\"python logo\" style=\"vertical-align: middle; margin-top: -1px\"/></li>\n",
      "          <li><a href=\"https://www.python.org/\">Python</a> &#187;</li>\n",
      "          <li class=\"switchers\">\n",
      "            <div class=\"language_switcher_placeholder\"></div>\n",
      "            <div class=\"version_switcher_placeholder\"></div>\n",
      "          </li>\n",
      "          <li>\n",
      "              \n",
      "          </li>\n",
      "    <li id=\"cpython-language-and-version\">\n",
      "      <a href=\"#\">3.9.20 Documentation</a> &#187;\n",
      "    </li>\n",
      "\n",
      "                <li class=\"right\">\n",
      "                    \n",
      "\n",
      "    <div class=\"inline-search\" role=\"search\">\n",
      "        <form class=\"inline-search\" action=\"search.html\" method=\"get\">\n",
      "          <input placeholder=\"Quick search\" aria-label=\"Quick search\" type=\"text\" name=\"q\" />\n",
      "          <input type=\"submit\" value=\"Go\" />\n",
      "          <input type=\"hidden\" name=\"check_keywords\" value=\"yes\" />\n",
      "          <input type=\"hidden\" name=\"area\" value=\"default\" />\n",
      "        </form>\n",
      "    </div>\n",
      "                     |\n",
      "                </li>\n",
      "            \n",
      "      </ul>\n",
      "    </div>  \n",
      "    <div class=\"footer\">\n",
      "    &copy; <a href=\"copyright.html\">Copyright</a> 2001-2024, Python Software Foundation.\n",
      "    <br />\n",
      "    This page is licensed under the Python Software Foundation License Version 2.\n",
      "    <br />\n",
      "    Examples, recipes, and other code in the documentation are additionally licensed under the Zero Clause BSD License.\n",
      "    <br />\n",
      "    See <a href=\"/license.html\">History and License</a> for more information.<br />\n",
      "    <br />\n",
      "\n",
      "    The Python Software Foundation is a non-profit corporation.\n",
      "<a href=\"https://www.python.org/psf/donations/\">Please donate.</a>\n",
      "<br />\n",
      "    <br />\n",
      "\n",
      "    Last updated on Sep 09, 2024.\n",
      "    <a href=\"/bugs.html\">Found a bug</a>?\n",
      "    <br />\n",
      "\n",
      "    Created using <a href=\"https://www.sphinx-doc.org/\">Sphinx</a> 2.4.4.\n",
      "    </div>\n",
      "\n",
      "    <script type=\"text/javascript\" src=\"_static/switchers.js\"></script>\n",
      "  </body>\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install lxml\n",
    "import re\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def bs4_extractor(html: str) -> str:\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    return re.sub(r\"\\n\\n+\", \"\\n\\n\", soup.text).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9.20 Documentation\n",
      "\n",
      "Download\n",
      "Download these documents\n",
      "Docs by version\n",
      "\n",
      "Python 3.14 (in development)\n",
      "Python 3.13 (pre-release)\n",
      "Python 3.12 (stable)\n",
      "Python 3.11 (security-fixes)\n",
      "Python 3.10 (security-fixes)\n",
      "Python 3.9 (security-fixes)\n",
      "Python 3.8 (security-fixes)\n",
      "Python 3.7 (EOL)\n",
      "Python 3.6 (EOL)\n",
      "Python 3.5 (EOL)\n",
      "Python 3.4 (EOL)\n",
      "Python 3.3 (EOL)\n",
      "Python 3.2 (EOL)\n",
      "Python 3.1 (EOL)\n",
      "Python 3.0 (EOL)\n",
      "Python 2.7 (EOL)\n",
      "Python 2.6 (EOL)\n",
      "All versions\n",
      "\n",
      "Other resources\n",
      "\n",
      "PEP Index\n",
      "Beginner's Guide\n",
      "Book List\n",
      "Audio/Visual Talks\n",
      "Python Developer’s Guide\n",
      "\n",
      "Navigation\n",
      "\n",
      "index\n",
      "\n",
      "modules |\n",
      "\n",
      "Python »\n",
      "\n",
      "3.9.20 Documentation »\n",
      "    \n",
      "\n",
      "                     |\n",
      "                \n",
      "\n",
      "Python 3.9.20 documentation\n",
      "\n",
      "  Welcome! This is the official documentation for Python 3.9.20.\n",
      "  \n",
      "Parts of the documentation:\n",
      "\n",
      "What's new in Python 3.9?\n",
      " or all \"What's new\" documents since 2.0\n",
      "Tutorial\n",
      "start here\n",
      "Library Reference\n",
      "keep this under your pillow\n",
      "Language Reference\n",
      "describes syntax and language elements\n",
      "Python Setup and Usage\n",
      "how to use Python on different platforms\n",
      "Python HOWTOs\n",
      "in-depth documents on specific topics\n",
      "\n",
      "Installing Python Modules\n",
      "installing from the Python Package Index & other sources\n",
      "Distributing Python Modules\n",
      "publishing modules for installation by others\n",
      "Extending and Embedding\n",
      "tutorial for C/C++ programmers\n",
      "Python/C API\n",
      "reference for C/C++ programmers\n",
      "FAQs\n",
      "frequently asked questions (with answers!)\n",
      "\n",
      "Indices and tables:\n",
      "\n",
      "Global Module Index\n",
      "quick access to all modules\n",
      "General Index\n",
      "all functions, classes, terms\n",
      "Glossary\n",
      "the most important terms explained\n",
      "\n",
      "Search page\n",
      "search this documentation\n",
      "Complete Table of Contents\n",
      "lists all sections and subsections\n",
      "\n",
      "Meta information:\n",
      "\n",
      "Reporting bugs\n",
      "Contributing to Docs\n",
      "About the documentation\n",
      "\n",
      "History and License of Python\n",
      "Copyright\n",
      "\n",
      "Download\n",
      "Download these documents\n",
      "Docs by version\n",
      "\n",
      "Python 3.14 (in development)\n",
      "Python 3.13 (pre-release)\n",
      "Python 3.12 (stable)\n",
      "Python 3.11 (security-fixes)\n",
      "Python 3.10 (security-fixes)\n",
      "Python 3.9 (security-fixes)\n",
      "Python 3.8 (security-fixes)\n",
      "Python 3.7 (EOL)\n",
      "Python 3.6 (EOL)\n",
      "Python 3.5 (EOL)\n",
      "Python 3.4 (EOL)\n",
      "Python 3.3 (EOL)\n",
      "Python 3.2 (EOL)\n",
      "Python 3.1 (EOL)\n",
      "Python 3.0 (EOL)\n",
      "Python 2.7 (EOL)\n",
      "Python 2.6 (EOL)\n",
      "All versions\n",
      "\n",
      "Other resources\n",
      "\n",
      "PEP Index\n",
      "Beginner's Guide\n",
      "Book List\n",
      "Audio/Visual Talks\n",
      "Python Developer’s Guide\n",
      "\n",
      "Navigation\n",
      "\n",
      "index\n",
      "\n",
      "modules |\n",
      "\n",
      "Python »\n",
      "\n",
      "3.9.20 Documentation »\n",
      "    \n",
      "\n",
      "                     |\n",
      "                \n",
      "\n",
      "    © Copyright 2001-2024, Python Software Foundation.\n",
      "    \n",
      "    This page is licensed under the Python Software Foundation License Version 2.\n",
      "    \n",
      "    Examples, recipes, and other code in the documentation are additionally licensed under the Zero Clause BSD License.\n",
      "    \n",
      "    See History and License for more information.\n",
      "\n",
      "    The Python Software Foundation is a non-profit corporation.\n",
      "Please donate.\n",
      "\n",
      "    Last updated on Sep 09, 2024.\n",
      "    Found a bug?\n",
      "    \n",
      "\n",
      "    Created using Sphinx 2.4.4.\n"
     ]
    }
   ],
   "source": [
    "content = bs4_extractor(docs[0].page_content)\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abdullah\\AppData\\Local\\Temp\\ipykernel_10976\\171819319.py:8: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  soup = BeautifulSoup(html, \"lxml\")\n",
      "C:\\Users\\abdullah\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\html\\parser.py:171: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  k = self.parse_starttag(i)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python/C API Reference Manual — Python 3.9.20 documentation\n",
      "\n",
      "Previous topic\n",
      "1. Embedding Python in Another Application\n",
      "Next topic\n",
      "Introduction\n",
      "\n",
      "This Page\n",
      "\n",
      "Report a Bug\n",
      "\n",
      "Show Source\n",
      "        \n",
      "\n",
      "Navigation\n",
      "\n",
      "index\n",
      "\n",
      "modules |\n",
      "\n",
      "next |\n",
      "\n",
      "previous |\n",
      "\n",
      "Python »\n",
      "\n",
      "3.9.20 Documentation »\n",
      "    \n",
      "\n",
      "                     |\n",
      "                \n",
      "\n",
      "Python/C API Reference ManualÂ¶\n",
      "This manual documents the API used by C and C++ programmers who want to write\n",
      "extension modules or embed Python.  It is a companion to Extending and Embedding the Python Interpreter,\n",
      "which describes the general principles of extension writing but does not\n",
      "document the API functions in detail.\n",
      "\n",
      "Introduction\n",
      "Coding standards\n",
      "Include Files\n",
      "Useful macros\n",
      "Objects, Types and Reference Counts\n",
      "Exceptions\n",
      "Embedding Python\n",
      "Debugging Builds\n",
      "\n",
      "Stable Application Binary Interface\n",
      "The Very High Level Layer\n",
      "Reference Counting\n",
      "Exception Handling\n",
      "Printing and clearing\n",
      "Raising exceptions\n",
      "Issuing warnings\n",
      "Querying the error indicator\n",
      "Signal Handling\n",
      "Exception Classes\n",
      "Exception Objects\n",
      "Unicode Exception Objects\n",
      "Recursion Control\n",
      "Standard Exceptions\n",
      "Standard Warning Categories\n",
      "\n",
      "Utilities\n",
      "Operating System Utilities\n",
      "System Functions\n",
      "Process Control\n",
      "Importing Modules\n",
      "Data marshalling support\n",
      "Parsing arguments and building values\n",
      "String conversion and formatting\n",
      "Reflection\n",
      "Codec registry and support functions\n",
      "\n",
      "Abstract Objects Layer\n",
      "Object Protocol\n",
      "Call Protocol\n",
      "Number Protocol\n",
      "Sequence Protocol\n",
      "Mapping Protocol\n",
      "Iterator Protocol\n",
      "Buffer Protocol\n",
      "Old Buffer Protocol\n",
      "\n",
      "Concrete Objects Layer\n",
      "Fundamental Objects\n",
      "Numeric Objects\n",
      "Sequence Objects\n",
      "Container Objects\n",
      "Function Objects\n",
      "Other Objects\n",
      "\n",
      "Initialization, Finalization, and Threads\n",
      "Before Python Initialization\n",
      "Global configuration variables\n",
      "Initializing and finalizing the interpreter\n",
      "Process-wide parameters\n",
      "Thread State and the Global Interpreter Lock\n",
      "Sub-interpreter support\n",
      "Asynchronous Notifications\n",
      "Profiling and Tracing\n",
      "Advanced Debugger Support\n",
      "Thread Local Storage Support\n",
      "\n",
      "Python Initialization Configuration\n",
      "PyWideStringList\n",
      "PyStatus\n",
      "PyPreConfig\n",
      "Preinitialization with PyPreConfig\n",
      "PyConfig\n",
      "Initialization with PyConfig\n",
      "Isolated Configuration\n",
      "Python Configuration\n",
      "Path Configuration\n",
      "Py_RunMain()\n",
      "Py_GetArgcArgv()\n",
      "Multi-Phase Initialization Private Provisional API\n",
      "\n",
      "Memory Management\n",
      "Overview\n",
      "Raw Memory Interface\n",
      "Memory Interface\n",
      "Object allocators\n",
      "Default Memory Allocators\n",
      "Customize Memory Allocators\n",
      "The pymalloc allocator\n",
      "tracemalloc C API\n",
      "Examples\n",
      "\n",
      "Object Implementation Support\n",
      "Allocating Objects on the Heap\n",
      "Common Object Structures\n",
      "Type Objects\n",
      "Number Object Structures\n",
      "Mapping Object Structures\n",
      "Sequence Object Structures\n",
      "Buffer Object Structures\n",
      "Async Object Structures\n",
      "Slot Type typedefs\n",
      "Examples\n",
      "Supporting Cyclic Garbage Collection\n",
      "\n",
      "API and ABI Versioning\n",
      "\n",
      "Previous topic\n",
      "1. Embedding Python in Another Application\n",
      "Next topic\n",
      "Introduction\n",
      "\n",
      "This Page\n",
      "\n",
      "Report a Bug\n",
      "\n",
      "Show Source\n",
      "        \n",
      "\n",
      "Navigation\n",
      "\n",
      "index\n",
      "\n",
      "modules |\n",
      "\n",
      "next |\n",
      "\n",
      "previous |\n",
      "\n",
      "Python »\n",
      "\n",
      "3.9.20 Documentation »\n",
      "    \n",
      "\n",
      "                     |\n",
      "                \n",
      "\n",
      "    © Copyright 2001-2024, Python Software Foundation.\n",
      "    \n",
      "    This page is licensed under the Python Software Foundation License Version 2.\n",
      "    \n",
      "    Examples, recipes, and other code in the documentation are additionally licensed under the Zero Clause BSD License.\n",
      "    \n",
      "    See History and License for more information.\n",
      "\n",
      "    The Python Software Foundation is a non-profit corporation.\n",
      "Please donate.\n",
      "\n",
      "    Last updated on Sep 09, 2024.\n",
      "    Found a bug?\n",
      "    \n",
      "\n",
      "    Created using Sphinx 2.4.4.\n"
     ]
    }
   ],
   "source": [
    "loader = RecursiveUrlLoader(\"https://docs.python.org/3.9/\", extractor=bs4_extractor)\n",
    "docs = loader.load()\n",
    "print(docs[1].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Glossary — Python 3.9.20 documentation\n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "      \n",
      "    \n",
      "\n",
      "    \n",
      "    \n",
      "      @media only screen {\n",
      "        table.full-width-table {\n",
      "            width: 100%;\n",
      "        }\n",
      "      }\n",
      "    \n",
      "\n",
      "            \n",
      "             \n",
      "\n",
      "  \n",
      "\n",
      "    \n",
      "    \n",
      "        \n",
      "    \n",
      "    \n",
      "         \n",
      "             \n",
      "         \n",
      "        \n",
      "        \n",
      "            \n",
      "                \n",
      "            \n",
      "            \n",
      "            \n",
      "        \n",
      "    \n",
      "    \n",
      "        \n",
      "            \n",
      "  Previous topic\n",
      "  “Why is Python Installed on my Computer?” FAQ\n",
      "  Next topic\n",
      "  About these documents\n",
      "  \n",
      "    This Page\n",
      "    \n",
      "      Report a Bug\n",
      "      \n",
      "        Show Source\n",
      "        \n",
      "      \n",
      "    \n",
      "  \n",
      "        \n",
      "    \n",
      "\n",
      "  \n",
      "    \n",
      "      Navigation\n",
      "      \n",
      "        \n",
      "          index\n",
      "        \n",
      "          modules |\n",
      "        \n",
      "          next |\n",
      "        \n",
      "          previous |\n",
      "\n",
      "          \n",
      "          Python »\n",
      "          \n",
      "            \n",
      "            \n",
      "          \n",
      "          \n",
      "              \n",
      "          \n",
      "    \n",
      "      3.9.20 Documentation »\n",
      "    \n",
      "\n",
      "                \n",
      "                    \n",
      "\n",
      "    \n",
      "        \n",
      "          \n",
      "          \n",
      "          \n",
      "          \n",
      "        \n",
      "    \n",
      "                     |\n",
      "                \n",
      "            \n",
      "      \n",
      "        \n",
      "\n",
      "    \n",
      "      \n",
      "        \n",
      "          \n",
      "            \n",
      "  \n",
      "Glossary¶\n",
      "\n",
      ">>>The default Python prompt of the interactive shell.  Often seen for code\n",
      "examples which can be executed interactively in the interpreter.\n",
      "\n",
      "...Can refer to:\n",
      "\n",
      "The default Python prompt of the interactive shell when entering the\n",
      "code for an indented code block, when within a pair of matching left and\n",
      "right delimiters (parentheses, square brackets, curly braces or triple\n",
      "quotes), or after specifying a decorator.\n",
      "The Ellipsis built-in constant.\n",
      "\n",
      "2to3A tool that tries to convert Python 2.x code to Python 3.x code by\n",
      "handling most of the incompatibilities which can be detected by parsing the\n",
      "source and traversing the parse tree.\n",
      "2to3 is available in the standard library as lib2to3; a standalone\n",
      "entry point is provided as Tools/scripts/2to3.  See\n",
      "2to3 - Automated Python 2 to 3 code translation.\n",
      "\n",
      "abstract base classAbstract base classes complement duck-typing by\n",
      "providing a way to define interfaces when other techniques like\n",
      "hasattr() would be clumsy or subtly wrong (for example with\n",
      "magic methods).  ABCs introduce virtual\n",
      "subclasses, which are classes that don’t inherit from a class but are\n",
      "still recognized by isinstance() and issubclass(); see the\n",
      "abc module documentation.  Python comes with many built-in ABCs for\n",
      "data structures (in the collections.abc module), numbers (in the\n",
      "numbers module), streams (in the io module), import finders\n",
      "and loaders (in the importlib.abc module).  You can create your own\n",
      "ABCs with the abc module.\n",
      "\n",
      "annotationA label associated with a variable, a class\n",
      "attribute or a function parameter or return value,\n",
      "used by convention as a type hint.\n",
      "Annotations of local variables cannot be accessed at runtime, but\n",
      "annotations of global variables, class attributes, and functions\n",
      "are stored in the __annotations__\n",
      "special attribute of modules, classes, and functions,\n",
      "respectively.\n",
      "See variable annotation, function annotation, PEP 484\n",
      "and PEP 526, which describe this functionality.\n",
      "\n",
      "argumentA value passed to a function (or method) when calling the\n",
      "function.  There are two kinds of argument:\n",
      "\n",
      "keyword argument: an argument preceded by an identifier (e.g.\n",
      "name=) in a function call or passed as a value in a dictionary\n",
      "preceded by **.  For example, 3 and 5 are both keyword\n",
      "arguments in the following calls to complex():\n",
      "complex(real=3, imag=5)\n",
      "complex(**{'real': 3, 'imag': 5})\n",
      "\n",
      "positional argument: an argument that is not a keyword argument.\n",
      "Positional arguments can appear at the beginning of an argument list\n",
      "and/or be passed as elements of an iterable preceded by *.\n",
      "For example, 3 and 5 are both positional arguments in the\n",
      "following calls:\n",
      "complex(3, 5)\n",
      "complex(*(3, 5))\n",
      "\n",
      "Arguments are assigned to the named local variables in a function body.\n",
      "See the Calls section for the rules governing this assignment.\n",
      "Syntactically, any expression can be used to represent an argument; the\n",
      "evaluated value is assigned to the local variable.\n",
      "See also the parameter glossary entry, the FAQ question on\n",
      "the difference between arguments and parameters, and PEP 362.\n",
      "\n",
      "asynchronous context managerAn object which controls the environment seen in an\n",
      "async with statement by defining __aenter__() and\n",
      "__aexit__() methods.  Introduced by PEP 492.\n",
      "\n",
      "asynchronous generatorA function which returns an asynchronous generator iterator.  It\n",
      "looks like a coroutine function defined with async def except\n",
      "that it contains yield expressions for producing a series of\n",
      "values usable in an async for loop.\n",
      "Usually refers to an asynchronous generator function, but may refer to an\n",
      "asynchronous generator iterator in some contexts.  In cases where the\n",
      "intended meaning isn’t clear, using the full terms avoids ambiguity.\n",
      "An asynchronous generator function may contain await\n",
      "expressions as well as async for, and async with\n",
      "statements.\n",
      "\n",
      "asynchronous generator iteratorAn object created by a asynchronous generator function.\n",
      "This is an asynchronous iterator which when called using the\n",
      "__anext__() method returns an awaitable object which will execute\n",
      "the body of the asynchronous generator function until the next\n",
      "yield expression.\n",
      "Each yield temporarily suspends processing, remembering the\n",
      "location execution state (including local variables and pending\n",
      "try-statements).  When the asynchronous generator iterator effectively\n",
      "resumes with another awaitable returned by __anext__(), it\n",
      "picks up where it left off.  See PEP 492 and PEP 525.\n",
      "\n",
      "asynchronous iterableAn object, that can be used in an async for statement.\n",
      "Must return an asynchronous iterator from its\n",
      "__aiter__() method.  Introduced by PEP 492.\n",
      "\n",
      "asynchronous iteratorAn object that implements the __aiter__() and __anext__()\n",
      "methods.  __anext__ must return an awaitable object.\n",
      "async for resolves the awaitables returned by an asynchronous\n",
      "iterator’s __anext__() method until it raises a\n",
      "StopAsyncIteration exception.  Introduced by PEP 492.\n",
      "\n",
      "attributeA value associated with an object which is referenced by name using\n",
      "dotted expressions.  For example, if an object o has an attribute\n",
      "a it would be referenced as o.a.\n",
      "\n",
      "awaitableAn object that can be used in an await expression.  Can be\n",
      "a coroutine or an object with an __await__() method.\n",
      "See also PEP 492.\n",
      "\n",
      "BDFLBenevolent Dictator For Life, a.k.a. Guido van Rossum, Python’s creator.\n",
      "\n",
      "binary fileA file object able to read and write\n",
      "bytes-like objects.\n",
      "Examples of binary files are files opened in binary mode ('rb',\n",
      "'wb' or 'rb+'), sys.stdin.buffer,\n",
      "sys.stdout.buffer, and instances of io.BytesIO and\n",
      "gzip.GzipFile.\n",
      "See also text file for a file object able to read and write\n",
      "str objects.\n",
      "\n",
      "bytes-like objectAn object that supports the Buffer Protocol and can\n",
      "export a C-contiguous buffer. This includes all bytes,\n",
      "bytearray, and array.array objects, as well as many\n",
      "common memoryview objects.  Bytes-like objects can\n",
      "be used for various operations that work with binary data; these include\n",
      "compression, saving to a binary file, and sending over a socket.\n",
      "Some operations need the binary data to be mutable.  The documentation\n",
      "often refers to these as “read-write bytes-like objects”.  Example\n",
      "mutable buffer objects include bytearray and a\n",
      "memoryview of a bytearray.\n",
      "Other operations require the binary data to be stored in\n",
      "immutable objects (“read-only bytes-like objects”); examples\n",
      "of these include bytes and a memoryview\n",
      "of a bytes object.\n",
      "\n",
      "bytecodePython source code is compiled into bytecode, the internal representation\n",
      "of a Python program in the CPython interpreter.  The bytecode is also\n",
      "cached in .pyc files so that executing the same file is\n",
      "faster the second time (recompilation from source to bytecode can be\n",
      "avoided).  This “intermediate language” is said to run on a\n",
      "virtual machine that executes the machine code corresponding to\n",
      "each bytecode. Do note that bytecodes are not expected to work between\n",
      "different Python virtual machines, nor to be stable between Python\n",
      "releases.\n",
      "A list of bytecode instructions can be found in the documentation for\n",
      "the dis module.\n",
      "\n",
      "callbackA subroutine function which is passed as an argument to be executed at\n",
      "some point in the future.\n",
      "\n",
      "classA template for creating user-defined objects. Class definitions\n",
      "normally contain method definitions which operate on instances of the\n",
      "class.\n",
      "\n",
      "class variableA variable defined in a class and intended to be modified only at\n",
      "class level (i.e., not in an instance of the class).\n",
      "\n",
      "coercionThe implicit conversion of an instance of one type to another during an\n",
      "operation which involves two arguments of the same type.  For example,\n",
      "int(3.15) converts the floating point number to the integer 3, but\n",
      "in 3+4.5, each argument is of a different type (one int, one float),\n",
      "and both must be converted to the same type before they can be added or it\n",
      "will raise a TypeError.  Without coercion, all arguments of even\n",
      "compatible types would have to be normalized to the same value by the\n",
      "programmer, e.g., float(3)+4.5 rather than just 3+4.5.\n",
      "\n",
      "complex numberAn extension of the familiar real number system in which all numbers are\n",
      "expressed as a sum of a real part and an imaginary part.  Imaginary\n",
      "numbers are real multiples of the imaginary unit (the square root of\n",
      "-1), often written i in mathematics or j in\n",
      "engineering.  Python has built-in support for complex numbers, which are\n",
      "written with this latter notation; the imaginary part is written with a\n",
      "j suffix, e.g., 3+1j.  To get access to complex equivalents of the\n",
      "math module, use cmath.  Use of complex numbers is a fairly\n",
      "advanced mathematical feature.  If you’re not aware of a need for them,\n",
      "it’s almost certain you can safely ignore them.\n",
      "\n",
      "context managerAn object which controls the environment seen in a with\n",
      "statement by defining __enter__() and __exit__() methods.\n",
      "See PEP 343.\n",
      "\n",
      "context variableA variable which can have different values depending on its context.\n",
      "This is similar to Thread-Local Storage in which each execution\n",
      "thread may have a different value for a variable. However, with context\n",
      "variables, there may be several contexts in one execution thread and the\n",
      "main usage for context variables is to keep track of variables in\n",
      "concurrent asynchronous tasks.\n",
      "See contextvars.\n",
      "\n",
      "contiguousA buffer is considered contiguous exactly if it is either\n",
      "C-contiguous or Fortran contiguous.  Zero-dimensional buffers are\n",
      "C and Fortran contiguous.  In one-dimensional arrays, the items\n",
      "must be laid out in memory next to each other, in order of\n",
      "increasing indexes starting from zero.  In multidimensional\n",
      "C-contiguous arrays, the last index varies the fastest when\n",
      "visiting items in order of memory address.  However, in\n",
      "Fortran contiguous arrays, the first index varies the fastest.\n",
      "\n",
      "coroutineCoroutines are a more generalized form of subroutines. Subroutines are\n",
      "entered at one point and exited at another point.  Coroutines can be\n",
      "entered, exited, and resumed at many different points.  They can be\n",
      "implemented with the async def statement.  See also\n",
      "PEP 492.\n",
      "\n",
      "coroutine functionA function which returns a coroutine object.  A coroutine\n",
      "function may be defined with the async def statement,\n",
      "and may contain await, async for, and\n",
      "async with keywords.  These were introduced\n",
      "by PEP 492.\n",
      "\n",
      "CPythonThe canonical implementation of the Python programming language, as\n",
      "distributed on python.org.  The term “CPython”\n",
      "is used when necessary to distinguish this implementation from others\n",
      "such as Jython or IronPython.\n",
      "\n",
      "decoratorA function returning another function, usually applied as a function\n",
      "transformation using the @wrapper syntax.  Common examples for\n",
      "decorators are classmethod() and staticmethod().\n",
      "The decorator syntax is merely syntactic sugar, the following two\n",
      "function definitions are semantically equivalent:\n",
      "def f(arg):\n",
      "    ...\n",
      "f = staticmethod(f)\n",
      "\n",
      "@staticmethod\n",
      "def f(arg):\n",
      "    ...\n",
      "\n",
      "The same concept exists for classes, but is less commonly used there.  See\n",
      "the documentation for function definitions and\n",
      "class definitions for more about decorators.\n",
      "\n",
      "descriptorAny object which defines the methods __get__(), __set__(), or\n",
      "__delete__().  When a class attribute is a descriptor, its special\n",
      "binding behavior is triggered upon attribute lookup.  Normally, using\n",
      "a.b to get, set or delete an attribute looks up the object named b in\n",
      "the class dictionary for a, but if b is a descriptor, the respective\n",
      "descriptor method gets called.  Understanding descriptors is a key to a\n",
      "deep understanding of Python because they are the basis for many features\n",
      "including functions, methods, properties, class methods, static methods,\n",
      "and reference to super classes.\n",
      "For more information about descriptors’ methods, see Implementing Descriptors\n",
      "or the Descriptor How To Guide.\n",
      "\n",
      "dictionaryAn associative array, where arbitrary keys are mapped to values.  The\n",
      "keys can be any object with __hash__() and __eq__() methods.\n",
      "Called a hash in Perl.\n",
      "\n",
      "dictionary comprehensionA compact way to process all or part of the elements in an iterable and\n",
      "return a dictionary with the results. results = {n: n ** 2 for n in\n",
      "range(10)} generates a dictionary containing key n mapped to\n",
      "value n ** 2. See Displays for lists, sets and dictionaries.\n",
      "\n",
      "dictionary viewThe objects returned from dict.keys(), dict.values(), and\n",
      "dict.items() are called dictionary views. They provide a dynamic\n",
      "view on the dictionary’s entries, which means that when the dictionary\n",
      "changes, the view reflects these changes. To force the\n",
      "dictionary view to become a full list use list(dictview).  See\n",
      "Dictionary view objects.\n",
      "\n",
      "docstringA string literal which appears as the first expression in a class,\n",
      "function or module.  While ignored when the suite is executed, it is\n",
      "recognized by the compiler and put into the __doc__ attribute\n",
      "of the enclosing class, function or module.  Since it is available via\n",
      "introspection, it is the canonical place for documentation of the\n",
      "object.\n",
      "\n",
      "duck-typingA programming style which does not look at an object’s type to determine\n",
      "if it has the right interface; instead, the method or attribute is simply\n",
      "called or used (“If it looks like a duck and quacks like a duck, it\n",
      "must be a duck.”)  By emphasizing interfaces rather than specific types,\n",
      "well-designed code improves its flexibility by allowing polymorphic\n",
      "substitution.  Duck-typing avoids tests using type() or\n",
      "isinstance().  (Note, however, that duck-typing can be complemented\n",
      "with abstract base classes.)  Instead, it\n",
      "typically employs hasattr() tests or EAFP programming.\n",
      "\n",
      "EAFPEasier to ask for forgiveness than permission.  This common Python coding\n",
      "style assumes the existence of valid keys or attributes and catches\n",
      "exceptions if the assumption proves false.  This clean and fast style is\n",
      "characterized by the presence of many try and except\n",
      "statements.  The technique contrasts with the LBYL style\n",
      "common to many other languages such as C.\n",
      "\n",
      "expressionA piece of syntax which can be evaluated to some value.  In other words,\n",
      "an expression is an accumulation of expression elements like literals,\n",
      "names, attribute access, operators or function calls which all return a\n",
      "value.  In contrast to many other languages, not all language constructs\n",
      "are expressions.  There are also statements which cannot be used\n",
      "as expressions, such as while.  Assignments are also statements,\n",
      "not expressions.\n",
      "\n",
      "extension moduleA module written in C or C++, using Python’s C API to interact with the\n",
      "core and with user code.\n",
      "\n",
      "f-stringString literals prefixed with 'f' or 'F' are commonly called\n",
      "“f-strings” which is short for\n",
      "formatted string literals.  See also PEP 498.\n",
      "\n",
      "file objectAn object exposing a file-oriented API (with methods such as\n",
      "read() or write()) to an underlying resource.  Depending\n",
      "on the way it was created, a file object can mediate access to a real\n",
      "on-disk file or to another type of storage or communication device\n",
      "(for example standard input/output, in-memory buffers, sockets, pipes,\n",
      "etc.).  File objects are also called file-like objects or\n",
      "streams.\n",
      "There are actually three categories of file objects: raw\n",
      "binary files, buffered\n",
      "binary files and text files.\n",
      "Their interfaces are defined in the io module.  The canonical\n",
      "way to create a file object is by using the open() function.\n",
      "\n",
      "file-like objectA synonym for file object.\n",
      "\n",
      "finderAn object that tries to find the loader for a module that is\n",
      "being imported.\n",
      "Since Python 3.3, there are two types of finder: meta path finders for use with sys.meta_path, and path\n",
      "entry finders for use with sys.path_hooks.\n",
      "See PEP 302, PEP 420 and PEP 451 for much more detail.\n",
      "\n",
      "floor divisionMathematical division that rounds down to nearest integer.  The floor\n",
      "division operator is //.  For example, the expression 11 // 4\n",
      "evaluates to 2 in contrast to the 2.75 returned by float true\n",
      "division.  Note that (-11) // 4 is -3 because that is -2.75\n",
      "rounded downward. See PEP 238.\n",
      "\n",
      "functionA series of statements which returns some value to a caller. It can also\n",
      "be passed zero or more arguments which may be used in\n",
      "the execution of the body. See also parameter, method,\n",
      "and the Function definitions section.\n",
      "\n",
      "function annotationAn annotation of a function parameter or return value.\n",
      "Function annotations are usually used for\n",
      "type hints: for example, this function is expected to take two\n",
      "int arguments and is also expected to have an int\n",
      "return value:\n",
      "def sum_two_numbers(a: int, b: int) -> int:\n",
      "   return a + b\n",
      "\n",
      "Function annotation syntax is explained in section Function definitions.\n",
      "See variable annotation and PEP 484,\n",
      "which describe this functionality.\n",
      "\n",
      "__future__A future statement, from __future__ import <feature>,\n",
      "directs the compiler to compile the current module using syntax or\n",
      "semantics that will become standard in a future release of Python.\n",
      "The __future__ module documents the possible values of\n",
      "feature.  By importing this module and evaluating its variables,\n",
      "you can see when a new feature was first added to the language and\n",
      "when it will (or did) become the default:\n",
      ">>> import __future__\n",
      ">>> __future__.division\n",
      "_Feature((2, 2, 0, 'alpha', 2), (3, 0, 0, 'alpha', 0), 8192)\n",
      "\n",
      "garbage collectionThe process of freeing memory when it is not used anymore.  Python\n",
      "performs garbage collection via reference counting and a cyclic garbage\n",
      "collector that is able to detect and break reference cycles.  The\n",
      "garbage collector can be controlled using the gc module.\n",
      "\n",
      "generatorA function which returns a generator iterator.  It looks like a\n",
      "normal function except that it contains yield expressions\n",
      "for producing a series of values usable in a for-loop or that can be\n",
      "retrieved one at a time with the next() function.\n",
      "Usually refers to a generator function, but may refer to a\n",
      "generator iterator in some contexts.  In cases where the intended\n",
      "meaning isn’t clear, using the full terms avoids ambiguity.\n",
      "\n",
      "generator iteratorAn object created by a generator function.\n",
      "Each yield temporarily suspends processing, remembering the\n",
      "location execution state (including local variables and pending\n",
      "try-statements).  When the generator iterator resumes, it picks up where\n",
      "it left off (in contrast to functions which start fresh on every\n",
      "invocation).\n",
      "\n",
      "generator expressionAn expression that returns an iterator.  It looks like a normal expression\n",
      "followed by a for clause defining a loop variable, range,\n",
      "and an optional if clause.  The combined expression\n",
      "generates values for an enclosing function:\n",
      ">>> sum(i*i for i in range(10))         # sum of squares 0, 1, 4, ... 81\n",
      "285\n",
      "\n",
      "generic functionA function composed of multiple functions implementing the same operation\n",
      "for different types. Which implementation should be used during a call is\n",
      "determined by the dispatch algorithm.\n",
      "See also the single dispatch glossary entry, the\n",
      "functools.singledispatch() decorator, and PEP 443.\n",
      "\n",
      "generic typeA type that can be parameterized; typically a\n",
      "container class such as list or\n",
      "dict. Used for type hints and\n",
      "annotations.\n",
      "For more details, see generic alias types,\n",
      "PEP 483, PEP 484, PEP 585, and the typing module.\n",
      "\n",
      "GILSee global interpreter lock.\n",
      "\n",
      "global interpreter lockThe mechanism used by the CPython interpreter to assure that\n",
      "only one thread executes Python bytecode at a time.\n",
      "This simplifies the CPython implementation by making the object model\n",
      "(including critical built-in types such as dict) implicitly\n",
      "safe against concurrent access.  Locking the entire interpreter\n",
      "makes it easier for the interpreter to be multi-threaded, at the\n",
      "expense of much of the parallelism afforded by multi-processor\n",
      "machines.\n",
      "However, some extension modules, either standard or third-party,\n",
      "are designed so as to release the GIL when doing computationally-intensive\n",
      "tasks such as compression or hashing.  Also, the GIL is always released\n",
      "when doing I/O.\n",
      "Past efforts to create a “free-threaded” interpreter (one which locks\n",
      "shared data at a much finer granularity) have not been successful\n",
      "because performance suffered in the common single-processor case. It\n",
      "is believed that overcoming this performance issue would make the\n",
      "implementation much more complicated and therefore costlier to maintain.\n",
      "\n",
      "hash-based pycA bytecode cache file that uses the hash rather than the last-modified\n",
      "time of the corresponding source file to determine its validity. See\n",
      "Cached bytecode invalidation.\n",
      "\n",
      "hashableAn object is hashable if it has a hash value which never changes during\n",
      "its lifetime (it needs a __hash__() method), and can be compared to\n",
      "other objects (it needs an __eq__() method).  Hashable objects which\n",
      "compare equal must have the same hash value.\n",
      "Hashability makes an object usable as a dictionary key and a set member,\n",
      "because these data structures use the hash value internally.\n",
      "Most of Python’s immutable built-in objects are hashable; mutable\n",
      "containers (such as lists or dictionaries) are not; immutable\n",
      "containers (such as tuples and frozensets) are only hashable if\n",
      "their elements are hashable.  Objects which are\n",
      "instances of user-defined classes are hashable by default.  They all\n",
      "compare unequal (except with themselves), and their hash value is derived\n",
      "from their id().\n",
      "\n",
      "IDLEAn Integrated Development Environment for Python.  IDLE is a basic editor\n",
      "and interpreter environment which ships with the standard distribution of\n",
      "Python.\n",
      "\n",
      "immutableAn object with a fixed value.  Immutable objects include numbers, strings and\n",
      "tuples.  Such an object cannot be altered.  A new object has to\n",
      "be created if a different value has to be stored.  They play an important\n",
      "role in places where a constant hash value is needed, for example as a key\n",
      "in a dictionary.\n",
      "\n",
      "import pathA list of locations (or path entries) that are\n",
      "searched by the path based finder for modules to import. During\n",
      "import, this list of locations usually comes from sys.path, but\n",
      "for subpackages it may also come from the parent package’s __path__\n",
      "attribute.\n",
      "\n",
      "importingThe process by which Python code in one module is made available to\n",
      "Python code in another module.\n",
      "\n",
      "importerAn object that both finds and loads a module; both a\n",
      "finder and loader object.\n",
      "\n",
      "interactivePython has an interactive interpreter which means you can enter\n",
      "statements and expressions at the interpreter prompt, immediately\n",
      "execute them and see their results.  Just launch python with no\n",
      "arguments (possibly by selecting it from your computer’s main\n",
      "menu). It is a very powerful way to test out new ideas or inspect\n",
      "modules and packages (remember help(x)).\n",
      "\n",
      "interpretedPython is an interpreted language, as opposed to a compiled one,\n",
      "though the distinction can be blurry because of the presence of the\n",
      "bytecode compiler.  This means that source files can be run directly\n",
      "without explicitly creating an executable which is then run.\n",
      "Interpreted languages typically have a shorter development/debug cycle\n",
      "than compiled ones, though their programs generally also run more\n",
      "slowly.  See also interactive.\n",
      "\n",
      "interpreter shutdownWhen asked to shut down, the Python interpreter enters a special phase\n",
      "where it gradually releases all allocated resources, such as modules\n",
      "and various critical internal structures.  It also makes several calls\n",
      "to the garbage collector. This can trigger\n",
      "the execution of code in user-defined destructors or weakref callbacks.\n",
      "Code executed during the shutdown phase can encounter various\n",
      "exceptions as the resources it relies on may not function anymore\n",
      "(common examples are library modules or the warnings machinery).\n",
      "The main reason for interpreter shutdown is that the __main__ module\n",
      "or the script being run has finished executing.\n",
      "\n",
      "iterableAn object capable of returning its members one at a time. Examples of\n",
      "iterables include all sequence types (such as list, str,\n",
      "and tuple) and some non-sequence types like dict,\n",
      "file objects, and objects of any classes you define\n",
      "with an __iter__() method or with a __getitem__() method\n",
      "that implements Sequence semantics.\n",
      "Iterables can be\n",
      "used in a for loop and in many other places where a sequence is\n",
      "needed (zip(), map(), …).  When an iterable object is passed\n",
      "as an argument to the built-in function iter(), it returns an\n",
      "iterator for the object.  This iterator is good for one pass over the set\n",
      "of values.  When using iterables, it is usually not necessary to call\n",
      "iter() or deal with iterator objects yourself.  The for\n",
      "statement does that automatically for you, creating a temporary unnamed\n",
      "variable to hold the iterator for the duration of the loop.  See also\n",
      "iterator, sequence, and generator.\n",
      "\n",
      "iteratorAn object representing a stream of data.  Repeated calls to the iterator’s\n",
      "__next__() method (or passing it to the built-in function\n",
      "next()) return successive items in the stream.  When no more data\n",
      "are available a StopIteration exception is raised instead.  At this\n",
      "point, the iterator object is exhausted and any further calls to its\n",
      "__next__() method just raise StopIteration again.  Iterators\n",
      "are required to have an __iter__() method that returns the iterator\n",
      "object itself so every iterator is also iterable and may be used in most\n",
      "places where other iterables are accepted.  One notable exception is code\n",
      "which attempts multiple iteration passes.  A container object (such as a\n",
      "list) produces a fresh new iterator each time you pass it to the\n",
      "iter() function or use it in a for loop.  Attempting this\n",
      "with an iterator will just return the same exhausted iterator object used\n",
      "in the previous iteration pass, making it appear like an empty container.\n",
      "More information can be found in Iterator Types.\n",
      "\n",
      "key functionA key function or collation function is a callable that returns a value\n",
      "used for sorting or ordering.  For example, locale.strxfrm() is\n",
      "used to produce a sort key that is aware of locale specific sort\n",
      "conventions.\n",
      "A number of tools in Python accept key functions to control how elements\n",
      "are ordered or grouped.  They include min(), max(),\n",
      "sorted(), list.sort(), heapq.merge(),\n",
      "heapq.nsmallest(), heapq.nlargest(), and\n",
      "itertools.groupby().\n",
      "There are several ways to create a key function.  For example. the\n",
      "str.lower() method can serve as a key function for case insensitive\n",
      "sorts.  Alternatively, a key function can be built from a\n",
      "lambda expression such as lambda r: (r[0], r[2]).  Also,\n",
      "the operator module provides three key function constructors:\n",
      "attrgetter(), itemgetter(), and\n",
      "methodcaller().  See the Sorting HOW TO for examples of how to create and use key functions.\n",
      "\n",
      "keyword argumentSee argument.\n",
      "\n",
      "lambdaAn anonymous inline function consisting of a single expression\n",
      "which is evaluated when the function is called.  The syntax to create\n",
      "a lambda function is lambda [parameters]: expression\n",
      "\n",
      "LBYLLook before you leap.  This coding style explicitly tests for\n",
      "pre-conditions before making calls or lookups.  This style contrasts with\n",
      "the EAFP approach and is characterized by the presence of many\n",
      "if statements.\n",
      "In a multi-threaded environment, the LBYL approach can risk introducing a\n",
      "race condition between “the looking” and “the leaping”.  For example, the\n",
      "code, if key in mapping: return mapping[key] can fail if another\n",
      "thread removes key from mapping after the test, but before the lookup.\n",
      "This issue can be solved with locks or by using the EAFP approach.\n",
      "\n",
      "listA built-in Python sequence.  Despite its name it is more akin\n",
      "to an array in other languages than to a linked list since access to\n",
      "elements is O(1).\n",
      "\n",
      "list comprehensionA compact way to process all or part of the elements in a sequence and\n",
      "return a list with the results.  result = ['{:#04x}'.format(x) for x in\n",
      "range(256) if x % 2 == 0] generates a list of strings containing\n",
      "even hex numbers (0x..) in the range from 0 to 255. The if\n",
      "clause is optional.  If omitted, all elements in range(256) are\n",
      "processed.\n",
      "\n",
      "loaderAn object that loads a module. It must define a method named\n",
      "load_module(). A loader is typically returned by a\n",
      "finder. See PEP 302 for details and\n",
      "importlib.abc.Loader for an abstract base class.\n",
      "\n",
      "magic methodAn informal synonym for special method.\n",
      "\n",
      "mappingA container object that supports arbitrary key lookups and implements the\n",
      "methods specified in the Mapping or\n",
      "MutableMapping\n",
      "abstract base classes.  Examples\n",
      "include dict, collections.defaultdict,\n",
      "collections.OrderedDict and collections.Counter.\n",
      "\n",
      "meta path finderA finder returned by a search of sys.meta_path.  Meta path\n",
      "finders are related to, but different from path entry finders.\n",
      "See importlib.abc.MetaPathFinder for the methods that meta path\n",
      "finders implement.\n",
      "\n",
      "metaclassThe class of a class.  Class definitions create a class name, a class\n",
      "dictionary, and a list of base classes.  The metaclass is responsible for\n",
      "taking those three arguments and creating the class.  Most object oriented\n",
      "programming languages provide a default implementation.  What makes Python\n",
      "special is that it is possible to create custom metaclasses.  Most users\n",
      "never need this tool, but when the need arises, metaclasses can provide\n",
      "powerful, elegant solutions.  They have been used for logging attribute\n",
      "access, adding thread-safety, tracking object creation, implementing\n",
      "singletons, and many other tasks.\n",
      "More information can be found in Metaclasses.\n",
      "\n",
      "methodA function which is defined inside a class body.  If called as an attribute\n",
      "of an instance of that class, the method will get the instance object as\n",
      "its first argument (which is usually called self).\n",
      "See function and nested scope.\n",
      "\n",
      "method resolution orderMethod Resolution Order is the order in which base classes are searched\n",
      "for a member during lookup. See The Python 2.3 Method Resolution Order for details of the\n",
      "algorithm used by the Python interpreter since the 2.3 release.\n",
      "\n",
      "moduleAn object that serves as an organizational unit of Python code.  Modules\n",
      "have a namespace containing arbitrary Python objects.  Modules are loaded\n",
      "into Python by the process of importing.\n",
      "See also package.\n",
      "\n",
      "module specA namespace containing the import-related information used to load a\n",
      "module. An instance of importlib.machinery.ModuleSpec.\n",
      "\n",
      "MROSee method resolution order.\n",
      "\n",
      "mutableMutable objects can change their value but keep their id().  See\n",
      "also immutable.\n",
      "\n",
      "named tupleThe term “named tuple” applies to any type or class that inherits from\n",
      "tuple and whose indexable elements are also accessible using named\n",
      "attributes.  The type or class may have other features as well.\n",
      "Several built-in types are named tuples, including the values returned\n",
      "by time.localtime() and os.stat().  Another example is\n",
      "sys.float_info:\n",
      ">>> sys.float_info[1]                   # indexed access\n",
      "1024\n",
      ">>> sys.float_info.max_exp              # named field access\n",
      "1024\n",
      ">>> isinstance(sys.float_info, tuple)   # kind of tuple\n",
      "True\n",
      "\n",
      "Some named tuples are built-in types (such as the above examples).\n",
      "Alternatively, a named tuple can be created from a regular class\n",
      "definition that inherits from tuple and that defines named\n",
      "fields.  Such a class can be written by hand or it can be created with\n",
      "the factory function collections.namedtuple().  The latter\n",
      "technique also adds some extra methods that may not be found in\n",
      "hand-written or built-in named tuples.\n",
      "\n",
      "namespaceThe place where a variable is stored.  Namespaces are implemented as\n",
      "dictionaries.  There are the local, global and built-in namespaces as well\n",
      "as nested namespaces in objects (in methods).  Namespaces support\n",
      "modularity by preventing naming conflicts.  For instance, the functions\n",
      "builtins.open and os.open() are distinguished by\n",
      "their namespaces.  Namespaces also aid readability and maintainability by\n",
      "making it clear which module implements a function.  For instance, writing\n",
      "random.seed() or itertools.islice() makes it clear that those\n",
      "functions are implemented by the random and itertools\n",
      "modules, respectively.\n",
      "\n",
      "namespace packageA PEP 420 package which serves only as a container for\n",
      "subpackages.  Namespace packages may have no physical representation,\n",
      "and specifically are not like a regular package because they\n",
      "have no __init__.py file.\n",
      "See also module.\n",
      "\n",
      "nested scopeThe ability to refer to a variable in an enclosing definition.  For\n",
      "instance, a function defined inside another function can refer to\n",
      "variables in the outer function.  Note that nested scopes by default work\n",
      "only for reference and not for assignment.  Local variables both read and\n",
      "write in the innermost scope.  Likewise, global variables read and write\n",
      "to the global namespace.  The nonlocal allows writing to outer\n",
      "scopes.\n",
      "\n",
      "new-style classOld name for the flavor of classes now used for all class objects.  In\n",
      "earlier Python versions, only new-style classes could use Python’s newer,\n",
      "versatile features like __slots__, descriptors,\n",
      "properties, __getattribute__(), class methods, and static methods.\n",
      "\n",
      "objectAny data with state (attributes or value) and defined behavior\n",
      "(methods).  Also the ultimate base class of any new-style\n",
      "class.\n",
      "\n",
      "packageA Python module which can contain submodules or recursively,\n",
      "subpackages.  Technically, a package is a Python module with an\n",
      "__path__ attribute.\n",
      "See also regular package and namespace package.\n",
      "\n",
      "parameterA named entity in a function (or method) definition that\n",
      "specifies an argument (or in some cases, arguments) that the\n",
      "function can accept.  There are five kinds of parameter:\n",
      "\n",
      "positional-or-keyword: specifies an argument that can be passed\n",
      "either positionally or as a keyword argument.  This is the default kind of parameter, for example foo\n",
      "and bar in the following:\n",
      "def func(foo, bar=None): ...\n",
      "\n",
      "positional-only: specifies an argument that can be supplied only\n",
      "by position. Positional-only parameters can be defined by including a\n",
      "/ character in the parameter list of the function definition after\n",
      "them, for example posonly1 and posonly2 in the following:\n",
      "def func(posonly1, posonly2, /, positional_or_keyword): ...\n",
      "\n",
      "keyword-only: specifies an argument that can be supplied only\n",
      "by keyword.  Keyword-only parameters can be defined by including a\n",
      "single var-positional parameter or bare * in the parameter list\n",
      "of the function definition before them, for example kw_only1 and\n",
      "kw_only2 in the following:\n",
      "def func(arg, *, kw_only1, kw_only2): ...\n",
      "\n",
      "var-positional: specifies that an arbitrary sequence of\n",
      "positional arguments can be provided (in addition to any positional\n",
      "arguments already accepted by other parameters).  Such a parameter can\n",
      "be defined by prepending the parameter name with *, for example\n",
      "args in the following:\n",
      "def func(*args, **kwargs): ...\n",
      "\n",
      "var-keyword: specifies that arbitrarily many keyword arguments\n",
      "can be provided (in addition to any keyword arguments already accepted\n",
      "by other parameters).  Such a parameter can be defined by prepending\n",
      "the parameter name with **, for example kwargs in the example\n",
      "above.\n",
      "\n",
      "Parameters can specify both optional and required arguments, as well as\n",
      "default values for some optional arguments.\n",
      "See also the argument glossary entry, the FAQ question on\n",
      "the difference between arguments and parameters, the inspect.Parameter class, the\n",
      "Function definitions section, and PEP 362.\n",
      "\n",
      "path entryA single location on the import path which the path\n",
      "based finder consults to find modules for importing.\n",
      "\n",
      "path entry finderA finder returned by a callable on sys.path_hooks\n",
      "(i.e. a path entry hook) which knows how to locate modules given\n",
      "a path entry.\n",
      "See importlib.abc.PathEntryFinder for the methods that path entry\n",
      "finders implement.\n",
      "\n",
      "path entry hookA callable on the sys.path_hook list which returns a path\n",
      "entry finder if it knows how to find modules on a specific path\n",
      "entry.\n",
      "\n",
      "path based finderOne of the default meta path finders which\n",
      "searches an import path for modules.\n",
      "\n",
      "path-like objectAn object representing a file system path. A path-like object is either\n",
      "a str or bytes object representing a path, or an object\n",
      "implementing the os.PathLike protocol. An object that supports\n",
      "the os.PathLike protocol can be converted to a str or\n",
      "bytes file system path by calling the os.fspath() function;\n",
      "os.fsdecode() and os.fsencode() can be used to guarantee a\n",
      "str or bytes result instead, respectively. Introduced\n",
      "by PEP 519.\n",
      "\n",
      "PEPPython Enhancement Proposal. A PEP is a design document\n",
      "providing information to the Python community, or describing a new\n",
      "feature for Python or its processes or environment. PEPs should\n",
      "provide a concise technical specification and a rationale for proposed\n",
      "features.\n",
      "PEPs are intended to be the primary mechanisms for proposing major new\n",
      "features, for collecting community input on an issue, and for documenting\n",
      "the design decisions that have gone into Python. The PEP author is\n",
      "responsible for building consensus within the community and documenting\n",
      "dissenting opinions.\n",
      "See PEP 1.\n",
      "\n",
      "portionA set of files in a single directory (possibly stored in a zip file)\n",
      "that contribute to a namespace package, as defined in PEP 420.\n",
      "\n",
      "positional argumentSee argument.\n",
      "\n",
      "provisional APIA provisional API is one which has been deliberately excluded from\n",
      "the standard library’s backwards compatibility guarantees.  While major\n",
      "changes to such interfaces are not expected, as long as they are marked\n",
      "provisional, backwards incompatible changes (up to and including removal\n",
      "of the interface) may occur if deemed necessary by core developers.  Such\n",
      "changes will not be made gratuitously – they will occur only if serious\n",
      "fundamental flaws are uncovered that were missed prior to the inclusion\n",
      "of the API.\n",
      "Even for provisional APIs, backwards incompatible changes are seen as\n",
      "a “solution of last resort” - every attempt will still be made to find\n",
      "a backwards compatible resolution to any identified problems.\n",
      "This process allows the standard library to continue to evolve over\n",
      "time, without locking in problematic design errors for extended periods\n",
      "of time.  See PEP 411 for more details.\n",
      "\n",
      "provisional packageSee provisional API.\n",
      "\n",
      "Python 3000Nickname for the Python 3.x release line (coined long ago when the\n",
      "release of version 3 was something in the distant future.)  This is also\n",
      "abbreviated “Py3k”.\n",
      "\n",
      "PythonicAn idea or piece of code which closely follows the most common idioms\n",
      "of the Python language, rather than implementing code using concepts\n",
      "common to other languages.  For example, a common idiom in Python is\n",
      "to loop over all elements of an iterable using a for\n",
      "statement.  Many other languages don’t have this type of construct, so\n",
      "people unfamiliar with Python sometimes use a numerical counter instead:\n",
      "for i in range(len(food)):\n",
      "    print(food[i])\n",
      "\n",
      "As opposed to the cleaner, Pythonic method:\n",
      "for piece in food:\n",
      "    print(piece)\n",
      "\n",
      "qualified nameA dotted name showing the “path” from a module’s global scope to a\n",
      "class, function or method defined in that module, as defined in\n",
      "PEP 3155.  For top-level functions and classes, the qualified name\n",
      "is the same as the object’s name:\n",
      ">>> class C:\n",
      "...     class D:\n",
      "...         def meth(self):\n",
      "...             pass\n",
      "...\n",
      ">>> C.__qualname__\n",
      "'C'\n",
      ">>> C.D.__qualname__\n",
      "'C.D'\n",
      ">>> C.D.meth.__qualname__\n",
      "'C.D.meth'\n",
      "\n",
      "When used to refer to modules, the fully qualified name means the\n",
      "entire dotted path to the module, including any parent packages,\n",
      "e.g. email.mime.text:\n",
      ">>> import email.mime.text\n",
      ">>> email.mime.text.__name__\n",
      "'email.mime.text'\n",
      "\n",
      "reference countThe number of references to an object.  When the reference count of an\n",
      "object drops to zero, it is deallocated.  Reference counting is\n",
      "generally not visible to Python code, but it is a key element of the\n",
      "CPython implementation.  The sys module defines a\n",
      "getrefcount() function that programmers can call to return the\n",
      "reference count for a particular object.\n",
      "\n",
      "regular packageA traditional package, such as a directory containing an\n",
      "__init__.py file.\n",
      "See also namespace package.\n",
      "\n",
      "__slots__A declaration inside a class that saves memory by pre-declaring space for\n",
      "instance attributes and eliminating instance dictionaries.  Though\n",
      "popular, the technique is somewhat tricky to get right and is best\n",
      "reserved for rare cases where there are large numbers of instances in a\n",
      "memory-critical application.\n",
      "\n",
      "sequenceAn iterable which supports efficient element access using integer\n",
      "indices via the __getitem__() special method and defines a\n",
      "__len__() method that returns the length of the sequence.\n",
      "Some built-in sequence types are list, str,\n",
      "tuple, and bytes. Note that dict also\n",
      "supports __getitem__() and __len__(), but is considered a\n",
      "mapping rather than a sequence because the lookups use arbitrary\n",
      "immutable keys rather than integers.\n",
      "The collections.abc.Sequence abstract base class\n",
      "defines a much richer interface that goes beyond just\n",
      "__getitem__() and __len__(), adding count(),\n",
      "index(), __contains__(), and\n",
      "__reversed__(). Types that implement this expanded\n",
      "interface can be registered explicitly using\n",
      "register().\n",
      "\n",
      "set comprehensionA compact way to process all or part of the elements in an iterable and\n",
      "return a set with the results. results = {c for c in 'abracadabra' if\n",
      "c not in 'abc'} generates the set of strings {'r', 'd'}.  See\n",
      "Displays for lists, sets and dictionaries.\n",
      "\n",
      "single dispatchA form of generic function dispatch where the implementation is\n",
      "chosen based on the type of a single argument.\n",
      "\n",
      "sliceAn object usually containing a portion of a sequence.  A slice is\n",
      "created using the subscript notation, [] with colons between numbers\n",
      "when several are given, such as in variable_name[1:3:5].  The bracket\n",
      "(subscript) notation uses slice objects internally.\n",
      "\n",
      "special methodA method that is called implicitly by Python to execute a certain\n",
      "operation on a type, such as addition.  Such methods have names starting\n",
      "and ending with double underscores.  Special methods are documented in\n",
      "Special method names.\n",
      "\n",
      "statementA statement is part of a suite (a “block” of code).  A statement is either\n",
      "an expression or one of several constructs with a keyword, such\n",
      "as if, while or for.\n",
      "\n",
      "text encodingA string in Python is a sequence of Unicode code points (in range\n",
      "U+0000–U+10FFFF). To store or transfer a string, it needs to be\n",
      "serialized as a sequence of bytes.\n",
      "Serializing a string into a sequence of bytes is known as “encoding”, and\n",
      "recreating the string from the sequence of bytes is known as “decoding”.\n",
      "There are a variety of different text serialization\n",
      "codecs, which are collectively referred to as\n",
      "“text encodings”.\n",
      "\n",
      "text fileA file object able to read and write str objects.\n",
      "Often, a text file actually accesses a byte-oriented datastream\n",
      "and handles the text encoding automatically.\n",
      "Examples of text files are files opened in text mode ('r' or 'w'),\n",
      "sys.stdin, sys.stdout, and instances of\n",
      "io.StringIO.\n",
      "See also binary file for a file object able to read and write\n",
      "bytes-like objects.\n",
      "\n",
      "triple-quoted stringA string which is bound by three instances of either a quotation mark\n",
      "(”) or an apostrophe (‘).  While they don’t provide any functionality\n",
      "not available with single-quoted strings, they are useful for a number\n",
      "of reasons.  They allow you to include unescaped single and double\n",
      "quotes within a string and they can span multiple lines without the\n",
      "use of the continuation character, making them especially useful when\n",
      "writing docstrings.\n",
      "\n",
      "typeThe type of a Python object determines what kind of object it is; every\n",
      "object has a type.  An object’s type is accessible as its\n",
      "__class__ attribute or can be retrieved with\n",
      "type(obj).\n",
      "\n",
      "type aliasA synonym for a type, created by assigning the type to an identifier.\n",
      "Type aliases are useful for simplifying type hints.\n",
      "For example:\n",
      "def remove_gray_shades(\n",
      "        colors: list[tuple[int, int, int]]) -> list[tuple[int, int, int]]:\n",
      "    pass\n",
      "\n",
      "could be made more readable like this:\n",
      "Color = tuple[int, int, int]\n",
      "\n",
      "def remove_gray_shades(colors: list[Color]) -> list[Color]:\n",
      "    pass\n",
      "\n",
      "See typing and PEP 484, which describe this functionality.\n",
      "\n",
      "type hintAn annotation that specifies the expected type for a variable, a class\n",
      "attribute, or a function parameter or return value.\n",
      "Type hints are optional and are not enforced by Python but\n",
      "they are useful to static type analysis tools, and aid IDEs with code\n",
      "completion and refactoring.\n",
      "Type hints of global variables, class attributes, and functions,\n",
      "but not local variables, can be accessed using\n",
      "typing.get_type_hints().\n",
      "See typing and PEP 484, which describe this functionality.\n",
      "\n",
      "universal newlinesA manner of interpreting text streams in which all of the following are\n",
      "recognized as ending a line: the Unix end-of-line convention '\\n',\n",
      "the Windows convention '\\r\\n', and the old Macintosh convention\n",
      "'\\r'.  See PEP 278 and PEP 3116, as well as\n",
      "bytes.splitlines() for an additional use.\n",
      "\n",
      "variable annotationAn annotation of a variable or a class attribute.\n",
      "When annotating a variable or a class attribute, assignment is optional:\n",
      "class C:\n",
      "    field: 'annotation'\n",
      "\n",
      "Variable annotations are usually used for\n",
      "type hints: for example this variable is expected to take\n",
      "int values:\n",
      "count: int = 0\n",
      "\n",
      "Variable annotation syntax is explained in section Annotated assignment statements.\n",
      "See function annotation, PEP 484\n",
      "and PEP 526, which describe this functionality.\n",
      "\n",
      "virtual environmentA cooperatively isolated runtime environment that allows Python users\n",
      "and applications to install and upgrade Python distribution packages\n",
      "without interfering with the behaviour of other Python applications\n",
      "running on the same system.\n",
      "See also venv.\n",
      "\n",
      "virtual machineA computer defined entirely in software.  Python’s virtual machine\n",
      "executes the bytecode emitted by the bytecode compiler.\n",
      "\n",
      "Zen of PythonListing of Python design principles and philosophies that are helpful in\n",
      "understanding and using the language.  The listing can be found by typing\n",
      "“import this” at the interactive prompt.\n",
      "\n",
      "          \n",
      "        \n",
      "      \n",
      "      \n",
      "        \n",
      "  Previous topic\n",
      "  “Why is Python Installed on my Computer?” FAQ\n",
      "  Next topic\n",
      "  About these documents\n",
      "  \n",
      "    This Page\n",
      "    \n",
      "      Report a Bug\n",
      "      \n",
      "        Show Source\n",
      "        \n",
      "      \n",
      "    \n",
      "  \n",
      "        \n",
      "      \n",
      "      \n",
      "      \n",
      "    \n",
      "      Navigation\n",
      "      \n",
      "        \n",
      "          index\n",
      "        \n",
      "          modules |\n",
      "        \n",
      "          next |\n",
      "        \n",
      "          previous |\n",
      "\n",
      "          \n",
      "          Python »\n",
      "          \n",
      "            \n",
      "            \n",
      "          \n",
      "          \n",
      "              \n",
      "          \n",
      "    \n",
      "      3.9.20 Documentation »\n",
      "    \n",
      "\n",
      "                \n",
      "                    \n",
      "\n",
      "    \n",
      "        \n",
      "          \n",
      "          \n",
      "          \n",
      "          \n",
      "        \n",
      "    \n",
      "                     |\n",
      "                \n",
      "            \n",
      "      \n",
      "      \n",
      "    \n",
      "    © Copyright 2001-2024, Python Software Foundation.\n",
      "    \n",
      "    This page is licensed under the Python Software Foundation License Version 2.\n",
      "    \n",
      "    Examples, recipes, and other code in the documentation are additionally licensed under the Zero Clause BSD License.\n",
      "    \n",
      "    See History and License for more information.\n",
      "    \n",
      "\n",
      "    The Python Software Foundation is a non-profit corporation.\n",
      "Please donate.\n",
      "\n",
      "    \n",
      "\n",
      "    Last updated on Sep 09, 2024.\n",
      "    Found a bug?\n",
      "    \n",
      "\n",
      "    Created using Sphinx 2.4.4.\n"
     ]
    }
   ],
   "source": [
    "print(docs[1].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FireCrawl\n",
    "\n",
    "FireCrawl crawls and convert any website into LLM-ready data. It crawls all accessible subpages and give you clean markdown and metadata for each. No sitemap required.\n",
    "\n",
    "FireCrawl handles complex tasks such as reverse proxies, caching, rate limits, and content blocked by JavaScript. Built by the mendable.ai team.\n",
    "\n",
    "https://www.firecrawl.dev/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'ogUrl': 'https://python.langchain.com/docs/integrations/document_loaders/firecrawl/', 'title': 'FireCrawl | 🦜️🔗 LangChain', 'ogImage': 'https://python.langchain.com/img/brand/theme-image.png', 'ogTitle': 'FireCrawl | 🦜️🔗 LangChain', 'language': 'en', 'sourceURL': 'https://python.langchain.com/docs/integrations/document_loaders/firecrawl/', 'description': 'FireCrawl crawls and convert any website into LLM-ready data. It crawls all accessible subpages and give you clean markdown and metadata for each. No sitemap required.', 'ogDescription': 'FireCrawl crawls and convert any website into LLM-ready data. It crawls all accessible subpages and give you clean markdown and metadata for each. No sitemap required.', 'pageStatusCode': 200, 'ogLocaleAlternate': []}, page_content='[Skip to main content](#__docusaurus_skipToContent_fallback)\\n\\nShare your thoughts on AI agents. [Take the 3-min survey](https://langchain.typeform.com/state-of-agents).\\n\\n[![🦜️🔗 LangChain](https://python.langchain.com/img/brand/wordmark.png)](/)[Integrations](/docs/integrations/platforms/) [API Reference](https://python.langchain.com/api_reference/)\\n\\nMore\\n\\n- [Contributing](/docs/contributing/)\\n- [People](/docs/people/)\\n- * * *\\n\\n- [LangSmith](https://docs.smith.langchain.com)\\n- [LangGraph](https://langchain-ai.github.io/langgraph/)\\n- [LangChain Hub](https://smith.langchain.com/hub)\\n- [LangChain JS/TS](https://js.langchain.com)\\n\\nv0.3\\n\\n- [v0.3](/docs/introduction/)\\n- [v0.2](https://python.langchain.com/v0.2/docs/introduction)\\n- [v0.1](https://python.langchain.com/v0.1/docs/get_started/introduction)\\n\\n[💬](https://chat.langchain.com)  [GitHub repository](https://github.com/langchain-ai/langchain)\\n\\nSearch`` `K`\\n\\n- [Providers](/docs/integrations/platforms/)\\n\\n  - [Providers](/docs/integrations/platforms/)\\n  - [Anthropic](/docs/integrations/platforms/anthropic/)\\n  - [AWS](/docs/integrations/platforms/aws/)\\n  - [Google](/docs/integrations/platforms/google/)\\n  - [Hugging Face](/docs/integrations/platforms/huggingface/)\\n  - [Microsoft](/docs/integrations/platforms/microsoft/)\\n  - [OpenAI](/docs/integrations/platforms/openai/)\\n  - [More](/docs/integrations/providers/)\\n- [Components](/docs/integrations/components/)\\n\\n  - [Chat models](/docs/integrations/chat/)\\n\\n    - [Chat models](/docs/integrations/chat/)\\n    - [AI21 Labs](/docs/integrations/chat/ai21/)\\n    - [Alibaba Cloud PAI EAS](/docs/integrations/chat/alibaba_cloud_pai_eas/)\\n    - [Anthropic](/docs/integrations/chat/anthropic/)\\n    - [\\\\[Deprecated\\\\] Experimental Anthropic Tools Wrapper](/docs/integrations/chat/anthropic_functions/)\\n    - [Anyscale](/docs/integrations/chat/anyscale/)\\n    - [Azure OpenAI](/docs/integrations/chat/azure_chat_openai/)\\n    - [Azure ML Endpoint](/docs/integrations/chat/azureml_chat_endpoint/)\\n    - [Baichuan Chat](/docs/integrations/chat/baichuan/)\\n    - [Baidu Qianfan](/docs/integrations/chat/baidu_qianfan_endpoint/)\\n    - [AWS Bedrock](/docs/integrations/chat/bedrock/)\\n    - [Cerebras](/docs/integrations/chat/cerebras/)\\n    - [Cohere](/docs/integrations/chat/cohere/)\\n    - [Coze Chat](/docs/integrations/chat/coze/)\\n    - [Dappier AI](/docs/integrations/chat/dappier/)\\n    - [Databricks](/docs/integrations/chat/databricks/)\\n    - [DeepInfra](/docs/integrations/chat/deepinfra/)\\n    - [Eden AI](/docs/integrations/chat/edenai/)\\n    - [Ernie Bot Chat](/docs/integrations/chat/ernie/)\\n    - [EverlyAI](/docs/integrations/chat/everlyai/)\\n    - [Fireworks](/docs/integrations/chat/fireworks/)\\n    - [Friendli](/docs/integrations/chat/friendli/)\\n    - [GigaChat](/docs/integrations/chat/gigachat/)\\n    - [Google AI](/docs/integrations/chat/google_generative_ai/)\\n    - [Google Cloud Vertex AI](/docs/integrations/chat/google_vertex_ai_palm/)\\n    - [GPTRouter](/docs/integrations/chat/gpt_router/)\\n    - [Groq](/docs/integrations/chat/groq/)\\n    - [ChatHuggingFace](/docs/integrations/chat/huggingface/)\\n    - [IBM watsonx.ai](/docs/integrations/chat/ibm_watsonx/)\\n    - [JinaChat](/docs/integrations/chat/jinachat/)\\n    - [Kinetica](/docs/integrations/chat/kinetica/)\\n    - [Konko](/docs/integrations/chat/konko/)\\n    - [LiteLLM](/docs/integrations/chat/litellm/)\\n    - [LiteLLM Router](/docs/integrations/chat/litellm_router/)\\n    - [Llama 2 Chat](/docs/integrations/chat/llama2_chat/)\\n    - [Llama API](/docs/integrations/chat/llama_api/)\\n    - [LlamaEdge](/docs/integrations/chat/llama_edge/)\\n    - [Llama.cpp](/docs/integrations/chat/llamacpp/)\\n    - [maritalk](/docs/integrations/chat/maritalk/)\\n    - [MiniMax](/docs/integrations/chat/minimax/)\\n    - [MistralAI](/docs/integrations/chat/mistralai/)\\n    - [MLX](/docs/integrations/chat/mlx/)\\n    - [Moonshot](/docs/integrations/chat/moonshot/)\\n    - [NVIDIA AI Endpoints](/docs/integrations/chat/nvidia_ai_endpoints/)\\n    - [OCIGenAI](/docs/integrations/chat/oci_generative_ai/)\\n    - [ChatOctoAI](/docs/integrations/chat/octoai/)\\n    - [Ollama](/docs/integrations/chat/ollama/)\\n    - [OpenAI](/docs/integrations/chat/openai/)\\n    - [Perplexity](/docs/integrations/chat/perplexity/)\\n    - [PremAI](/docs/integrations/chat/premai/)\\n    - [PromptLayer ChatOpenAI](/docs/integrations/chat/promptlayer_chatopenai/)\\n    - [Snowflake Cortex](/docs/integrations/chat/snowflake/)\\n    - [solar](/docs/integrations/chat/solar/)\\n    - [SparkLLM Chat](/docs/integrations/chat/sparkllm/)\\n    - [Nebula (Symbl.ai)](/docs/integrations/chat/symblai_nebula/)\\n    - [Tencent Hunyuan](/docs/integrations/chat/tencent_hunyuan/)\\n    - [Together](/docs/integrations/chat/together/)\\n    - [Tongyi Qwen](/docs/integrations/chat/tongyi/)\\n    - [Upstage](/docs/integrations/chat/upstage/)\\n    - [vLLM Chat](/docs/integrations/chat/vllm/)\\n    - [Volc Enging Maas](/docs/integrations/chat/volcengine_maas/)\\n    - [YandexGPT](/docs/integrations/chat/yandex/)\\n    - [ChatYI](/docs/integrations/chat/yi/)\\n    - [Yuan2.0](/docs/integrations/chat/yuan2/)\\n    - [ZHIPU AI](/docs/integrations/chat/zhipuai/)\\n  - [LLMs](/docs/integrations/llms/)\\n\\n    - [LLMs](/docs/integrations/llms/)\\n    - [AI21 Labs](/docs/integrations/llms/ai21/)\\n    - [Aleph Alpha](/docs/integrations/llms/aleph_alpha/)\\n    - [Alibaba Cloud PAI EAS](/docs/integrations/llms/alibabacloud_pai_eas_endpoint/)\\n    - [Amazon API Gateway](/docs/integrations/llms/amazon_api_gateway/)\\n    - [Anthropic](/docs/integrations/llms/anthropic/)\\n    - [Anyscale](/docs/integrations/llms/anyscale/)\\n    - [Aphrodite Engine](/docs/integrations/llms/aphrodite/)\\n    - [Arcee](/docs/integrations/llms/arcee/)\\n    - [Azure ML](/docs/integrations/llms/azure_ml/)\\n    - [Azure OpenAI](/docs/integrations/llms/azure_openai/)\\n    - [Baichuan LLM](/docs/integrations/llms/baichuan/)\\n    - [Baidu Qianfan](/docs/integrations/llms/baidu_qianfan_endpoint/)\\n    - [Banana](/docs/integrations/llms/banana/)\\n    - [Baseten](/docs/integrations/llms/baseten/)\\n    - [Beam](/docs/integrations/llms/beam/)\\n    - [Bedrock](/docs/integrations/llms/bedrock/)\\n    - [Bittensor](/docs/integrations/llms/bittensor/)\\n    - [CerebriumAI](/docs/integrations/llms/cerebriumai/)\\n    - [ChatGLM](/docs/integrations/llms/chatglm/)\\n    - [Clarifai](/docs/integrations/llms/clarifai/)\\n    - [Cloudflare Workers AI](/docs/integrations/llms/cloudflare_workersai/)\\n    - [Cohere](/docs/integrations/llms/cohere/)\\n    - [C Transformers](/docs/integrations/llms/ctransformers/)\\n    - [CTranslate2](/docs/integrations/llms/ctranslate2/)\\n    - [Databricks](/docs/integrations/llms/databricks/)\\n    - [DeepInfra](/docs/integrations/llms/deepinfra/)\\n    - [DeepSparse](/docs/integrations/llms/deepsparse/)\\n    - [Eden AI](/docs/integrations/llms/edenai/)\\n    - [ExLlamaV2](/docs/integrations/llms/exllamav2/)\\n    - [Fireworks](/docs/integrations/llms/fireworks/)\\n    - [ForefrontAI](/docs/integrations/llms/forefrontai/)\\n    - [Friendli](/docs/integrations/llms/friendli/)\\n    - [GigaChat](/docs/integrations/llms/gigachat/)\\n    - [Google AI](/docs/integrations/llms/google_ai/)\\n    - [Google Cloud Vertex AI](/docs/integrations/llms/google_vertex_ai_palm/)\\n    - [GooseAI](/docs/integrations/llms/gooseai/)\\n    - [GPT4All](/docs/integrations/llms/gpt4all/)\\n    - [Gradient](/docs/integrations/llms/gradient/)\\n    - [Huggingface Endpoints](/docs/integrations/llms/huggingface_endpoint/)\\n    - [Hugging Face Local Pipelines](/docs/integrations/llms/huggingface_pipelines/)\\n    - [IBM watsonx.ai](/docs/integrations/llms/ibm_watsonx/)\\n    - [IPEX-LLM](/docs/integrations/llms/ipex_llm/)\\n    - [Javelin AI Gateway Tutorial](/docs/integrations/llms/javelin/)\\n    - [JSONFormer](/docs/integrations/llms/jsonformer_experimental/)\\n    - [KoboldAI API](/docs/integrations/llms/koboldai/)\\n    - [Konko](/docs/integrations/llms/konko/)\\n    - [Layerup Security](/docs/integrations/llms/layerup_security/)\\n    - [Llama.cpp](/docs/integrations/llms/llamacpp/)\\n    - [Llamafile](/docs/integrations/llms/llamafile/)\\n    - [LM Format Enforcer](/docs/integrations/llms/lmformatenforcer_experimental/)\\n    - [Manifest](/docs/integrations/llms/manifest/)\\n    - [Minimax](/docs/integrations/llms/minimax/)\\n    - [MLX Local Pipelines](/docs/integrations/llms/mlx_pipelines/)\\n    - [Modal](/docs/integrations/llms/modal/)\\n    - [MoonshotChat](/docs/integrations/llms/moonshot/)\\n    - [MosaicML](/docs/integrations/llms/mosaicml/)\\n    - [NLP Cloud](/docs/integrations/llms/nlpcloud/)\\n    - [oci\\\\_generative\\\\_ai](/docs/integrations/llms/oci_generative_ai/)\\n    - [OCI Data Science Model Deployment Endpoint](/docs/integrations/llms/oci_model_deployment_endpoint/)\\n    - [OctoAI](/docs/integrations/llms/octoai/)\\n    - [Ollama](/docs/integrations/llms/ollama/)\\n    - [OpaquePrompts](/docs/integrations/llms/opaqueprompts/)\\n    - [OpenAI](/docs/integrations/llms/openai/)\\n    - [OpenLLM](/docs/integrations/llms/openllm/)\\n    - [OpenLM](/docs/integrations/llms/openlm/)\\n    - [OpenVINO](/docs/integrations/llms/openvino/)\\n    - [Petals](/docs/integrations/llms/petals/)\\n    - [PipelineAI](/docs/integrations/llms/pipelineai/)\\n    - [Predibase](/docs/integrations/llms/predibase/)\\n    - [Prediction Guard](/docs/integrations/llms/predictionguard/)\\n    - [PromptLayer OpenAI](/docs/integrations/llms/promptlayer_openai/)\\n    - [RELLM](/docs/integrations/llms/rellm_experimental/)\\n    - [Replicate](/docs/integrations/llms/replicate/)\\n    - [Runhouse](/docs/integrations/llms/runhouse/)\\n    - [SageMakerEndpoint](/docs/integrations/llms/sagemaker/)\\n    - [SambaNova](/docs/integrations/llms/sambanova/)\\n    - [Solar](/docs/integrations/llms/solar/)\\n    - [SparkLLM](/docs/integrations/llms/sparkllm/)\\n    - [StochasticAI](/docs/integrations/llms/stochasticai/)\\n    - [Nebula (Symbl.ai)](/docs/integrations/llms/symblai_nebula/)\\n    - [TextGen](/docs/integrations/llms/textgen/)\\n    - [Titan Takeoff](/docs/integrations/llms/titan_takeoff/)\\n    - [Together AI](/docs/integrations/llms/together/)\\n    - [Tongyi Qwen](/docs/integrations/llms/tongyi/)\\n    - [vLLM](/docs/integrations/llms/vllm/)\\n    - [Volc Engine Maas](/docs/integrations/llms/volcengine_maas/)\\n    - [Intel Weight-Only Quantization](/docs/integrations/llms/weight_only_quantization/)\\n    - [Writer](/docs/integrations/llms/writer/)\\n    - [Xorbits Inference (Xinference)](/docs/integrations/llms/xinference/)\\n    - [YandexGPT](/docs/integrations/llms/yandex/)\\n    - [Yi](/docs/integrations/llms/yi/)\\n    - [Yuan2.0](/docs/integrations/llms/yuan2/)\\n  - [Embedding models](/docs/integrations/text_embedding/)\\n\\n    - [Embedding models](/docs/integrations/text_embedding/)\\n    - [AI21](/docs/integrations/text_embedding/ai21/)\\n    - [Aleph Alpha](/docs/integrations/text_embedding/aleph_alpha/)\\n    - [Anyscale](/docs/integrations/text_embedding/anyscale/)\\n    - [ascend](/docs/integrations/text_embedding/ascend/)\\n    - [AwaDB](/docs/integrations/text_embedding/awadb/)\\n    - [AzureOpenAI](/docs/integrations/text_embedding/azureopenai/)\\n    - [Baichuan Text Embeddings](/docs/integrations/text_embedding/baichuan/)\\n    - [Baidu Qianfan](/docs/integrations/text_embedding/baidu_qianfan_endpoint/)\\n    - [Bedrock](/docs/integrations/text_embedding/bedrock/)\\n    - [BGE on Hugging Face](/docs/integrations/text_embedding/bge_huggingface/)\\n    - [Bookend AI](/docs/integrations/text_embedding/bookend/)\\n    - [Clarifai](/docs/integrations/text_embedding/clarifai/)\\n    - [Cloudflare Workers AI](/docs/integrations/text_embedding/cloudflare_workersai/)\\n    - [Clova Embeddings](/docs/integrations/text_embedding/clova/)\\n    - [Cohere](/docs/integrations/text_embedding/cohere/)\\n    - [DashScope](/docs/integrations/text_embedding/dashscope/)\\n    - [Databricks](/docs/integrations/text_embedding/databricks/)\\n    - [DeepInfra](/docs/integrations/text_embedding/deepinfra/)\\n    - [EDEN AI](/docs/integrations/text_embedding/edenai/)\\n    - [Elasticsearch](/docs/integrations/text_embedding/elasticsearch/)\\n    - [Embaas](/docs/integrations/text_embedding/embaas/)\\n    - [ERNIE](/docs/integrations/text_embedding/ernie/)\\n    - [Fake Embeddings](/docs/integrations/text_embedding/fake/)\\n    - [FastEmbed by Qdrant](/docs/integrations/text_embedding/fastembed/)\\n    - [Fireworks](/docs/integrations/text_embedding/fireworks/)\\n    - [GigaChat](/docs/integrations/text_embedding/gigachat/)\\n    - [Google Generative AI Embeddings](/docs/integrations/text_embedding/google_generative_ai/)\\n    - [Google Vertex AI](/docs/integrations/text_embedding/google_vertex_ai_palm/)\\n    - [GPT4All](/docs/integrations/text_embedding/gpt4all/)\\n    - [Gradient](/docs/integrations/text_embedding/gradient/)\\n    - [Hugging Face](/docs/integrations/text_embedding/huggingfacehub/)\\n    - [IBM watsonx.ai](/docs/integrations/text_embedding/ibm_watsonx/)\\n    - [Infinity](/docs/integrations/text_embedding/infinity/)\\n    - [Instruct Embeddings on Hugging Face](/docs/integrations/text_embedding/instruct_embeddings/)\\n    - [Local BGE Embeddings with IPEX-LLM on Intel CPU](/docs/integrations/text_embedding/ipex_llm/)\\n    - [Local BGE Embeddings with IPEX-LLM on Intel GPU](/docs/integrations/text_embedding/ipex_llm_gpu/)\\n    - [Intel® Extension for Transformers Quantized Text Embeddings](/docs/integrations/text_embedding/itrex/)\\n    - [Jina](/docs/integrations/text_embedding/jina/)\\n    - [John Snow Labs](/docs/integrations/text_embedding/johnsnowlabs_embedding/)\\n    - [LASER Language-Agnostic SEntence Representations Embeddings by Meta AI](/docs/integrations/text_embedding/laser/)\\n    - [Llama.cpp](/docs/integrations/text_embedding/llamacpp/)\\n    - [llamafile](/docs/integrations/text_embedding/llamafile/)\\n    - [LLMRails](/docs/integrations/text_embedding/llm_rails/)\\n    - [LocalAI](/docs/integrations/text_embedding/localai/)\\n    - [MiniMax](/docs/integrations/text_embedding/minimax/)\\n    - [MistralAI](/docs/integrations/text_embedding/mistralai/)\\n    - [ModelScope](/docs/integrations/text_embedding/modelscope_hub/)\\n    - [MosaicML](/docs/integrations/text_embedding/mosaicml/)\\n    - [NLP Cloud](/docs/integrations/text_embedding/nlp_cloud/)\\n    - [Nomic](/docs/integrations/text_embedding/nomic/)\\n    - [NVIDIA NIMs](/docs/integrations/text_embedding/nvidia_ai_endpoints/)\\n    - [Oracle Cloud Infrastructure Generative AI](/docs/integrations/text_embedding/oci_generative_ai/)\\n    - [Ollama](/docs/integrations/text_embedding/ollama/)\\n    - [OpenClip](/docs/integrations/text_embedding/open_clip/)\\n    - [OpenAI](/docs/integrations/text_embedding/openai/)\\n    - [OpenVINO](/docs/integrations/text_embedding/openvino/)\\n    - [Embedding Documents using Optimized and Quantized Embedders](/docs/integrations/text_embedding/optimum_intel/)\\n    - [Oracle AI Vector Search: Generate Embeddings](/docs/integrations/text_embedding/oracleai/)\\n    - [OVHcloud](/docs/integrations/text_embedding/ovhcloud/)\\n    - [Pinecone Embeddings](/docs/integrations/text_embedding/pinecone/)\\n    - [PremAI](/docs/integrations/text_embedding/premai/)\\n    - [SageMaker](/docs/integrations/text_embedding/sagemaker-endpoint/)\\n    - [SambaNova](/docs/integrations/text_embedding/sambanova/)\\n    - [Self Hosted](/docs/integrations/text_embedding/self-hosted/)\\n    - [Sentence Transformers on Hugging Face](/docs/integrations/text_embedding/sentence_transformers/)\\n    - [Solar](/docs/integrations/text_embedding/solar/)\\n    - [SpaCy](/docs/integrations/text_embedding/spacy_embedding/)\\n    - [SparkLLM Text Embeddings](/docs/integrations/text_embedding/sparkllm/)\\n    - [TensorFlow Hub](/docs/integrations/text_embedding/tensorflowhub/)\\n    - [Text Embeddings Inference](/docs/integrations/text_embedding/text_embeddings_inference/)\\n    - [TextEmbed - Embedding Inference Server](/docs/integrations/text_embedding/textembed/)\\n    - [Titan Takeoff](/docs/integrations/text_embedding/titan_takeoff/)\\n    - [Together AI](/docs/integrations/text_embedding/together/)\\n    - [Upstage](/docs/integrations/text_embedding/upstage/)\\n    - [Volc Engine](/docs/integrations/text_embedding/volcengine/)\\n    - [Voyage AI](/docs/integrations/text_embedding/voyageai/)\\n    - [Xorbits inference (Xinference)](/docs/integrations/text_embedding/xinference/)\\n    - [YandexGPT](/docs/integrations/text_embedding/yandex/)\\n    - [ZhipuAI](/docs/integrations/text_embedding/zhipuai/)\\n  - [Document loaders](/docs/integrations/document_loaders/)\\n\\n    - [Document loaders](/docs/integrations/document_loaders/)\\n    - [acreom](/docs/integrations/document_loaders/acreom/)\\n    - [AirbyteLoader](/docs/integrations/document_loaders/airbyte/)\\n    - [Airbyte CDK (Deprecated)](/docs/integrations/document_loaders/airbyte_cdk/)\\n    - [Airbyte Gong (Deprecated)](/docs/integrations/document_loaders/airbyte_gong/)\\n    - [Airbyte Hubspot (Deprecated)](/docs/integrations/document_loaders/airbyte_hubspot/)\\n    - [Airbyte JSON (Deprecated)](/docs/integrations/document_loaders/airbyte_json/)\\n    - [Airbyte Salesforce (Deprecated)](/docs/integrations/document_loaders/airbyte_salesforce/)\\n    - [Airbyte Shopify (Deprecated)](/docs/integrations/document_loaders/airbyte_shopify/)\\n    - [Airbyte Stripe (Deprecated)](/docs/integrations/document_loaders/airbyte_stripe/)\\n    - [Airbyte Typeform (Deprecated)](/docs/integrations/document_loaders/airbyte_typeform/)\\n    - [Airbyte Zendesk Support (Deprecated)](/docs/integrations/document_loaders/airbyte_zendesk_support/)\\n    - [Airtable](/docs/integrations/document_loaders/airtable/)\\n    - [Alibaba Cloud MaxCompute](/docs/integrations/document_loaders/alibaba_cloud_maxcompute/)\\n    - [Amazon Textract](/docs/integrations/document_loaders/amazon_textract/)\\n    - [Apify Dataset](/docs/integrations/document_loaders/apify_dataset/)\\n    - [ArcGIS](/docs/integrations/document_loaders/arcgis/)\\n    - [ArxivLoader](/docs/integrations/document_loaders/arxiv/)\\n    - [AssemblyAI Audio Transcripts](/docs/integrations/document_loaders/assemblyai/)\\n    - [AstraDB](/docs/integrations/document_loaders/astradb/)\\n    - [Async Chromium](/docs/integrations/document_loaders/async_chromium/)\\n    - [AsyncHtml](/docs/integrations/document_loaders/async_html/)\\n    - [Athena](/docs/integrations/document_loaders/athena/)\\n    - [AWS S3 Directory](/docs/integrations/document_loaders/aws_s3_directory/)\\n    - [AWS S3 File](/docs/integrations/document_loaders/aws_s3_file/)\\n    - [AZLyrics](/docs/integrations/document_loaders/azlyrics/)\\n    - [Azure AI Data](/docs/integrations/document_loaders/azure_ai_data/)\\n    - [Azure Blob Storage Container](/docs/integrations/document_loaders/azure_blob_storage_container/)\\n    - [Azure Blob Storage File](/docs/integrations/document_loaders/azure_blob_storage_file/)\\n    - [Azure AI Document Intelligence](/docs/integrations/document_loaders/azure_document_intelligence/)\\n    - [BibTeX](/docs/integrations/document_loaders/bibtex/)\\n    - [BiliBili](/docs/integrations/document_loaders/bilibili/)\\n    - [Blackboard](/docs/integrations/document_loaders/blackboard/)\\n    - [Blockchain](/docs/integrations/document_loaders/blockchain/)\\n    - [Box](/docs/integrations/document_loaders/box/)\\n    - [Brave Search](/docs/integrations/document_loaders/brave_search/)\\n    - [Browserbase](/docs/integrations/document_loaders/browserbase/)\\n    - [Browserless](/docs/integrations/document_loaders/browserless/)\\n    - [BSHTMLLoader](/docs/integrations/document_loaders/bshtml/)\\n    - [Cassandra](/docs/integrations/document_loaders/cassandra/)\\n    - [ChatGPT Data](/docs/integrations/document_loaders/chatgpt_loader/)\\n    - [College Confidential](/docs/integrations/document_loaders/college_confidential/)\\n    - [Concurrent Loader](/docs/integrations/document_loaders/concurrent/)\\n    - [Confluence](/docs/integrations/document_loaders/confluence/)\\n    - [CoNLL-U](/docs/integrations/document_loaders/conll-u/)\\n    - [Copy Paste](/docs/integrations/document_loaders/copypaste/)\\n    - [Couchbase](/docs/integrations/document_loaders/couchbase/)\\n    - [CSV](/docs/integrations/document_loaders/csv/)\\n    - [Cube Semantic Layer](/docs/integrations/document_loaders/cube_semantic/)\\n    - [Datadog Logs](/docs/integrations/document_loaders/datadog_logs/)\\n    - [Dedoc](/docs/integrations/document_loaders/dedoc/)\\n    - [Diffbot](/docs/integrations/document_loaders/diffbot/)\\n    - [Discord](/docs/integrations/document_loaders/discord/)\\n    - [Docugami](/docs/integrations/document_loaders/docugami/)\\n    - [Docusaurus](/docs/integrations/document_loaders/docusaurus/)\\n    - [Dropbox](/docs/integrations/document_loaders/dropbox/)\\n    - [DuckDB](/docs/integrations/document_loaders/duckdb/)\\n    - [Email](/docs/integrations/document_loaders/email/)\\n    - [EPub](/docs/integrations/document_loaders/epub/)\\n    - [Etherscan](/docs/integrations/document_loaders/etherscan/)\\n    - [EverNote](/docs/integrations/document_loaders/evernote/)\\n    - example\\\\_data\\n\\n    - [Facebook Chat](/docs/integrations/document_loaders/facebook_chat/)\\n    - [Fauna](/docs/integrations/document_loaders/fauna/)\\n    - [Figma](/docs/integrations/document_loaders/figma/)\\n    - [FireCrawl](/docs/integrations/document_loaders/firecrawl/)\\n    - [Geopandas](/docs/integrations/document_loaders/geopandas/)\\n    - [Git](/docs/integrations/document_loaders/git/)\\n    - [GitBook](/docs/integrations/document_loaders/gitbook/)\\n    - [GitHub](/docs/integrations/document_loaders/github/)\\n    - [Glue Catalog](/docs/integrations/document_loaders/glue_catalog/)\\n    - [Google AlloyDB for PostgreSQL](/docs/integrations/document_loaders/google_alloydb/)\\n    - [Google BigQuery](/docs/integrations/document_loaders/google_bigquery/)\\n    - [Google Bigtable](/docs/integrations/document_loaders/google_bigtable/)\\n    - [Google Cloud SQL for SQL server](/docs/integrations/document_loaders/google_cloud_sql_mssql/)\\n    - [Google Cloud SQL for MySQL](/docs/integrations/document_loaders/google_cloud_sql_mysql/)\\n    - [Google Cloud SQL for PostgreSQL](/docs/integrations/document_loaders/google_cloud_sql_pg/)\\n    - [Google Cloud Storage Directory](/docs/integrations/document_loaders/google_cloud_storage_directory/)\\n    - [Google Cloud Storage File](/docs/integrations/document_loaders/google_cloud_storage_file/)\\n    - [Google Firestore in Datastore Mode](/docs/integrations/document_loaders/google_datastore/)\\n    - [Google Drive](/docs/integrations/document_loaders/google_drive/)\\n    - [Google El Carro for Oracle Workloads](/docs/integrations/document_loaders/google_el_carro/)\\n    - [Google Firestore (Native Mode)](/docs/integrations/document_loaders/google_firestore/)\\n    - [Google Memorystore for Redis](/docs/integrations/document_loaders/google_memorystore_redis/)\\n    - [Google Spanner](/docs/integrations/document_loaders/google_spanner/)\\n    - [Google Speech-to-Text Audio Transcripts](/docs/integrations/document_loaders/google_speech_to_text/)\\n    - [Grobid](/docs/integrations/document_loaders/grobid/)\\n    - [Gutenberg](/docs/integrations/document_loaders/gutenberg/)\\n    - [Hacker News](/docs/integrations/document_loaders/hacker_news/)\\n    - [Huawei OBS Directory](/docs/integrations/document_loaders/huawei_obs_directory/)\\n    - [Huawei OBS File](/docs/integrations/document_loaders/huawei_obs_file/)\\n    - [HuggingFace dataset](/docs/integrations/document_loaders/hugging_face_dataset/)\\n    - [iFixit](/docs/integrations/document_loaders/ifixit/)\\n    - [Images](/docs/integrations/document_loaders/image/)\\n    - [Image captions](/docs/integrations/document_loaders/image_captions/)\\n    - [IMSDb](/docs/integrations/document_loaders/imsdb/)\\n    - [Iugu](/docs/integrations/document_loaders/iugu/)\\n    - [Joplin](/docs/integrations/document_loaders/joplin/)\\n    - [JSONLoader](/docs/integrations/document_loaders/json/)\\n    - [Jupyter Notebook](/docs/integrations/document_loaders/jupyter_notebook/)\\n    - [Kinetica](/docs/integrations/document_loaders/kinetica/)\\n    - [lakeFS](/docs/integrations/document_loaders/lakefs/)\\n    - [LangSmith](/docs/integrations/document_loaders/langsmith/)\\n    - [LarkSuite (FeiShu)](/docs/integrations/document_loaders/larksuite/)\\n    - [LLM Sherpa](/docs/integrations/document_loaders/llmsherpa/)\\n    - [Mastodon](/docs/integrations/document_loaders/mastodon/)\\n    - [MathPixPDFLoader](/docs/integrations/document_loaders/mathpix/)\\n    - [MediaWiki Dump](/docs/integrations/document_loaders/mediawikidump/)\\n    - [Merge Documents Loader](/docs/integrations/document_loaders/merge_doc/)\\n    - [mhtml](/docs/integrations/document_loaders/mhtml/)\\n    - [Microsoft Excel](/docs/integrations/document_loaders/microsoft_excel/)\\n    - [Microsoft OneDrive](/docs/integrations/document_loaders/microsoft_onedrive/)\\n    - [Microsoft OneNote](/docs/integrations/document_loaders/microsoft_onenote/)\\n    - [Microsoft PowerPoint](/docs/integrations/document_loaders/microsoft_powerpoint/)\\n    - [Microsoft SharePoint](/docs/integrations/document_loaders/microsoft_sharepoint/)\\n    - [Microsoft Word](/docs/integrations/document_loaders/microsoft_word/)\\n    - [Near Blockchain](/docs/integrations/document_loaders/mintbase/)\\n    - [Modern Treasury](/docs/integrations/document_loaders/modern_treasury/)\\n    - [MongoDB](/docs/integrations/document_loaders/mongodb/)\\n    - [News URL](/docs/integrations/document_loaders/news/)\\n    - [Notion DB 2/2](/docs/integrations/document_loaders/notion/)\\n    - [Nuclia](/docs/integrations/document_loaders/nuclia/)\\n    - [Obsidian](/docs/integrations/document_loaders/obsidian/)\\n    - [Open Document Format (ODT)](/docs/integrations/document_loaders/odt/)\\n    - [Open City Data](/docs/integrations/document_loaders/open_city_data/)\\n    - [Oracle Autonomous Database](/docs/integrations/document_loaders/oracleadb_loader/)\\n    - [Oracle AI Vector Search: Document Processing](/docs/integrations/document_loaders/oracleai/)\\n    - [Org-mode](/docs/integrations/document_loaders/org_mode/)\\n    - [Pandas DataFrame](/docs/integrations/document_loaders/pandas_dataframe/)\\n    - [PDFMiner](/docs/integrations/document_loaders/pdfminer/)\\n    - [PDFPlumber](/docs/integrations/document_loaders/pdfplumber/)\\n    - [Pebblo Safe DocumentLoader](/docs/integrations/document_loaders/pebblo/)\\n    - [Polars DataFrame](/docs/integrations/document_loaders/polars_dataframe/)\\n    - [Psychic](/docs/integrations/document_loaders/psychic/)\\n    - [PubMed](/docs/integrations/document_loaders/pubmed/)\\n    - [PyMuPDF](/docs/integrations/document_loaders/pymupdf/)\\n    - [PyPDFDirectoryLoader](/docs/integrations/document_loaders/pypdfdirectory/)\\n    - [PyPDFium2Loader](/docs/integrations/document_loaders/pypdfium2/)\\n    - [PyPDFLoader](/docs/integrations/document_loaders/pypdfloader/)\\n    - [PySpark](/docs/integrations/document_loaders/pyspark_dataframe/)\\n    - [Quip](/docs/integrations/document_loaders/quip/)\\n    - [ReadTheDocs Documentation](/docs/integrations/document_loaders/readthedocs_documentation/)\\n    - [Recursive URL](/docs/integrations/document_loaders/recursive_url/)\\n    - [Reddit](/docs/integrations/document_loaders/reddit/)\\n    - [Roam](/docs/integrations/document_loaders/roam/)\\n    - [Rockset](/docs/integrations/document_loaders/rockset/)\\n    - [rspace](/docs/integrations/document_loaders/rspace/)\\n    - [RSS Feeds](/docs/integrations/document_loaders/rss/)\\n    - [RST](/docs/integrations/document_loaders/rst/)\\n    - [scrapfly](/docs/integrations/document_loaders/scrapfly/)\\n    - [ScrapingAnt](/docs/integrations/document_loaders/scrapingant/)\\n    - [Sitemap](/docs/integrations/document_loaders/sitemap/)\\n    - [Slack](/docs/integrations/document_loaders/slack/)\\n    - [Snowflake](/docs/integrations/document_loaders/snowflake/)\\n    - [Source Code](/docs/integrations/document_loaders/source_code/)\\n    - [Spider](/docs/integrations/document_loaders/spider/)\\n    - [Spreedly](/docs/integrations/document_loaders/spreedly/)\\n    - [Stripe](/docs/integrations/document_loaders/stripe/)\\n    - [Subtitle](/docs/integrations/document_loaders/subtitle/)\\n    - [SurrealDB](/docs/integrations/document_loaders/surrealdb/)\\n    - [Telegram](/docs/integrations/document_loaders/telegram/)\\n    - [Tencent COS Directory](/docs/integrations/document_loaders/tencent_cos_directory/)\\n    - [Tencent COS File](/docs/integrations/document_loaders/tencent_cos_file/)\\n    - [TensorFlow Datasets](/docs/integrations/document_loaders/tensorflow_datasets/)\\n    - [TiDB](/docs/integrations/document_loaders/tidb/)\\n    - [2Markdown](/docs/integrations/document_loaders/tomarkdown/)\\n    - [TOML](/docs/integrations/document_loaders/toml/)\\n    - [Trello](/docs/integrations/document_loaders/trello/)\\n    - [TSV](/docs/integrations/document_loaders/tsv/)\\n    - [Twitter](/docs/integrations/document_loaders/twitter/)\\n    - [Unstructured](/docs/integrations/document_loaders/unstructured_file/)\\n    - [UnstructuredMarkdownLoader](/docs/integrations/document_loaders/unstructured_markdown/)\\n    - [UnstructuredPDFLoader](/docs/integrations/document_loaders/unstructured_pdfloader/)\\n    - [Upstage](/docs/integrations/document_loaders/upstage/)\\n    - [URL](/docs/integrations/document_loaders/url/)\\n    - [Vsdx](/docs/integrations/document_loaders/vsdx/)\\n    - [Weather](/docs/integrations/document_loaders/weather/)\\n    - [WebBaseLoader](/docs/integrations/document_loaders/web_base/)\\n    - [WhatsApp Chat](/docs/integrations/document_loaders/whatsapp_chat/)\\n    - [Wikipedia](/docs/integrations/document_loaders/wikipedia/)\\n    - [UnstructuredXMLLoader](/docs/integrations/document_loaders/xml/)\\n    - [Xorbits Pandas DataFrame](/docs/integrations/document_loaders/xorbits/)\\n    - [YouTube audio](/docs/integrations/document_loaders/youtube_audio/)\\n    - [YouTube transcripts](/docs/integrations/document_loaders/youtube_transcript/)\\n    - [Yuque](/docs/integrations/document_loaders/yuque/)\\n  - [Vector stores](/docs/integrations/vectorstores/)\\n\\n    - [Vectorstores](/docs/integrations/vectorstores/)\\n    - [Activeloop Deep Lake](/docs/integrations/vectorstores/activeloop_deeplake/)\\n    - [Aerospike](/docs/integrations/vectorstores/aerospike/)\\n    - [Alibaba Cloud OpenSearch](/docs/integrations/vectorstores/alibabacloud_opensearch/)\\n    - [AnalyticDB](/docs/integrations/vectorstores/analyticdb/)\\n    - [Annoy](/docs/integrations/vectorstores/annoy/)\\n    - [Apache Doris](/docs/integrations/vectorstores/apache_doris/)\\n    - [ApertureDB](/docs/integrations/vectorstores/aperturedb/)\\n    - [Astra DB Vector Store](/docs/integrations/vectorstores/astradb/)\\n    - [Atlas](/docs/integrations/vectorstores/atlas/)\\n    - [AwaDB](/docs/integrations/vectorstores/awadb/)\\n    - [Azure Cosmos DB Mongo vCore](/docs/integrations/vectorstores/azure_cosmos_db/)\\n    - [Azure Cosmos DB No SQL](/docs/integrations/vectorstores/azure_cosmos_db_no_sql/)\\n    - [Azure AI Search](/docs/integrations/vectorstores/azuresearch/)\\n    - [Bagel](/docs/integrations/vectorstores/bagel/)\\n    - [BagelDB](/docs/integrations/vectorstores/bageldb/)\\n    - [Baidu Cloud ElasticSearch VectorSearch](/docs/integrations/vectorstores/baiducloud_vector_search/)\\n    - [Baidu VectorDB](/docs/integrations/vectorstores/baiduvectordb/)\\n    - [Apache Cassandra](/docs/integrations/vectorstores/cassandra/)\\n    - [Chroma](/docs/integrations/vectorstores/chroma/)\\n    - [Clarifai](/docs/integrations/vectorstores/clarifai/)\\n    - [ClickHouse](/docs/integrations/vectorstores/clickhouse/)\\n    - [Couchbase](/docs/integrations/vectorstores/couchbase/)\\n    - [DashVector](/docs/integrations/vectorstores/dashvector/)\\n    - [Databricks](/docs/integrations/vectorstores/databricks_vector_search/)\\n    - [DingoDB](/docs/integrations/vectorstores/dingo/)\\n    - [DocArray HnswSearch](/docs/integrations/vectorstores/docarray_hnsw/)\\n    - [DocArray InMemorySearch](/docs/integrations/vectorstores/docarray_in_memory/)\\n    - [Amazon Document DB](/docs/integrations/vectorstores/documentdb/)\\n    - [DuckDB](/docs/integrations/vectorstores/duckdb/)\\n    - [China Mobile ECloud ElasticSearch VectorSearch](/docs/integrations/vectorstores/ecloud_vector_search/)\\n    - [Elasticsearch](/docs/integrations/vectorstores/elasticsearch/)\\n    - [Epsilla](/docs/integrations/vectorstores/epsilla/)\\n    - [Faiss](/docs/integrations/vectorstores/faiss/)\\n    - [Faiss (Async)](/docs/integrations/vectorstores/faiss_async/)\\n    - [Google AlloyDB for PostgreSQL](/docs/integrations/vectorstores/google_alloydb/)\\n    - [Google BigQuery Vector Search](/docs/integrations/vectorstores/google_bigquery_vector_search/)\\n    - [Google Cloud SQL for MySQL](/docs/integrations/vectorstores/google_cloud_sql_mysql/)\\n    - [Google Cloud SQL for PostgreSQL](/docs/integrations/vectorstores/google_cloud_sql_pg/)\\n    - [Firestore](/docs/integrations/vectorstores/google_firestore/)\\n    - [Google Memorystore for Redis](/docs/integrations/vectorstores/google_memorystore_redis/)\\n    - [Google Spanner](/docs/integrations/vectorstores/google_spanner/)\\n    - [Google Vertex AI Feature Store](/docs/integrations/vectorstores/google_vertex_ai_feature_store/)\\n    - [Google Vertex AI Vector Search](/docs/integrations/vectorstores/google_vertex_ai_vector_search/)\\n    - [Hippo](/docs/integrations/vectorstores/hippo/)\\n    - [Hologres](/docs/integrations/vectorstores/hologres/)\\n    - [Infinispan](/docs/integrations/vectorstores/infinispanvs/)\\n    - [Jaguar Vector Database](/docs/integrations/vectorstores/jaguar/)\\n    - [KDB.AI](/docs/integrations/vectorstores/kdbai/)\\n    - [Kinetica](/docs/integrations/vectorstores/kinetica/)\\n    - [LanceDB](/docs/integrations/vectorstores/lancedb/)\\n    - [Lantern](/docs/integrations/vectorstores/lantern/)\\n    - [LLMRails](/docs/integrations/vectorstores/llm_rails/)\\n    - [ManticoreSearch VectorStore](/docs/integrations/vectorstores/manticore_search/)\\n    - [Marqo](/docs/integrations/vectorstores/marqo/)\\n    - [Meilisearch](/docs/integrations/vectorstores/meilisearch/)\\n    - [Amazon MemoryDB](/docs/integrations/vectorstores/memorydb/)\\n    - [Milvus](/docs/integrations/vectorstores/milvus/)\\n    - [Momento Vector Index (MVI)](/docs/integrations/vectorstores/momento_vector_index/)\\n    - [MongoDB Atlas](/docs/integrations/vectorstores/mongodb_atlas/)\\n    - [MyScale](/docs/integrations/vectorstores/myscale/)\\n    - [Neo4j Vector Index](/docs/integrations/vectorstores/neo4jvector/)\\n    - [NucliaDB](/docs/integrations/vectorstores/nucliadb/)\\n    - [OpenSearch](/docs/integrations/vectorstores/opensearch/)\\n    - [Oracle AI Vector Search: Vector Store](/docs/integrations/vectorstores/oracle/)\\n    - [Pathway](/docs/integrations/vectorstores/pathway/)\\n    - [Postgres Embedding](/docs/integrations/vectorstores/pgembedding/)\\n    - [PGVecto.rs](/docs/integrations/vectorstores/pgvecto_rs/)\\n    - [PGVector](/docs/integrations/vectorstores/pgvector/)\\n    - [Pinecone](/docs/integrations/vectorstores/pinecone/)\\n    - [Qdrant](/docs/integrations/vectorstores/qdrant/)\\n    - [Redis](/docs/integrations/vectorstores/redis/)\\n    - [Relyt](/docs/integrations/vectorstores/relyt/)\\n    - [Rockset](/docs/integrations/vectorstores/rockset/)\\n    - [SAP HANA Cloud Vector Engine](/docs/integrations/vectorstores/sap_hanavector/)\\n    - [ScaNN](/docs/integrations/vectorstores/scann/)\\n    - [SemaDB](/docs/integrations/vectorstores/semadb/)\\n    - [SingleStoreDB](/docs/integrations/vectorstores/singlestoredb/)\\n    - [scikit-learn](/docs/integrations/vectorstores/sklearn/)\\n    - [SQLite-VSS](/docs/integrations/vectorstores/sqlitevss/)\\n    - [StarRocks](/docs/integrations/vectorstores/starrocks/)\\n    - [Supabase (Postgres)](/docs/integrations/vectorstores/supabase/)\\n    - [SurrealDB](/docs/integrations/vectorstores/surrealdb/)\\n    - [Tair](/docs/integrations/vectorstores/tair/)\\n    - [Tencent Cloud VectorDB](/docs/integrations/vectorstores/tencentvectordb/)\\n    - [ThirdAI NeuralDB](/docs/integrations/vectorstores/thirdai_neuraldb/)\\n    - [TiDB Vector](/docs/integrations/vectorstores/tidb_vector/)\\n    - [Tigris](/docs/integrations/vectorstores/tigris/)\\n    - [TileDB](/docs/integrations/vectorstores/tiledb/)\\n    - [Timescale Vector (Postgres)](/docs/integrations/vectorstores/timescalevector/)\\n    - [Typesense](/docs/integrations/vectorstores/typesense/)\\n    - [Upstash Vector](/docs/integrations/vectorstores/upstash/)\\n    - [USearch](/docs/integrations/vectorstores/usearch/)\\n    - [Vald](/docs/integrations/vectorstores/vald/)\\n    - [Intel\\'s Visual Data Management System (VDMS)](/docs/integrations/vectorstores/vdms/)\\n    - [Vearch](/docs/integrations/vectorstores/vearch/)\\n    - [Vectara](/docs/integrations/vectorstores/vectara/)\\n    - [Vespa](/docs/integrations/vectorstores/vespa/)\\n    - [viking DB](/docs/integrations/vectorstores/vikingdb/)\\n    - [vlite](/docs/integrations/vectorstores/vlite/)\\n    - [Weaviate](/docs/integrations/vectorstores/weaviate/)\\n    - [Xata](/docs/integrations/vectorstores/xata/)\\n    - [Yellowbrick](/docs/integrations/vectorstores/yellowbrick/)\\n    - [Zep](/docs/integrations/vectorstores/zep/)\\n    - [Zep Cloud](/docs/integrations/vectorstores/zep_cloud/)\\n    - [Zilliz](/docs/integrations/vectorstores/zilliz/)\\n  - [Retrievers](/docs/integrations/retrievers/)\\n\\n    - [Retrievers](/docs/integrations/retrievers/)\\n    - [Activeloop Deep Memory](/docs/integrations/retrievers/activeloop/)\\n    - [Amazon Kendra](/docs/integrations/retrievers/amazon_kendra_retriever/)\\n    - [Arcee](/docs/integrations/retrievers/arcee/)\\n    - [Arxiv](/docs/integrations/retrievers/arxiv/)\\n    - [AskNews](/docs/integrations/retrievers/asknews/)\\n    - [Azure AI Search](/docs/integrations/retrievers/azure_ai_search/)\\n    - [Bedrock (Knowledge Bases)](/docs/integrations/retrievers/bedrock/)\\n    - [BM25](/docs/integrations/retrievers/bm25/)\\n    - [Box](/docs/integrations/retrievers/box/)\\n    - [BREEBS (Open Knowledge)](/docs/integrations/retrievers/breebs/)\\n    - [Chaindesk](/docs/integrations/retrievers/chaindesk/)\\n    - [ChatGPT plugin](/docs/integrations/retrievers/chatgpt-plugin/)\\n    - [Cohere reranker](/docs/integrations/retrievers/cohere-reranker/)\\n    - [Cohere RAG](/docs/integrations/retrievers/cohere/)\\n    - [DocArray](/docs/integrations/retrievers/docarray_retriever/)\\n    - [Dria](/docs/integrations/retrievers/dria_index/)\\n    - [ElasticSearch BM25](/docs/integrations/retrievers/elastic_search_bm25/)\\n    - [Elasticsearch](/docs/integrations/retrievers/elasticsearch_retriever/)\\n    - [Embedchain](/docs/integrations/retrievers/embedchain/)\\n    - [FlashRank reranker](/docs/integrations/retrievers/flashrank-reranker/)\\n    - [Fleet AI Context](/docs/integrations/retrievers/fleet_context/)\\n    - [Google Drive](/docs/integrations/retrievers/google_drive/)\\n    - [Google Vertex AI Search](/docs/integrations/retrievers/google_vertex_ai_search/)\\n    - [JaguarDB Vector Database](/docs/integrations/retrievers/jaguar/)\\n    - [Kay.ai](/docs/integrations/retrievers/kay/)\\n    - [Kinetica Vectorstore based Retriever](/docs/integrations/retrievers/kinetica/)\\n    - [kNN](/docs/integrations/retrievers/knn/)\\n    - [LLMLingua Document Compressor](/docs/integrations/retrievers/llmlingua/)\\n    - [LOTR (Merger Retriever)](/docs/integrations/retrievers/merger_retriever/)\\n    - [Metal](/docs/integrations/retrievers/metal/)\\n    - [Milvus Hybrid Search](/docs/integrations/retrievers/milvus_hybrid_search/)\\n    - [NanoPQ (Product Quantization)](/docs/integrations/retrievers/nanopq/)\\n    - [Outline](/docs/integrations/retrievers/outline/)\\n    - [Pinecone Hybrid Search](/docs/integrations/retrievers/pinecone_hybrid_search/)\\n    - [PubMed](/docs/integrations/retrievers/pubmed/)\\n    - [Qdrant Sparse Vector](/docs/integrations/retrievers/qdrant-sparse/)\\n    - [RAGatouille](/docs/integrations/retrievers/ragatouille/)\\n    - [RePhraseQuery](/docs/integrations/retrievers/re_phrase/)\\n    - [Rememberizer](/docs/integrations/retrievers/rememberizer/)\\n    - [SEC filing](/docs/integrations/retrievers/sec_filings/)\\n    - [Self-querying retrievers](/docs/integrations/retrievers/self_query/)\\n\\n    - [SingleStoreDB](/docs/integrations/retrievers/singlestoredb/)\\n    - [SVM](/docs/integrations/retrievers/svm/)\\n    - [TavilySearchAPI](/docs/integrations/retrievers/tavily/)\\n    - [TF-IDF](/docs/integrations/retrievers/tf_idf/)\\n    - [\\\\*\\\\*NeuralDB\\\\*\\\\*](/docs/integrations/retrievers/thirdai_neuraldb/)\\n    - [Vespa](/docs/integrations/retrievers/vespa/)\\n    - [Weaviate Hybrid Search](/docs/integrations/retrievers/weaviate-hybrid/)\\n    - [Wikipedia](/docs/integrations/retrievers/wikipedia/)\\n    - [You.com](/docs/integrations/retrievers/you-retriever/)\\n    - [Zep Cloud](/docs/integrations/retrievers/zep_cloud_memorystore/)\\n    - [Zep Open Source](/docs/integrations/retrievers/zep_memorystore/)\\n    - [Zilliz Cloud Pipeline](/docs/integrations/retrievers/zilliz_cloud_pipeline/)\\n  - [Tools/Toolkits](/docs/integrations/tools/)\\n\\n    - [Tools](/docs/integrations/tools/)\\n    - [AINetwork Toolkit](/docs/integrations/tools/ainetwork/)\\n    - [Alpha Vantage](/docs/integrations/tools/alpha_vantage/)\\n    - [Amadeus Toolkit](/docs/integrations/tools/amadeus/)\\n    - [ArXiv](/docs/integrations/tools/arxiv/)\\n    - [AskNews](/docs/integrations/tools/asknews/)\\n    - [AWS Lambda](/docs/integrations/tools/awslambda/)\\n    - [Azure AI Services Toolkit](/docs/integrations/tools/azure_ai_services/)\\n    - [Azure Cognitive Services Toolkit](/docs/integrations/tools/azure_cognitive_services/)\\n    - [Azure Container Apps dynamic sessions](/docs/integrations/tools/azure_dynamic_sessions/)\\n    - [Shell (bash)](/docs/integrations/tools/bash/)\\n    - [Bearly Code Interpreter](/docs/integrations/tools/bearly/)\\n    - [Bing Search](/docs/integrations/tools/bing_search/)\\n    - [Brave Search](/docs/integrations/tools/brave_search/)\\n    - [Cassandra Database Toolkit](/docs/integrations/tools/cassandra_database/)\\n    - [ChatGPT Plugins](/docs/integrations/tools/chatgpt_plugins/)\\n    - [ClickUp Toolkit](/docs/integrations/tools/clickup/)\\n    - [Cogniswitch Toolkit](/docs/integrations/tools/cogniswitch/)\\n    - [Connery Toolkit and Tools](/docs/integrations/tools/connery/)\\n    - [Dall-E Image Generator](/docs/integrations/tools/dalle_image_generator/)\\n    - [Databricks Unity Catalog (UC)](/docs/integrations/tools/databricks/)\\n    - [DataForSEO](/docs/integrations/tools/dataforseo/)\\n    - [Dataherald](/docs/integrations/tools/dataherald/)\\n    - [DuckDuckGo Search](/docs/integrations/tools/ddg/)\\n    - [E2B Data Analysis](/docs/integrations/tools/e2b_data_analysis/)\\n    - [Eden AI](/docs/integrations/tools/edenai_tools/)\\n    - [Eleven Labs Text2Speech](/docs/integrations/tools/eleven_labs_tts/)\\n    - [Exa Search](/docs/integrations/tools/exa_search/)\\n    - [File System](/docs/integrations/tools/filesystem/)\\n    - [FinancialDatasets Toolkit](/docs/integrations/tools/financial_datasets/)\\n    - [Github Toolkit](/docs/integrations/tools/github/)\\n    - [Gitlab Toolkit](/docs/integrations/tools/gitlab/)\\n    - [Gmail Toolkit](/docs/integrations/tools/gmail/)\\n    - [Golden Query](/docs/integrations/tools/golden_query/)\\n    - [Google Cloud Text-to-Speech](/docs/integrations/tools/google_cloud_texttospeech/)\\n    - [Google Drive](/docs/integrations/tools/google_drive/)\\n    - [Google Finance](/docs/integrations/tools/google_finance/)\\n    - [Google Imagen](/docs/integrations/tools/google_imagen/)\\n    - [Google Jobs](/docs/integrations/tools/google_jobs/)\\n    - [Google Lens](/docs/integrations/tools/google_lens/)\\n    - [Google Places](/docs/integrations/tools/google_places/)\\n    - [Google Scholar](/docs/integrations/tools/google_scholar/)\\n    - [Google Search](/docs/integrations/tools/google_search/)\\n    - [Google Serper](/docs/integrations/tools/google_serper/)\\n    - [Google Trends](/docs/integrations/tools/google_trends/)\\n    - [Gradio](/docs/integrations/tools/gradio_tools/)\\n    - [GraphQL](/docs/integrations/tools/graphql/)\\n    - [HuggingFace Hub Tools](/docs/integrations/tools/huggingface_tools/)\\n    - [Human as a tool](/docs/integrations/tools/human_tools/)\\n    - [IFTTT WebHooks](/docs/integrations/tools/ifttt/)\\n    - [Infobip](/docs/integrations/tools/infobip/)\\n    - [Ionic Shopping Tool](/docs/integrations/tools/ionic_shopping/)\\n    - [Jina Search](/docs/integrations/tools/jina_search/)\\n    - [Jira Toolkit](/docs/integrations/tools/jira/)\\n    - [JSON Toolkit](/docs/integrations/tools/json/)\\n    - [Lemon Agent](/docs/integrations/tools/lemonai/)\\n    - [Memorize](/docs/integrations/tools/memorize/)\\n    - [Mojeek Search](/docs/integrations/tools/mojeek_search/)\\n    - [MultiOn Toolkit](/docs/integrations/tools/multion/)\\n    - [NASA Toolkit](/docs/integrations/tools/nasa/)\\n    - [Nuclia Understanding](/docs/integrations/tools/nuclia/)\\n    - [NVIDIA Riva: ASR and TTS](/docs/integrations/tools/nvidia_riva/)\\n    - [Office365 Toolkit](/docs/integrations/tools/office365/)\\n    - [OpenAPI Toolkit](/docs/integrations/tools/openapi/)\\n    - [Natural Language API Toolkits](/docs/integrations/tools/openapi_nla/)\\n    - [OpenWeatherMap](/docs/integrations/tools/openweathermap/)\\n    - [Oracle AI Vector Search: Generate Summary](/docs/integrations/tools/oracleai/)\\n    - [Pandas Dataframe](/docs/integrations/tools/pandas/)\\n    - [Passio NutritionAI](/docs/integrations/tools/passio_nutrition_ai/)\\n    - [PlayWright Browser Toolkit](/docs/integrations/tools/playwright/)\\n    - [Polygon IO Toolkit and Tools](/docs/integrations/tools/polygon/)\\n    - [PowerBI Toolkit](/docs/integrations/tools/powerbi/)\\n    - [PubMed](/docs/integrations/tools/pubmed/)\\n    - [Python REPL](/docs/integrations/tools/python/)\\n    - [Reddit Search](/docs/integrations/tools/reddit_search/)\\n    - [Requests Toolkit](/docs/integrations/tools/requests/)\\n    - [Riza Code Interpreter](/docs/integrations/tools/riza/)\\n    - [Robocorp Toolkit](/docs/integrations/tools/robocorp/)\\n    - [SceneXplain](/docs/integrations/tools/sceneXplain/)\\n    - [SearchApi](/docs/integrations/tools/searchapi/)\\n    - [SearxNG Search](/docs/integrations/tools/searx_search/)\\n    - [Semantic Scholar API Tool](/docs/integrations/tools/semanticscholar/)\\n    - [SerpAPI](/docs/integrations/tools/serpapi/)\\n    - [Slack Toolkit](/docs/integrations/tools/slack/)\\n    - [Spark SQL Toolkit](/docs/integrations/tools/spark_sql/)\\n    - [SQLDatabase Toolkit](/docs/integrations/tools/sql_database/)\\n    - [StackExchange](/docs/integrations/tools/stackexchange/)\\n    - [Steam Toolkit](/docs/integrations/tools/steam/)\\n    - [Tavily Search](/docs/integrations/tools/tavily_search/)\\n    - [Twilio](/docs/integrations/tools/twilio/)\\n    - [Upstage](/docs/integrations/tools/upstage_groundedness_check/)\\n    - [Wikidata](/docs/integrations/tools/wikidata/)\\n    - [Wikipedia](/docs/integrations/tools/wikipedia/)\\n    - [Wolfram Alpha](/docs/integrations/tools/wolfram_alpha/)\\n    - [Yahoo Finance News](/docs/integrations/tools/yahoo_finance_news/)\\n    - [You.com Search](/docs/integrations/tools/you/)\\n    - [YouTube](/docs/integrations/tools/youtube/)\\n    - [Zapier Natural Language Actions](/docs/integrations/tools/zapier/)\\n    - [ZenGuard AI](/docs/integrations/tools/zenguard/)\\n  - [Key-value stores](/docs/integrations/stores/)\\n\\n    - [AstraDB](/docs/integrations/stores/astradb/)\\n    - [Cassandra](/docs/integrations/stores/cassandra/)\\n    - [Elasticsearch](/docs/integrations/stores/elasticsearch/)\\n    - [Local Filesystem](/docs/integrations/stores/file_system/)\\n    - [In-memory](/docs/integrations/stores/in_memory/)\\n    - [Key-value stores](/docs/integrations/stores/)\\n    - [Redis](/docs/integrations/stores/redis/)\\n    - [Upstash Redis](/docs/integrations/stores/upstash_redis/)\\n  - Other\\n\\n- [Home page](/)\\n- [Components](/docs/integrations/components/)\\n- [Document loaders](/docs/integrations/document_loaders/)\\n- FireCrawl\\n\\nOn this page\\n\\n# FireCrawl\\n\\n[FireCrawl](https://firecrawl.dev/?ref=langchain) crawls and convert any website into LLM-ready data. It crawls all accessible subpages and give you clean markdown and metadata for each. No sitemap required.\\n\\nFireCrawl handles complex tasks such as reverse proxies, caching, rate limits, and content blocked by JavaScript. Built by the [mendable.ai](https://mendable.ai) team.\\n\\n## Overview [\\u200b](\\\\#overview \"Direct link to Overview\")\\n\\n### Integration details [\\u200b](\\\\#integration-details \"Direct link to Integration details\")\\n\\n| Class | Package | Local | Serializable | [JS support](https://js.langchain.com/docs/integrations/document_loaders/web_loaders/firecrawl/) |\\n| :-- | :-- | :-: | :-: | :-: |\\n| [FireCrawlLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.firecrawl.FireCrawlLoader.html) | [langchain\\\\_community](https://python.langchain.com/api_reference/community/index.html) | ✅ | ❌ | ✅ |\\n\\n### Loader features [\\u200b](\\\\#loader-features \"Direct link to Loader features\")\\n\\n| Source | Document Lazy Loading | Native Async Support |\\n| :-: | :-: | :-: |\\n| FireCrawlLoader | ✅ | ❌ |\\n\\n## Setup [\\u200b](\\\\#setup \"Direct link to Setup\")\\n\\n### Credentials [\\u200b](\\\\#credentials \"Direct link to Credentials\")\\n\\nYou will need to get your own API key. Go to [this page](https://firecrawl.dev) to learn more.\\n\\n```codeBlockLines_e6Vv\\nimport getpass\\nimport os\\n\\nif \"FIRECRAWL_API_KEY\" not in os.environ:\\n    os.environ[\"FIRECRAWL_API_KEY\"] = getpass.getpass(\"Enter your Firecrawl API key: \")\\n\\n```\\n\\n### Installation [\\u200b](\\\\#installation \"Direct link to Installation\")\\n\\nYou will need to install both the `langchain_community` and `firecrawl-py` pacakges:\\n\\n```codeBlockLines_e6Vv\\n%pip install -qU firecrawl-py==0.0.20 langchain_community\\n\\n```\\n\\n## Initialization [\\u200b](\\\\#initialization \"Direct link to Initialization\")\\n\\n### Modes [\\u200b](\\\\#modes \"Direct link to Modes\")\\n\\n- `scrape`: Scrape single url and return the markdown.\\n- `crawl`: Crawl the url and all accessible sub pages and return the markdown for each one.\\n\\n```codeBlockLines_e6Vv\\nfrom langchain_community.document_loaders import FireCrawlLoader\\n\\nloader = FireCrawlLoader(url=\"https://firecrawl.dev\", mode=\"crawl\")\\n\\n```\\n\\n**API Reference:** [FireCrawlLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.firecrawl.FireCrawlLoader.html)\\n\\n## Load [\\u200b](\\\\#load \"Direct link to Load\")\\n\\n```codeBlockLines_e6Vv\\ndocs = loader.load()\\n\\ndocs[0]\\n\\n```\\n\\n```codeBlockLines_e6Vv\\nDocument(metadata={\\'ogUrl\\': \\'https://www.firecrawl.dev/\\', \\'title\\': \\'Home - Firecrawl\\', \\'robots\\': \\'follow, index\\', \\'ogImage\\': \\'https://www.firecrawl.dev/og.png?123\\', \\'ogTitle\\': \\'Firecrawl\\', \\'sitemap\\': {\\'lastmod\\': \\'2024-08-12T00:28:16.681Z\\', \\'changefreq\\': \\'weekly\\'}, \\'keywords\\': \\'Firecrawl,Markdown,Data,Mendable,Langchain\\', \\'sourceURL\\': \\'https://www.firecrawl.dev/\\', \\'ogSiteName\\': \\'Firecrawl\\', \\'description\\': \\'Firecrawl crawls and converts any website into clean markdown.\\', \\'ogDescription\\': \\'Turn any website into LLM-ready data.\\', \\'pageStatusCode\\': 200, \\'ogLocaleAlternate\\': []}, page_content=\\'Introducing [Smart Crawl!](https://www.firecrawl.dev/smart-crawl)\\\\n Join the waitlist to turn any website into an API with AI\\\\n\\\\n\\\\n\\\\n[🔥 Firecrawl](/)\\\\n\\\\n*   [Playground](/playground)\\\\n    \\\\n*   [Docs](https://docs.firecrawl.dev)\\\\n    \\\\n*   [Pricing](/pricing)\\\\n    \\\\n*   [Blog](/blog)\\\\n    \\\\n*   Beta Features\\\\n\\\\n[Log In](/signin)\\\\n[Log In](/signin)\\\\n[Sign Up](/signin/signup)\\\\n 8.9k\\\\n\\\\n[💥 Get 2 months free with yearly plan](/pricing)\\\\n\\\\nTurn websites into  \\\\n_LLM-ready_ data\\\\n=====================================\\\\n\\\\nPower your AI apps with clean data crawled from any website. It\\\\\\'s also open-source.\\\\n\\\\nStart for free (500 credits)Start for free[Talk to us](https://calendly.com/d/cj83-ngq-knk/meet-firecrawl)\\\\n\\\\nA product by\\\\n\\\\n[![Mendable Logo](https://www.firecrawl.dev/images/mendable_logo_transparent.png)Mendable](https://mendable.ai)\\\\n\\\\n![Example Webpage](https://www.firecrawl.dev/multiple-websites.png)\\\\n\\\\nCrawl, Scrape, Clean\\\\n--------------------\\\\n\\\\nWe crawl all accessible subpages and give you clean markdown for each. No sitemap required.\\\\n\\\\n    \\\\n      [\\\\\\\\\\\\n        {\\\\\\\\\\\\n          \"url\": \"https://www.firecrawl.dev/\",\\\\\\\\\\\\n          \"markdown\": \"## Welcome to Firecrawl\\\\\\\\\\\\n            Firecrawl is a web scraper that allows you to extract the content of a webpage.\"\\\\\\\\\\\\n        },\\\\\\\\\\\\n        {\\\\\\\\\\\\n          \"url\": \"https://www.firecrawl.dev/features\",\\\\\\\\\\\\n          \"markdown\": \"## Features\\\\\\\\\\\\n            Discover how Firecrawl\\\\\\'s cutting-edge features can \\\\\\\\\\\\n            transform your data operations.\"\\\\\\\\\\\\n        },\\\\\\\\\\\\n        {\\\\\\\\\\\\n          \"url\": \"https://www.firecrawl.dev/pricing\",\\\\\\\\\\\\n          \"markdown\": \"## Pricing Plans\\\\\\\\\\\\n            Choose the perfect plan that fits your needs.\"\\\\\\\\\\\\n        },\\\\\\\\\\\\n        {\\\\\\\\\\\\n          \"url\": \"https://www.firecrawl.dev/about\",\\\\\\\\\\\\n          \"markdown\": \"## About Us\\\\\\\\\\\\n            Learn more about Firecrawl\\\\\\'s mission and the \\\\\\\\\\\\n            team behind our innovative platform.\"\\\\\\\\\\\\n        }\\\\\\\\\\\\n      ]\\\\n      \\\\n\\\\nNote: The markdown has been edited for display purposes.\\\\n\\\\nTrusted by Top Companies\\\\n------------------------\\\\n\\\\n[![Customer Logo](https://www.firecrawl.dev/logos/zapier.png)](https://www.zapier.com)\\\\n\\\\n[![Customer Logo](https://www.firecrawl.dev/logos/gamma.svg)](https://gamma.app)\\\\n\\\\n[![Customer Logo](https://www.firecrawl.dev/logos/nvidia-com.png)](https://www.nvidia.com)\\\\n\\\\n[![Customer Logo](https://www.firecrawl.dev/logos/teller-io.svg)](https://www.teller.io)\\\\n\\\\n[![Customer Logo](https://www.firecrawl.dev/logos/stackai.svg)](https://www.stack-ai.com)\\\\n\\\\n[![Customer Logo](https://www.firecrawl.dev/logos/palladiumdigital-co-uk.svg)](https://www.palladiumdigital.co.uk)\\\\n\\\\n[![Customer Logo](https://www.firecrawl.dev/logos/worldwide-casting-com.svg)](https://www.worldwide-casting.com)\\\\n\\\\n[![Customer Logo](https://www.firecrawl.dev/logos/open-gov-sg.png)](https://www.open.gov.sg)\\\\n\\\\n[![Customer Logo](https://www.firecrawl.dev/logos/bain-com.svg)](https://www.bain.com)\\\\n\\\\n[![Customer Logo](https://www.firecrawl.dev/logos/demand-io.svg)](https://www.demand.io)\\\\n\\\\n[![Customer Logo](https://www.firecrawl.dev/logos/nocodegarden-io.png)](https://www.nocodegarden.io)\\\\n\\\\n[![Customer Logo](https://www.firecrawl.dev/logos/cyberagent-co-jp.svg)](https://www.cyberagent.co.jp)\\\\n\\\\nIntegrate today\\\\n---------------\\\\n\\\\nEnhance your applications with top-tier web scraping and crawling capabilities.\\\\n\\\\n#### Scrape\\\\n\\\\nExtract markdown or structured data from websites quickly and efficiently.\\\\n\\\\n#### Crawling\\\\n\\\\nNavigate and retrieve data from all accessible subpages, even without a sitemap.\\\\n\\\\nNode.js\\\\n\\\\nPython\\\\n\\\\ncURL\\\\n\\\\n1\\\\n\\\\n2\\\\n\\\\n3\\\\n\\\\n4\\\\n\\\\n5\\\\n\\\\n6\\\\n\\\\n7\\\\n\\\\n8\\\\n\\\\n9\\\\n\\\\n10\\\\n\\\\n// npm install @mendable/firecrawl-js  \\\\n  \\\\nimport FirecrawlApp from \\\\\\'@mendable/firecrawl-js\\\\\\';  \\\\n  \\\\nconst app \\\\\\\\= new FirecrawlApp({ apiKey: \"fc-YOUR\\\\\\\\_API\\\\\\\\_KEY\" });  \\\\n  \\\\n// Scrape a website:  \\\\nconst scrapeResult \\\\\\\\= await app.scrapeUrl(\\\\\\'firecrawl.dev\\\\\\');  \\\\n  \\\\nconsole.log(scrapeResult.data.markdown)\\\\n\\\\n#### Use well-known tools\\\\n\\\\nAlready fully integrated with the greatest existing tools and workflows.\\\\n\\\\n[![LlamaIndex](https://www.firecrawl.dev/logos/llamaindex.svg)](https://docs.llamaindex.ai/en/stable/examples/data_connectors/WebPageDemo/#using-firecrawl-reader/)\\\\n[![Langchain](https://www.firecrawl.dev/integrations/langchain.png)](https://python.langchain.com/docs/integrations/document_loaders/firecrawl/)\\\\n[![Dify](https://www.firecrawl.dev/logos/dify.png)](https://dify.ai/blog/dify-ai-blog-integrated-with-firecrawl/)\\\\n[![Dify](https://www.firecrawl.dev/integrations/langflow_2.png)](https://www.langflow.org/)\\\\n[![Flowise](https://www.firecrawl.dev/integrations/flowise.png)](https://flowiseai.com/)\\\\n[![CrewAI](https://www.firecrawl.dev/integrations/crewai.png)](https://crewai.com/)\\\\n\\\\n#### Start for free, scale easily\\\\n\\\\nKick off your journey for free and scale seamlessly as your project expands.\\\\n\\\\n[Try it out](/signin/signup)\\\\n\\\\n#### Open-source\\\\n\\\\nDeveloped transparently and collaboratively. Join our community of contributors.\\\\n\\\\n[Check out our repo](https://github.com/mendableai/firecrawl)\\\\n\\\\nWe handle the hard stuff\\\\n------------------------\\\\n\\\\nRotating proxies, caching, rate limits, js-blocked content and more\\\\n\\\\n#### Crawling\\\\n\\\\nFirecrawl crawls all accessible subpages, even without a sitemap.\\\\n\\\\n#### Dynamic content\\\\n\\\\nFirecrawl gathers data even if a website uses javascript to render content.\\\\n\\\\n#### To Markdown\\\\n\\\\nFirecrawl returns clean, well formatted markdown - ready for use in LLM applications\\\\n\\\\n#### Crawling Orchestration\\\\n\\\\nFirecrawl orchestrates the crawling process in parallel for the fastest results.\\\\n\\\\n#### Caching\\\\n\\\\nFirecrawl caches content, so you don\\\\\\'t have to wait for a full scrape unless new content exists.\\\\n\\\\n#### Built for AI\\\\n\\\\nBuilt by LLM engineers, for LLM engineers. Giving you clean data the way you want it.\\\\n\\\\nOur wall of love\\\\n\\\\nDon\\\\\\'t take our word for it\\\\n--------------------------\\\\n\\\\n![Greg Kamradt](https://www.firecrawl.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Ftestimonial-02.0afeb750.jpg&w=96&q=75)\\\\n\\\\nGreg Kamradt\\\\n\\\\n[@GregKamradt](https://twitter.com/GregKamradt/status/1780300642197840307)\\\\n\\\\nLLM structured data via API, handling requests, cleaning, and crawling. Enjoyed the early preview.\\\\n\\\\n![Amit Naik](https://www.firecrawl.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Ftestimonial-03.ff5dbe11.jpg&w=96&q=75)\\\\n\\\\nAmit Naik\\\\n\\\\n[@suprgeek](https://twitter.com/suprgeek/status/1780338213351035254)\\\\n\\\\n#llm success with RAG relies on Retrieval. Firecrawl by @mendableai structures web content for processing. 👏\\\\n\\\\n![Jerry Liu](https://www.firecrawl.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Ftestimonial-04.76bef0df.jpg&w=96&q=75)\\\\n\\\\nJerry Liu\\\\n\\\\n[@jerryjliu0](https://twitter.com/jerryjliu0/status/1781122933349572772)\\\\n\\\\nFirecrawl is awesome 🔥 Turns web pages into structured markdown for LLM apps, thanks to @mendableai.\\\\n\\\\n![Bardia Pourvakil](https://www.firecrawl.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Ftestimonial-01.025350bc.jpeg&w=96&q=75)\\\\n\\\\nBardia Pourvakil\\\\n\\\\n[@thepericulum](https://twitter.com/thepericulum/status/1781397799487078874)\\\\n\\\\nThese guys ship. I wanted types for their node SDK, and less than an hour later, I got them. Can\\\\\\'t recommend them enough.\\\\n\\\\n![latentsauce 🧘🏽](https://www.firecrawl.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Ftestimonial-07.c2285d35.jpeg&w=96&q=75)\\\\n\\\\nlatentsauce 🧘🏽\\\\n\\\\n[@latentsauce](https://twitter.com/latentsauce/status/1781738253927735331)\\\\n\\\\nFirecrawl simplifies data preparation significantly, exactly what I was hoping for. Thank you for creating Firecrawl ❤️❤️❤️\\\\n\\\\n![Greg Kamradt](https://www.firecrawl.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Ftestimonial-02.0afeb750.jpg&w=96&q=75)\\\\n\\\\nGreg Kamradt\\\\n\\\\n[@GregKamradt](https://twitter.com/GregKamradt/status/1780300642197840307)\\\\n\\\\nLLM structured data via API, handling requests, cleaning, and crawling. Enjoyed the early preview.\\\\n\\\\n![Amit Naik](https://www.firecrawl.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Ftestimonial-03.ff5dbe11.jpg&w=96&q=75)\\\\n\\\\nAmit Naik\\\\n\\\\n[@suprgeek](https://twitter.com/suprgeek/status/1780338213351035254)\\\\n\\\\n#llm success with RAG relies on Retrieval. Firecrawl by @mendableai structures web content for processing. 👏\\\\n\\\\n![Jerry Liu](https://www.firecrawl.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Ftestimonial-04.76bef0df.jpg&w=96&q=75)\\\\n\\\\nJerry Liu\\\\n\\\\n[@jerryjliu0](https://twitter.com/jerryjliu0/status/1781122933349572772)\\\\n\\\\nFirecrawl is awesome 🔥 Turns web pages into structured markdown for LLM apps, thanks to @mendableai.\\\\n\\\\n![Bardia Pourvakil](https://www.firecrawl.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Ftestimonial-01.025350bc.jpeg&w=96&q=75)\\\\n\\\\nBardia Pourvakil\\\\n\\\\n[@thepericulum](https://twitter.com/thepericulum/status/1781397799487078874)\\\\n\\\\nThese guys ship. I wanted types for their node SDK, and less than an hour later, I got them. Can\\\\\\'t recommend them enough.\\\\n\\\\n![latentsauce 🧘🏽](https://www.firecrawl.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Ftestimonial-07.c2285d35.jpeg&w=96&q=75)\\\\n\\\\nlatentsauce 🧘🏽\\\\n\\\\n[@latentsauce](https://twitter.com/latentsauce/status/1781738253927735331)\\\\n\\\\nFirecrawl simplifies data preparation significantly, exactly what I was hoping for. Thank you for creating Firecrawl ❤️❤️❤️\\\\n\\\\n![Michael Ning](https://www.firecrawl.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Ftestimonial-05.76d7cd3e.png&w=96&q=75)\\\\n\\\\nMichael Ning\\\\n\\\\n[](#)\\\\n\\\\nFirecrawl is impressive, saving us 2/3 the tokens and allowing gpt3.5turbo use over gpt4. Major savings in time and money.\\\\n\\\\n![Alex Reibman 🖇️](https://www.firecrawl.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Ftestimonial-06.4ee7cf5a.jpeg&w=96&q=75)\\\\n\\\\nAlex Reibman 🖇️\\\\n\\\\n[@AlexReibman](https://twitter.com/AlexReibman/status/1780299595484131836)\\\\n\\\\nMoved our internal agent\\\\\\'s web scraping tool from Apify to Firecrawl because it benchmarked 50x faster with AgentOps.\\\\n\\\\n![Michael](https://www.firecrawl.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Ftestimonial-08.0bed40be.jpeg&w=96&q=75)\\\\n\\\\nMichael\\\\n\\\\n[@michael\\\\\\\\_chomsky](#)\\\\n\\\\nI really like some of the design decisions Firecrawl made, so I really want to share with others.\\\\n\\\\n![Paul Scott](https://www.firecrawl.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Ftestimonial-09.d303b5b4.png&w=96&q=75)\\\\n\\\\nPaul Scott\\\\n\\\\n[@palebluepaul](https://twitter.com/palebluepaul)\\\\n\\\\nAppreciating your lean approach, Firecrawl ticks off everything on our list without the cost prohibitive overkill.\\\\n\\\\n![Michael Ning](https://www.firecrawl.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Ftestimonial-05.76d7cd3e.png&w=96&q=75)\\\\n\\\\nMichael Ning\\\\n\\\\n[](#)\\\\n\\\\nFirecrawl is impressive, saving us 2/3 the tokens and allowing gpt3.5turbo use over gpt4. Major savings in time and money.\\\\n\\\\n![Alex Reibman 🖇️](https://www.firecrawl.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Ftestimonial-06.4ee7cf5a.jpeg&w=96&q=75)\\\\n\\\\nAlex Reibman 🖇️\\\\n\\\\n[@AlexReibman](https://twitter.com/AlexReibman/status/1780299595484131836)\\\\n\\\\nMoved our internal agent\\\\\\'s web scraping tool from Apify to Firecrawl because it benchmarked 50x faster with AgentOps.\\\\n\\\\n![Michael](https://www.firecrawl.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Ftestimonial-08.0bed40be.jpeg&w=96&q=75)\\\\n\\\\nMichael\\\\n\\\\n[@michael\\\\\\\\_chomsky](#)\\\\n\\\\nI really like some of the design decisions Firecrawl made, so I really want to share with others.\\\\n\\\\n![Paul Scott](https://www.firecrawl.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Ftestimonial-09.d303b5b4.png&w=96&q=75)\\\\n\\\\nPaul Scott\\\\n\\\\n[@palebluepaul](https://twitter.com/palebluepaul)\\\\n\\\\nAppreciating your lean approach, Firecrawl ticks off everything on our list without the cost prohibitive overkill.\\\\n\\\\nFlexible Pricing\\\\n----------------\\\\n\\\\nStart for free, then scale as you grow\\\\n\\\\nYearly (17% off)Yearly (2 months free)Monthly\\\\n\\\\nFree Plan\\\\n---------\\\\n\\\\n500 credits\\\\n\\\\n$0/month\\\\n\\\\n*   Scrape 500 pages\\\\n*   5 /scrape per min\\\\n*   1 /crawl per min\\\\n\\\\nGet Started\\\\n\\\\nHobby\\\\n-----\\\\n\\\\n3,000 credits\\\\n\\\\n$16/month\\\\n\\\\n*   Scrape 3,000 pages\\\\n*   10 /scrape per min\\\\n*   3 /crawl per min\\\\n\\\\nSubscribe\\\\n\\\\nStandardMost Popular\\\\n--------------------\\\\n\\\\n100,000 credits\\\\n\\\\n$83/month\\\\n\\\\n*   Scrape 100,000 pages\\\\n*   50 /scrape per min\\\\n*   10 /crawl per min\\\\n\\\\nSubscribe\\\\n\\\\nGrowth\\\\n------\\\\n\\\\n500,000 credits\\\\n\\\\n$333/month\\\\n\\\\n*   Scrape 500,000 pages\\\\n*   500 /scrape per min\\\\n*   50 /crawl per min\\\\n*   Priority Support\\\\n\\\\nSubscribe\\\\n\\\\nEnterprise Plan\\\\n---------------\\\\n\\\\nUnlimited credits. Custom RPMs.\\\\n\\\\nTalk to us\\\\n\\\\n*   Top priority support\\\\n*   Feature Acceleration\\\\n*   SLAs\\\\n*   Account Manager\\\\n*   Custom rate limits volume\\\\n*   Custom concurrency limits\\\\n*   Beta features access\\\\n*   CEO\\\\\\'s number\\\\n\\\\n\\\\\\\\* a /scrape refers to the [scrape](https://docs.firecrawl.dev/api-reference/endpoint/scrape)\\\\n API endpoint.\\\\n\\\\n\\\\\\\\* a /crawl refers to the [crawl](https://docs.firecrawl.dev/api-reference/endpoint/crawl)\\\\n API endpoint.\\\\n\\\\nScrape Credits\\\\n--------------\\\\n\\\\nScrape credits are consumed for each API request, varying by endpoint and feature.\\\\n\\\\n| Features | Credits per page |\\\\n| --- | --- |\\\\n| Scrape(/scrape) | 1   |\\\\n| Crawl(/crawl) | 1   |\\\\n| Search(/search) | 1   |\\\\n| Scrape + LLM extraction (/scrape) | 50  |\\\\n\\\\n[🔥](/)\\\\n\\\\nReady to _Build?_\\\\n-----------------\\\\n\\\\nStart scraping web data for your AI apps today.  \\\\nNo credit card needed.\\\\n\\\\n[Get Started](/signin)\\\\n\\\\n[Talk to us](https://calendly.com/d/cj83-ngq-knk/meet-firecrawl)\\\\n\\\\nFAQ\\\\n---\\\\n\\\\nFrequently asked questions about Firecrawl\\\\n\\\\n#### General\\\\n\\\\nWhat is Firecrawl?\\\\n\\\\nFirecrawl turns entire websites into clean, LLM-ready markdown or structured data. Scrape, crawl and extract the web with a single API. Ideal for AI companies looking to empower their LLM applications with web data.\\\\n\\\\nWhat sites work?\\\\n\\\\nFirecrawl is best suited for business websites, docs and help centers. We currently don\\\\\\'t support social media platforms.\\\\n\\\\nWho can benefit from using Firecrawl?\\\\n\\\\nFirecrawl is tailored for LLM engineers, data scientists, AI researchers, and developers looking to harness web data for training machine learning models, market research, content aggregation, and more. It simplifies the data preparation process, allowing professionals to focus on insights and model development.\\\\n\\\\nIs Firecrawl open-source?\\\\n\\\\nYes, it is. You can check out the repository on GitHub. Keep in mind that this repository is currently in its early stages of development. We are in the process of merging custom modules into this mono repository.\\\\n\\\\n#### Scraping & Crawling\\\\n\\\\nHow does Firecrawl handle dynamic content on websites?\\\\n\\\\nUnlike traditional web scrapers, Firecrawl is equipped to handle dynamic content rendered with JavaScript. It ensures comprehensive data collection from all accessible subpages, making it a reliable tool for scraping websites that rely heavily on JS for content delivery.\\\\n\\\\nWhy is it not crawling all the pages?\\\\n\\\\nThere are a few reasons why Firecrawl may not be able to crawl all the pages of a website. Some common reasons include rate limiting, and anti-scraping mechanisms, disallowing the crawler from accessing certain pages. If you\\\\\\'re experiencing issues with the crawler, please reach out to our support team at hello@firecrawl.com.\\\\n\\\\nCan Firecrawl crawl websites without a sitemap?\\\\n\\\\nYes, Firecrawl can access and crawl all accessible subpages of a website, even in the absence of a sitemap. This feature enables users to gather data from a wide array of web sources with minimal setup.\\\\n\\\\nWhat formats can Firecrawl convert web data into?\\\\n\\\\nFirecrawl specializes in converting web data into clean, well-formatted markdown. This format is particularly suited for LLM applications, offering a structured yet flexible way to represent web content.\\\\n\\\\nHow does Firecrawl ensure the cleanliness of the data?\\\\n\\\\nFirecrawl employs advanced algorithms to clean and structure the scraped data, removing unnecessary elements and formatting the content into readable markdown. This process ensures that the data is ready for use in LLM applications without further preprocessing.\\\\n\\\\nIs Firecrawl suitable for large-scale data scraping projects?\\\\n\\\\nAbsolutely. Firecrawl offers various pricing plans, including a Scale plan that supports scraping of millions of pages. With features like caching and scheduled syncs, it\\\\\\'s designed to efficiently handle large-scale data scraping and continuous updates, making it ideal for enterprises and large projects.\\\\n\\\\nDoes it respect robots.txt?\\\\n\\\\nYes, Firecrawl crawler respects the rules set in a website\\\\\\'s robots.txt file. If you notice any issues with the way Firecrawl interacts with your website, you can adjust the robots.txt file to control the crawler\\\\\\'s behavior. Firecrawl user agent name is \\\\\\'FirecrawlAgent\\\\\\'. If you notice any behavior that is not expected, please let us know at hello@firecrawl.com.\\\\n\\\\nWhat measures does Firecrawl take to handle web scraping challenges like rate limits and caching?\\\\n\\\\nFirecrawl is built to navigate common web scraping challenges, including reverse proxies, rate limits, and caching. It smartly manages requests and employs caching techniques to minimize bandwidth usage and avoid triggering anti-scraping mechanisms, ensuring reliable data collection.\\\\n\\\\nDoes Firecrawl handle captcha or authentication?\\\\n\\\\nFirecrawl avoids captcha by using stealth proxyies. When it encounters captcha, it attempts to solve it automatically, but this is not always possible. We are working to add support for more captcha solving methods. Firecrawl can handle authentication by providing auth headers to the API.\\\\n\\\\n#### API Related\\\\n\\\\nWhere can I find my API key?\\\\n\\\\nClick on the dashboard button on the top navigation menu when logged in and you will find your API key in the main screen and under API Keys.\\\\n\\\\n#### Billing\\\\n\\\\nIs Firecrawl free?\\\\n\\\\nFirecrawl is free for the first 500 scraped pages (500 free credits). After that, you can upgrade to our Standard or Scale plans for more credits.\\\\n\\\\nIs there a pay per use plan instead of monthly?\\\\n\\\\nNo we do not currently offer a pay per use plan, instead you can upgrade to our Standard or Growth plans for more credits and higher rate limits.\\\\n\\\\nHow many credit does scraping, crawling, and extraction cost?\\\\n\\\\nScraping costs 1 credit per page. Crawling costs 1 credit per page.\\\\n\\\\nDo you charge for failed requests (scrape, crawl, extract)?\\\\n\\\\nWe do not charge for any failed requests (scrape, crawl, extract). Please contact support at help@firecrawl.dev if you have any questions.\\\\n\\\\nWhat payment methods do you accept?\\\\n\\\\nWe accept payments through Stripe which accepts most major credit cards, debit cards, and PayPal.\\\\n\\\\n[🔥](/)\\\\n\\\\n© A product by Mendable.ai - All rights reserved.\\\\n\\\\n[StatusStatus](https://firecrawl.betteruptime.com)\\\\n[Terms of ServiceTerms of Service](/terms-of-service)\\\\n[Privacy PolicyPrivacy Policy](/privacy-policy)\\\\n\\\\n[Twitter](https://twitter.com/mendableai)\\\\n[GitHub](https://github.com/mendableai)\\\\n[Discord](https://discord.gg/gSmWdAkdwd)\\\\n\\\\n###### Helpful Links\\\\n\\\\n*   [Status](https://firecrawl.betteruptime.com/)\\\\n    \\\\n*   [Pricing](/pricing)\\\\n    \\\\n*   [Blog](https://www.firecrawl.dev/blog)\\\\n    \\\\n*   [Docs](https://docs.firecrawl.dev)\\\\n    \\\\n\\\\nBacked by![Y Combinator Logo](https://www.firecrawl.dev/images/yc.svg)\\\\n\\\\n![SOC 2 Type II](https://www.firecrawl.dev/soc2type2badge.png)\\\\n\\\\n###### Resources\\\\n\\\\n*   [Community](#0)\\\\n    \\\\n*   [Terms of service](#0)\\\\n    \\\\n*   [Collaboration features](#0)\\\\n    \\\\n\\\\n###### Legals\\\\n\\\\n*   [Refund policy](#0)\\\\n    \\\\n*   [Terms & Conditions](#0)\\\\n    \\\\n*   [Privacy policy](#0)\\\\n    \\\\n*   [Brand Kit](#0)\\')\\n\\n```\\n\\n```codeBlockLines_e6Vv\\nprint(docs[0].metadata)\\n\\n```\\n\\n```codeBlockLines_e6Vv\\n{\\'ogUrl\\': \\'https://www.firecrawl.dev/\\', \\'title\\': \\'Home - Firecrawl\\', \\'robots\\': \\'follow, index\\', \\'ogImage\\': \\'https://www.firecrawl.dev/og.png?123\\', \\'ogTitle\\': \\'Firecrawl\\', \\'sitemap\\': {\\'lastmod\\': \\'2024-08-12T00:28:16.681Z\\', \\'changefreq\\': \\'weekly\\'}, \\'keywords\\': \\'Firecrawl,Markdown,Data,Mendable,Langchain\\', \\'sourceURL\\': \\'https://www.firecrawl.dev/\\', \\'ogSiteName\\': \\'Firecrawl\\', \\'description\\': \\'Firecrawl crawls and converts any website into clean markdown.\\', \\'ogDescription\\': \\'Turn any website into LLM-ready data.\\', \\'pageStatusCode\\': 200, \\'ogLocaleAlternate\\': []}\\n\\n```\\n\\n## Lazy Load [\\u200b](\\\\#lazy-load \"Direct link to Lazy Load\")\\n\\nYou can use lazy loading to minimize memory requirements.\\n\\n```codeBlockLines_e6Vv\\npages = []\\nfor doc in loader.lazy_load():\\n    pages.append(doc)\\n    if len(pages) >= 10:\\n        # do some paged operation, e.g.\\n        # index.upsert(page)\\n\\n        pages = []\\n\\n```\\n\\n```codeBlockLines_e6Vv\\nlen(pages)\\n\\n```\\n\\n```codeBlockLines_e6Vv\\n8\\n\\n```\\n\\n```codeBlockLines_e6Vv\\nprint(pages[0].page_content[:100])\\nprint(pages[0].metadata)\\n\\n```\\n\\n```codeBlockLines_e6Vv\\nIntroducing [Smart Crawl!](https://www.firecrawl.dev/smart-crawl)\\n Join the waitlist to turn any web\\n{\\'ogUrl\\': \\'https://www.firecrawl.dev/blog/introducing-fire-engine-for-firecrawl\\', \\'title\\': \\'Introducing Fire Engine for Firecrawl\\', \\'robots\\': \\'follow, index\\', \\'ogImage\\': \\'https://www.firecrawl.dev/images/blog/fire-engine-launch.png\\', \\'ogTitle\\': \\'Introducing Fire Engine for Firecrawl\\', \\'sitemap\\': {\\'lastmod\\': \\'2024-08-06T00:00:00.000Z\\', \\'changefreq\\': \\'weekly\\'}, \\'keywords\\': \\'firecrawl,fireengine,web crawling,dashboard,web scraping,LLM,data extraction\\', \\'sourceURL\\': \\'https://www.firecrawl.dev/blog/introducing-fire-engine-for-firecrawl\\', \\'ogSiteName\\': \\'Firecrawl\\', \\'description\\': \\'The most scalable, reliable, and fast way to get web data for Firecrawl.\\', \\'ogDescription\\': \\'The most scalable, reliable, and fast way to get web data for Firecrawl.\\', \\'pageStatusCode\\': 200, \\'ogLocaleAlternate\\': []}\\n\\n```\\n\\n## Crawler Options [\\u200b](\\\\#crawler-options \"Direct link to Crawler Options\")\\n\\nYou can also pass `params` to the loader. This is a dictionary of options to pass to the crawler. See the [FireCrawl API documentation](https://github.com/mendableai/firecrawl-py) for more information.\\n\\n## API reference [\\u200b](\\\\#api-reference \"Direct link to API reference\")\\n\\nFor detailed documentation of all `FireCrawlLoader` features and configurations head to the API reference: [https://python.langchain.com/api\\\\_reference/community/document\\\\_loaders/langchain\\\\_community.document\\\\_loaders.firecrawl.FireCrawlLoader.html](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.firecrawl.FireCrawlLoader.html)\\n\\n## Related [\\u200b](\\\\#related \"Direct link to Related\")\\n\\n- Document loader [conceptual guide](/docs/concepts/#document-loaders)\\n- Document loader [how-to guides](/docs/how_to/#document-loaders)\\n\\n[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/integrations/document_loaders/firecrawl.ipynb)\\n\\n* * *\\n\\n#### Was this page helpful?\\n\\n#### You can also leave detailed feedback [on GitHub](https://github.com/langchain-ai/langchain/issues/new?assignees=&labels=03+-+Documentation&projects=&template=documentation.yml&title=DOC%3A+%3CIssue+related+to+/docs/integrations/document_loaders/firecrawl/%3E&url=https://python.langchain.com/docs/integrations/document_loaders/firecrawl/).\\n\\n[Previous\\\\\\\\\\n\\\\\\\\\\nFigma](/docs/integrations/document_loaders/figma/) [Next\\\\\\\\\\n\\\\\\\\\\nGeopandas](/docs/integrations/document_loaders/geopandas/)\\n\\n- [Overview](#overview)\\n  - [Integration details](#integration-details)\\n  - [Loader features](#loader-features)\\n- [Setup](#setup)\\n  - [Credentials](#credentials)\\n  - [Installation](#installation)\\n- [Initialization](#initialization)\\n  - [Modes](#modes)\\n- [Load](#load)\\n- [Lazy Load](#lazy-load)\\n- [Crawler Options](#crawler-options)\\n- [API reference](#api-reference)\\n- [Related](#related)\\n\\nCommunity\\n\\n- [Twitter](https://twitter.com/LangChainAI)\\n\\nGitHub\\n\\n- [Organization](https://github.com/langchain-ai)\\n- [Python](https://github.com/langchain-ai/langchain)\\n- [JS/TS](https://github.com/langchain-ai/langchainjs)\\n\\nMore\\n\\n- [Homepage](https://langchain.com)\\n- [Blog](https://blog.langchain.dev)\\n- [YouTube](https://www.youtube.com/@LangChain)\\n\\nCopyright © 2024 LangChain, Inc.')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import FireCrawlLoader\n",
    "api_key = \"fc-6bc21951e5eb4c8db24ccc90728855e6\"\n",
    "loader = FireCrawlLoader(url=\"https://python.langchain.com/docs/integrations/document_loaders/firecrawl/\", mode = \"crawl\", api_key=api_key)\n",
    "\n",
    "docs = loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Skip to main content](#__docusaurus_skipToContent_fallback)\n",
      "\n",
      "Share your thoughts on AI agents. [Take the 3-min survey](https://langchain.typeform.com/state-of-agents).\n",
      "\n",
      "[![🦜️🔗 LangChain](https://python.langchain.com/img/brand/wordmark.png)](/)[Integrations](/docs/integrations/platforms/) [API Reference](https://python.langchain.com/api_reference/)\n",
      "\n",
      "More\n",
      "\n",
      "- [Contributing](/docs/contributing/)\n",
      "- [People](/docs/people/)\n",
      "- * * *\n",
      "\n",
      "- [LangSmith](https://docs.smith.langchain.com)\n",
      "- [LangGraph](https://langchain-ai.github.io/langgraph/)\n",
      "- [LangChain Hub](https://smith.langchain.com/hub)\n",
      "- [LangChain JS/TS](https://js.langchain.com)\n",
      "\n",
      "v0.3\n",
      "\n",
      "- [v0.3](/docs/introduction/)\n",
      "- [v0.2](https://python.langchain.com/v0.2/docs/introduction)\n",
      "- [v0.1](https://python.langchain.com/v0.1/docs/get_started/introduction)\n",
      "\n",
      "[💬](https://chat.langchain.com)  [GitHub repository](https://github.com/langchain-ai/langchain)\n",
      "\n",
      "Search`` `K`\n",
      "\n",
      "- [Providers](/docs/integrations/platforms/)\n",
      "\n",
      "  - [Providers](/docs/integrations/platforms/)\n",
      "  - [Anthropic](/docs/integrations/platforms/anthropic/)\n",
      "  - [AWS](/docs/integrations/platforms/aws/)\n",
      "  - [Google](/docs/integrations/platforms/google/)\n",
      "  - [Hugging Face](/docs/integrations/platforms/huggingface/)\n",
      "  - [Microsoft](/docs/integrations/platforms/microsoft/)\n",
      "  - [OpenAI](/docs/integrations/platforms/openai/)\n",
      "  - [More](/docs/integrations/providers/)\n",
      "- [Components](/docs/integrations/components/)\n",
      "\n",
      "  - [Chat models](/docs/integrations/chat/)\n",
      "\n",
      "    - [Chat models](/docs/integrations/chat/)\n",
      "    - [AI21 Labs](/docs/integrations/chat/ai21/)\n",
      "    - [Alibaba Cloud PAI EAS](/docs/integrations/chat/alibaba_cloud_pai_eas/)\n",
      "    - [Anthropic](/docs/integrations/chat/anthropic/)\n",
      "    - [\\[Deprecated\\] Experimental Anthropic Tools Wrapper](/docs/integrations/chat/anthropic_functions/)\n",
      "    - [Anyscale](/docs/integrations/chat/anyscale/)\n",
      "    - [Azure OpenAI](/docs/integrations/chat/azure_chat_openai/)\n",
      "    - [Azure ML Endpoint](/docs/integrations/chat/azureml_chat_endpoint/)\n",
      "    - [Baichuan Chat](/docs/integrations/chat/baichuan/)\n",
      "    - [Baidu Qianfan](/docs/integrations/chat/baidu_qianfan_endpoint/)\n",
      "    - [AWS Bedrock](/docs/integrations/chat/bedrock/)\n",
      "    - [Cerebras](/docs/integrations/chat/cerebras/)\n",
      "    - [Cohere](/docs/integrations/chat/cohere/)\n",
      "    - [Coze Chat](/docs/integrations/chat/coze/)\n",
      "    - [Dappier AI](/docs/integrations/chat/dappier/)\n",
      "    - [Databricks](/docs/integrations/chat/databricks/)\n",
      "    - [DeepInfra](/docs/integrations/chat/deepinfra/)\n",
      "    - [Eden AI](/docs/integrations/chat/edenai/)\n",
      "    - [Ernie Bot Chat](/docs/integrations/chat/ernie/)\n",
      "    - [EverlyAI](/docs/integrations/chat/everlyai/)\n",
      "    - [Fireworks](/docs/integrations/chat/fireworks/)\n",
      "    - [Friendli](/docs/integrations/chat/friendli/)\n",
      "    - [GigaChat](/docs/integrations/chat/gigachat/)\n",
      "    - [Google AI](/docs/integrations/chat/google_generative_ai/)\n",
      "    - [Google Cloud Vertex AI](/docs/integrations/chat/google_vertex_ai_palm/)\n",
      "    - [GPTRouter](/docs/integrations/chat/gpt_router/)\n",
      "    - [Groq](/docs/integrations/chat/groq/)\n",
      "    - [ChatHuggingFace](/docs/integrations/chat/huggingface/)\n",
      "    - [IBM watsonx.ai](/docs/integrations/chat/ibm_watsonx/)\n",
      "    - [JinaChat](/docs/integrations/chat/jinachat/)\n",
      "    - [Kinetica](/docs/integrations/chat/kinetica/)\n",
      "    - [Konko](/docs/integrations/chat/konko/)\n",
      "    - [LiteLLM](/docs/integrations/chat/litellm/)\n",
      "    - [LiteLLM Router](/docs/integrations/chat/litellm_router/)\n",
      "    - [Llama 2 Chat](/docs/integrations/chat/llama2_chat/)\n",
      "    - [Llama API](/docs/integrations/chat/llama_api/)\n",
      "    - [LlamaEdge](/docs/integrations/chat/llama_edge/)\n",
      "    - [Llama.cpp](/docs/integrations/chat/llamacpp/)\n",
      "    - [maritalk](/docs/integrations/chat/maritalk/)\n",
      "    - [MiniMax](/docs/integrations/chat/minimax/)\n",
      "    - [MistralAI](/docs/integrations/chat/mistralai/)\n",
      "    - [MLX](/docs/integrations/chat/mlx/)\n",
      "    - [Moonshot](/docs/integrations/chat/moonshot/)\n",
      "    - [NVIDIA AI Endpoints](/docs/integrations/chat/nvidia_ai_endpoints/)\n",
      "    - [OCIGenAI](/docs/integrations/chat/oci_generative_ai/)\n",
      "    - [ChatOctoAI](/docs/integrations/chat/octoai/)\n",
      "    - [Ollama](/docs/integrations/chat/ollama/)\n",
      "    - [OpenAI](/docs/integrations/chat/openai/)\n",
      "    - [Perplexity](/docs/integrations/chat/perplexity/)\n",
      "    - [PremAI](/docs/integrations/chat/premai/)\n",
      "    - [PromptLayer ChatOpenAI](/docs/integrations/chat/promptlayer_chatopenai/)\n",
      "    - [Snowflake Cortex](/docs/integrations/chat/snowflake/)\n",
      "    - [solar](/docs/integrations/chat/solar/)\n",
      "    - [SparkLLM Chat](/docs/integrations/chat/sparkllm/)\n",
      "    - [Nebula (Symbl.ai)](/docs/integrations/chat/symblai_nebula/)\n",
      "    - [Tencent Hunyuan](/docs/integrations/chat/tencent_hunyuan/)\n",
      "    - [Together](/docs/integrations/chat/together/)\n",
      "    - [Tongyi Qwen](/docs/integrations/chat/tongyi/)\n",
      "    - [Upstage](/docs/integrations/chat/upstage/)\n",
      "    - [vLLM Chat](/docs/integrations/chat/vllm/)\n",
      "    - [Volc Enging Maas](/docs/integrations/chat/volcengine_maas/)\n",
      "    - [YandexGPT](/docs/integrations/chat/yandex/)\n",
      "    - [ChatYI](/docs/integrations/chat/yi/)\n",
      "    - [Yuan2.0](/docs/integrations/chat/yuan2/)\n",
      "    - [ZHIPU AI](/docs/integrations/chat/zhipuai/)\n",
      "  - [LLMs](/docs/integrations/llms/)\n",
      "\n",
      "    - [LLMs](/docs/integrations/llms/)\n",
      "    - [AI21 Labs](/docs/integrations/llms/ai21/)\n",
      "    - [Aleph Alpha](/docs/integrations/llms/aleph_alpha/)\n",
      "    - [Alibaba Cloud PAI EAS](/docs/integrations/llms/alibabacloud_pai_eas_endpoint/)\n",
      "    - [Amazon API Gateway](/docs/integrations/llms/amazon_api_gateway/)\n",
      "    - [Anthropic](/docs/integrations/llms/anthropic/)\n",
      "    - [Anyscale](/docs/integrations/llms/anyscale/)\n",
      "    - [Aphrodite Engine](/docs/integrations/llms/aphrodite/)\n",
      "    - [Arcee](/docs/integrations/llms/arcee/)\n",
      "    - [Azure ML](/docs/integrations/llms/azure_ml/)\n",
      "    - [Azure OpenAI](/docs/integrations/llms/azure_openai/)\n",
      "    - [Baichuan LLM](/docs/integrations/llms/baichuan/)\n",
      "    - [Baidu Qianfan](/docs/integrations/llms/baidu_qianfan_endpoint/)\n",
      "    - [Banana](/docs/integrations/llms/banana/)\n",
      "    - [Baseten](/docs/integrations/llms/baseten/)\n",
      "    - [Beam](/docs/integrations/llms/beam/)\n",
      "    - [Bedrock](/docs/integrations/llms/bedrock/)\n",
      "    - [Bittensor](/docs/integrations/llms/bittensor/)\n",
      "    - [CerebriumAI](/docs/integrations/llms/cerebriumai/)\n",
      "    - [ChatGLM](/docs/integrations/llms/chatglm/)\n",
      "    - [Clarifai](/docs/integrations/llms/clarifai/)\n",
      "    - [Cloudflare Workers AI](/docs/integrations/llms/cloudflare_workersai/)\n",
      "    - [Cohere](/docs/integrations/llms/cohere/)\n",
      "    - [C Transformers](/docs/integrations/llms/ctransformers/)\n",
      "    - [CTranslate2](/docs/integrations/llms/ctranslate2/)\n",
      "    - [Databricks](/docs/integrations/llms/databricks/)\n",
      "    - [DeepInfra](/docs/integrations/llms/deepinfra/)\n",
      "    - [DeepSparse](/docs/integrations/llms/deepsparse/)\n",
      "    - [Eden AI](/docs/integrations/llms/edenai/)\n",
      "    - [ExLlamaV2](/docs/integrations/llms/exllamav2/)\n",
      "    - [Fireworks](/docs/integrations/llms/fireworks/)\n",
      "    - [ForefrontAI](/docs/integrations/llms/forefrontai/)\n",
      "    - [Friendli](/docs/integrations/llms/friendli/)\n",
      "    - [GigaChat](/docs/integrations/llms/gigachat/)\n",
      "    - [Google AI](/docs/integrations/llms/google_ai/)\n",
      "    - [Google Cloud Vertex AI](/docs/integrations/llms/google_vertex_ai_palm/)\n",
      "    - [GooseAI](/docs/integrations/llms/gooseai/)\n",
      "    - [GPT4All](/docs/integrations/llms/gpt4all/)\n",
      "    - [Gradient](/docs/integrations/llms/gradient/)\n",
      "    - [Huggingface Endpoints](/docs/integrations/llms/huggingface_endpoint/)\n",
      "    - [Hugging Face Local Pipelines](/docs/integrations/llms/huggingface_pipelines/)\n",
      "    - [IBM watsonx.ai](/docs/integrations/llms/ibm_watsonx/)\n",
      "    - [IPEX-LLM](/docs/integrations/llms/ipex_llm/)\n",
      "    - [Javelin AI Gateway Tutorial](/docs/integrations/llms/javelin/)\n",
      "    - [JSONFormer](/docs/integrations/llms/jsonformer_experimental/)\n",
      "    - [KoboldAI API](/docs/integrations/llms/koboldai/)\n",
      "    - [Konko](/docs/integrations/llms/konko/)\n",
      "    - [Layerup Security](/docs/integrations/llms/layerup_security/)\n",
      "    - [Llama.cpp](/docs/integrations/llms/llamacpp/)\n",
      "    - [Llamafile](/docs/integrations/llms/llamafile/)\n",
      "    - [LM Format Enforcer](/docs/integrations/llms/lmformatenforcer_experimental/)\n",
      "    - [Manifest](/docs/integrations/llms/manifest/)\n",
      "    - [Minimax](/docs/integrations/llms/minimax/)\n",
      "    - [MLX Local Pipelines](/docs/integrations/llms/mlx_pipelines/)\n",
      "    - [Modal](/docs/integrations/llms/modal/)\n",
      "    - [MoonshotChat](/docs/integrations/llms/moonshot/)\n",
      "    - [MosaicML](/docs/integrations/llms/mosaicml/)\n",
      "    - [NLP Cloud](/docs/integrations/llms/nlpcloud/)\n",
      "    - [oci\\_generative\\_ai](/docs/integrations/llms/oci_generative_ai/)\n",
      "    - [OCI Data Science Model Deployment Endpoint](/docs/integrations/llms/oci_model_deployment_endpoint/)\n",
      "    - [OctoAI](/docs/integrations/llms/octoai/)\n",
      "    - [Ollama](/docs/integrations/llms/ollama/)\n",
      "    - [OpaquePrompts](/docs/integrations/llms/opaqueprompts/)\n",
      "    - [OpenAI](/docs/integrations/llms/openai/)\n",
      "    - [OpenLLM](/docs/integrations/llms/openllm/)\n",
      "    - [OpenLM](/docs/integrations/llms/openlm/)\n",
      "    - [OpenVINO](/docs/integrations/llms/openvino/)\n",
      "    - [Petals](/docs/integrations/llms/petals/)\n",
      "    - [PipelineAI](/docs/integrations/llms/pipelineai/)\n",
      "    - [Predibase](/docs/integrations/llms/predibase/)\n",
      "    - [Prediction Guard](/docs/integrations/llms/predictionguard/)\n",
      "    - [PromptLayer OpenAI](/docs/integrations/llms/promptlayer_openai/)\n",
      "    - [RELLM](/docs/integrations/llms/rellm_experimental/)\n",
      "    - [Replicate](/docs/integrations/llms/replicate/)\n",
      "    - [Runhouse](/docs/integrations/llms/runhouse/)\n",
      "    - [SageMakerEndpoint](/docs/integrations/llms/sagemaker/)\n",
      "    - [SambaNova](/docs/integrations/llms/sambanova/)\n",
      "    - [Solar](/docs/integrations/llms/solar/)\n",
      "    - [SparkLLM](/docs/integrations/llms/sparkllm/)\n",
      "    - [StochasticAI](/docs/integrations/llms/stochasticai/)\n",
      "    - [Nebula (Symbl.ai)](/docs/integrations/llms/symblai_nebula/)\n",
      "    - [TextGen](/docs/integrations/llms/textgen/)\n",
      "    - [Titan Takeoff](/docs/integrations/llms/titan_takeoff/)\n",
      "    - [Together AI](/docs/integrations/llms/together/)\n",
      "    - [Tongyi Qwen](/docs/integrations/llms/tongyi/)\n",
      "    - [vLLM](/docs/integrations/llms/vllm/)\n",
      "    - [Volc Engine Maas](/docs/integrations/llms/volcengine_maas/)\n",
      "    - [Intel Weight-Only Quantization](/docs/integrations/llms/weight_only_quantization/)\n",
      "    - [Writer](/docs/integrations/llms/writer/)\n",
      "    - [Xorbits Inference (Xinference)](/docs/integrations/llms/xinference/)\n",
      "    - [YandexGPT](/docs/integrations/llms/yandex/)\n",
      "    - [Yi](/docs/integrations/llms/yi/)\n",
      "    - [Yuan2.0](/docs/integrations/llms/yuan2/)\n",
      "  - [Embedding models](/docs/integrations/text_embedding/)\n",
      "\n",
      "    - [Embedding models](/docs/integrations/text_embedding/)\n",
      "    - [AI21](/docs/integrations/text_embedding/ai21/)\n",
      "    - [Aleph Alpha](/docs/integrations/text_embedding/aleph_alpha/)\n",
      "    - [Anyscale](/docs/integrations/text_embedding/anyscale/)\n",
      "    - [ascend](/docs/integrations/text_embedding/ascend/)\n",
      "    - [AwaDB](/docs/integrations/text_embedding/awadb/)\n",
      "    - [AzureOpenAI](/docs/integrations/text_embedding/azureopenai/)\n",
      "    - [Baichuan Text Embeddings](/docs/integrations/text_embedding/baichuan/)\n",
      "    - [Baidu Qianfan](/docs/integrations/text_embedding/baidu_qianfan_endpoint/)\n",
      "    - [Bedrock](/docs/integrations/text_embedding/bedrock/)\n",
      "    - [BGE on Hugging Face](/docs/integrations/text_embedding/bge_huggingface/)\n",
      "    - [Bookend AI](/docs/integrations/text_embedding/bookend/)\n",
      "    - [Clarifai](/docs/integrations/text_embedding/clarifai/)\n",
      "    - [Cloudflare Workers AI](/docs/integrations/text_embedding/cloudflare_workersai/)\n",
      "    - [Clova Embeddings](/docs/integrations/text_embedding/clova/)\n",
      "    - [Cohere](/docs/integrations/text_embedding/cohere/)\n",
      "    - [DashScope](/docs/integrations/text_embedding/dashscope/)\n",
      "    - [Databricks](/docs/integrations/text_embedding/databricks/)\n",
      "    - [DeepInfra](/docs/integrations/text_embedding/deepinfra/)\n",
      "    - [EDEN AI](/docs/integrations/text_embedding/edenai/)\n",
      "    - [Elasticsearch](/docs/integrations/text_embedding/elasticsearch/)\n",
      "    - [Embaas](/docs/integrations/text_embedding/embaas/)\n",
      "    - [ERNIE](/docs/integrations/text_embedding/ernie/)\n",
      "    - [Fake Embeddings](/docs/integrations/text_embedding/fake/)\n",
      "    - [FastEmbed by Qdrant](/docs/integrations/text_embedding/fastembed/)\n",
      "    - [Fireworks](/docs/integrations/text_embedding/fireworks/)\n",
      "    - [GigaChat](/docs/integrations/text_embedding/gigachat/)\n",
      "    - [Google Generative AI Embeddings](/docs/integrations/text_embedding/google_generative_ai/)\n",
      "    - [Google Vertex AI](/docs/integrations/text_embedding/google_vertex_ai_palm/)\n",
      "    - [GPT4All](/docs/integrations/text_embedding/gpt4all/)\n",
      "    - [Gradient](/docs/integrations/text_embedding/gradient/)\n",
      "    - [Hugging Face](/docs/integrations/text_embedding/huggingfacehub/)\n",
      "    - [IBM watsonx.ai](/docs/integrations/text_embedding/ibm_watsonx/)\n",
      "    - [Infinity](/docs/integrations/text_embedding/infinity/)\n",
      "    - [Instruct Embeddings on Hugging Face](/docs/integrations/text_embedding/instruct_embeddings/)\n",
      "    - [Local BGE Embeddings with IPEX-LLM on Intel CPU](/docs/integrations/text_embedding/ipex_llm/)\n",
      "    - [Local BGE Embeddings with IPEX-LLM on Intel GPU](/docs/integrations/text_embedding/ipex_llm_gpu/)\n",
      "    - [Intel® Extension for Transformers Quantized Text Embeddings](/docs/integrations/text_embedding/itrex/)\n",
      "    - [Jina](/docs/integrations/text_embedding/jina/)\n",
      "    - [John Snow Labs](/docs/integrations/text_embedding/johnsnowlabs_embedding/)\n",
      "    - [LASER Language-Agnostic SEntence Representations Embeddings by Meta AI](/docs/integrations/text_embedding/laser/)\n",
      "    - [Llama.cpp](/docs/integrations/text_embedding/llamacpp/)\n",
      "    - [llamafile](/docs/integrations/text_embedding/llamafile/)\n",
      "    - [LLMRails](/docs/integrations/text_embedding/llm_rails/)\n",
      "    - [LocalAI](/docs/integrations/text_embedding/localai/)\n",
      "    - [MiniMax](/docs/integrations/text_embedding/minimax/)\n",
      "    - [MistralAI](/docs/integrations/text_embedding/mistralai/)\n",
      "    - [ModelScope](/docs/integrations/text_embedding/modelscope_hub/)\n",
      "    - [MosaicML](/docs/integrations/text_embedding/mosaicml/)\n",
      "    - [NLP Cloud](/docs/integrations/text_embedding/nlp_cloud/)\n",
      "    - [Nomic](/docs/integrations/text_embedding/nomic/)\n",
      "    - [NVIDIA NIMs](/docs/integrations/text_embedding/nvidia_ai_endpoints/)\n",
      "    - [Oracle Cloud Infrastructure Generative AI](/docs/integrations/text_embedding/oci_generative_ai/)\n",
      "    - [Ollama](/docs/integrations/text_embedding/ollama/)\n",
      "    - [OpenClip](/docs/integrations/text_embedding/open_clip/)\n",
      "    - [OpenAI](/docs/integrations/text_embedding/openai/)\n",
      "    - [OpenVINO](/docs/integrations/text_embedding/openvino/)\n",
      "    - [Embedding Documents using Optimized and Quantized Embedders](/docs/integrations/text_embedding/optimum_intel/)\n",
      "    - [Oracle AI Vector Search: Generate Embeddings](/docs/integrations/text_embedding/oracleai/)\n",
      "    - [OVHcloud](/docs/integrations/text_embedding/ovhcloud/)\n",
      "    - [Pinecone Embeddings](/docs/integrations/text_embedding/pinecone/)\n",
      "    - [PremAI](/docs/integrations/text_embedding/premai/)\n",
      "    - [SageMaker](/docs/integrations/text_embedding/sagemaker-endpoint/)\n",
      "    - [SambaNova](/docs/integrations/text_embedding/sambanova/)\n",
      "    - [Self Hosted](/docs/integrations/text_embedding/self-hosted/)\n",
      "    - [Sentence Transformers on Hugging Face](/docs/integrations/text_embedding/sentence_transformers/)\n",
      "    - [Solar](/docs/integrations/text_embedding/solar/)\n",
      "    - [SpaCy](/docs/integrations/text_embedding/spacy_embedding/)\n",
      "    - [SparkLLM Text Embeddings](/docs/integrations/text_embedding/sparkllm/)\n",
      "    - [TensorFlow Hub](/docs/integrations/text_embedding/tensorflowhub/)\n",
      "    - [Text Embeddings Inference](/docs/integrations/text_embedding/text_embeddings_inference/)\n",
      "    - [TextEmbed - Embedding Inference Server](/docs/integrations/text_embedding/textembed/)\n",
      "    - [Titan Takeoff](/docs/integrations/text_embedding/titan_takeoff/)\n",
      "    - [Together AI](/docs/integrations/text_embedding/together/)\n",
      "    - [Upstage](/docs/integrations/text_embedding/upstage/)\n",
      "    - [Volc Engine](/docs/integrations/text_embedding/volcengine/)\n",
      "    - [Voyage AI](/docs/integrations/text_embedding/voyageai/)\n",
      "    - [Xorbits inference (Xinference)](/docs/integrations/text_embedding/xinference/)\n",
      "    - [YandexGPT](/docs/integrations/text_embedding/yandex/)\n",
      "    - [ZhipuAI](/docs/integrations/text_embedding/zhipuai/)\n",
      "  - [Document loaders](/docs/integrations/document_loaders/)\n",
      "\n",
      "    - [Document loaders](/docs/integrations/document_loaders/)\n",
      "    - [acreom](/docs/integrations/document_loaders/acreom/)\n",
      "    - [AirbyteLoader](/docs/integrations/document_loaders/airbyte/)\n",
      "    - [Airbyte CDK (Deprecated)](/docs/integrations/document_loaders/airbyte_cdk/)\n",
      "    - [Airbyte Gong (Deprecated)](/docs/integrations/document_loaders/airbyte_gong/)\n",
      "    - [Airbyte Hubspot (Deprecated)](/docs/integrations/document_loaders/airbyte_hubspot/)\n",
      "    - [Airbyte JSON (Deprecated)](/docs/integrations/document_loaders/airbyte_json/)\n",
      "    - [Airbyte Salesforce (Deprecated)](/docs/integrations/document_loaders/airbyte_salesforce/)\n",
      "    - [Airbyte Shopify (Deprecated)](/docs/integrations/document_loaders/airbyte_shopify/)\n",
      "    - [Airbyte Stripe (Deprecated)](/docs/integrations/document_loaders/airbyte_stripe/)\n",
      "    - [Airbyte Typeform (Deprecated)](/docs/integrations/document_loaders/airbyte_typeform/)\n",
      "    - [Airbyte Zendesk Support (Deprecated)](/docs/integrations/document_loaders/airbyte_zendesk_support/)\n",
      "    - [Airtable](/docs/integrations/document_loaders/airtable/)\n",
      "    - [Alibaba Cloud MaxCompute](/docs/integrations/document_loaders/alibaba_cloud_maxcompute/)\n",
      "    - [Amazon Textract](/docs/integrations/document_loaders/amazon_textract/)\n",
      "    - [Apify Dataset](/docs/integrations/document_loaders/apify_dataset/)\n",
      "    - [ArcGIS](/docs/integrations/document_loaders/arcgis/)\n",
      "    - [ArxivLoader](/docs/integrations/document_loaders/arxiv/)\n",
      "    - [AssemblyAI Audio Transcripts](/docs/integrations/document_loaders/assemblyai/)\n",
      "    - [AstraDB](/docs/integrations/document_loaders/astradb/)\n",
      "    - [Async Chromium](/docs/integrations/document_loaders/async_chromium/)\n",
      "    - [AsyncHtml](/docs/integrations/document_loaders/async_html/)\n",
      "    - [Athena](/docs/integrations/document_loaders/athena/)\n",
      "    - [AWS S3 Directory](/docs/integrations/document_loaders/aws_s3_directory/)\n",
      "    - [AWS S3 File](/docs/integrations/document_loaders/aws_s3_file/)\n",
      "    - [AZLyrics](/docs/integrations/document_loaders/azlyrics/)\n",
      "    - [Azure AI Data](/docs/integrations/document_loaders/azure_ai_data/)\n",
      "    - [Azure Blob Storage Container](/docs/integrations/document_loaders/azure_blob_storage_container/)\n",
      "    - [Azure Blob Storage File](/docs/integrations/document_loaders/azure_blob_storage_file/)\n",
      "    - [Azure AI Document Intelligence](/docs/integrations/document_loaders/azure_document_intelligence/)\n",
      "    - [BibTeX](/docs/integrations/document_loaders/bibtex/)\n",
      "    - [BiliBili](/docs/integrations/document_loaders/bilibili/)\n",
      "    - [Blackboard](/docs/integrations/document_loaders/blackboard/)\n",
      "    - [Blockchain](/docs/integrations/document_loaders/blockchain/)\n",
      "    - [Box](/docs/integrations/document_loaders/box/)\n",
      "    - [Brave Search](/docs/integrations/document_loaders/brave_search/)\n",
      "    - [Browserbase](/docs/integrations/document_loaders/browserbase/)\n",
      "    - [Browserless](/docs/integrations/document_loaders/browserless/)\n",
      "    - [BSHTMLLoader](/docs/integrations/document_loaders/bshtml/)\n",
      "    - [Cassandra](/docs/integrations/document_loaders/cassandra/)\n",
      "    - [ChatGPT Data](/docs/integrations/document_loaders/chatgpt_loader/)\n",
      "    - [College Confidential](/docs/integrations/document_loaders/college_confidential/)\n",
      "    - [Concurrent Loader](/docs/integrations/document_loaders/concurrent/)\n",
      "    - [Confluence](/docs/integrations/document_loaders/confluence/)\n",
      "    - [CoNLL-U](/docs/integrations/document_loaders/conll-u/)\n",
      "    - [Copy Paste](/docs/integrations/document_loaders/copypaste/)\n",
      "    - [Couchbase](/docs/integrations/document_loaders/couchbase/)\n",
      "    - [CSV](/docs/integrations/document_loaders/csv/)\n",
      "    - [Cube Semantic Layer](/docs/integrations/document_loaders/cube_semantic/)\n",
      "    - [Datadog Logs](/docs/integrations/document_loaders/datadog_logs/)\n",
      "    - [Dedoc](/docs/integrations/document_loaders/dedoc/)\n",
      "    - [Diffbot](/docs/integrations/document_loaders/diffbot/)\n",
      "    - [Discord](/docs/integrations/document_loaders/discord/)\n",
      "    - [Docugami](/docs/integrations/document_loaders/docugami/)\n",
      "    - [Docusaurus](/docs/integrations/document_loaders/docusaurus/)\n",
      "    - [Dropbox](/docs/integrations/document_loaders/dropbox/)\n",
      "    - [DuckDB](/docs/integrations/document_loaders/duckdb/)\n",
      "    - [Email](/docs/integrations/document_loaders/email/)\n",
      "    - [EPub](/docs/integrations/document_loaders/epub/)\n",
      "    - [Etherscan](/docs/integrations/document_loaders/etherscan/)\n",
      "    - [EverNote](/docs/integrations/document_loaders/evernote/)\n",
      "    - example\\_data\n",
      "\n",
      "    - [Facebook Chat](/docs/integrations/document_loaders/facebook_chat/)\n",
      "    - [Fauna](/docs/integrations/document_loaders/fauna/)\n",
      "    - [Figma](/docs/integrations/document_loaders/figma/)\n",
      "    - [FireCrawl](/docs/integrations/document_loaders/firecrawl/)\n",
      "    - [Geopandas](/docs/integrations/document_loaders/geopandas/)\n",
      "    - [Git](/docs/integrations/document_loaders/git/)\n",
      "    - [GitBook](/docs/integrations/document_loaders/gitbook/)\n",
      "    - [GitHub](/docs/integrations/document_loaders/github/)\n",
      "    - [Glue Catalog](/docs/integrations/document_loaders/glue_catalog/)\n",
      "    - [Google AlloyDB for PostgreSQL](/docs/integrations/document_loaders/google_alloydb/)\n",
      "    - [Google BigQuery](/docs/integrations/document_loaders/google_bigquery/)\n",
      "    - [Google Bigtable](/docs/integrations/document_loaders/google_bigtable/)\n",
      "    - [Google Cloud SQL for SQL server](/docs/integrations/document_loaders/google_cloud_sql_mssql/)\n",
      "    - [Google Cloud SQL for MySQL](/docs/integrations/document_loaders/google_cloud_sql_mysql/)\n",
      "    - [Google Cloud SQL for PostgreSQL](/docs/integrations/document_loaders/google_cloud_sql_pg/)\n",
      "    - [Google Cloud Storage Directory](/docs/integrations/document_loaders/google_cloud_storage_directory/)\n",
      "    - [Google Cloud Storage File](/docs/integrations/document_loaders/google_cloud_storage_file/)\n",
      "    - [Google Firestore in Datastore Mode](/docs/integrations/document_loaders/google_datastore/)\n",
      "    - [Google Drive](/docs/integrations/document_loaders/google_drive/)\n",
      "    - [Google El Carro for Oracle Workloads](/docs/integrations/document_loaders/google_el_carro/)\n",
      "    - [Google Firestore (Native Mode)](/docs/integrations/document_loaders/google_firestore/)\n",
      "    - [Google Memorystore for Redis](/docs/integrations/document_loaders/google_memorystore_redis/)\n",
      "    - [Google Spanner](/docs/integrations/document_loaders/google_spanner/)\n",
      "    - [Google Speech-to-Text Audio Transcripts](/docs/integrations/document_loaders/google_speech_to_text/)\n",
      "    - [Grobid](/docs/integrations/document_loaders/grobid/)\n",
      "    - [Gutenberg](/docs/integrations/document_loaders/gutenberg/)\n",
      "    - [Hacker News](/docs/integrations/document_loaders/hacker_news/)\n",
      "    - [Huawei OBS Directory](/docs/integrations/document_loaders/huawei_obs_directory/)\n",
      "    - [Huawei OBS File](/docs/integrations/document_loaders/huawei_obs_file/)\n",
      "    - [HuggingFace dataset](/docs/integrations/document_loaders/hugging_face_dataset/)\n",
      "    - [iFixit](/docs/integrations/document_loaders/ifixit/)\n",
      "    - [Images](/docs/integrations/document_loaders/image/)\n",
      "    - [Image captions](/docs/integrations/document_loaders/image_captions/)\n",
      "    - [IMSDb](/docs/integrations/document_loaders/imsdb/)\n",
      "    - [Iugu](/docs/integrations/document_loaders/iugu/)\n",
      "    - [Joplin](/docs/integrations/document_loaders/joplin/)\n",
      "    - [JSONLoader](/docs/integrations/document_loaders/json/)\n",
      "    - [Jupyter Notebook](/docs/integrations/document_loaders/jupyter_notebook/)\n",
      "    - [Kinetica](/docs/integrations/document_loaders/kinetica/)\n",
      "    - [lakeFS](/docs/integrations/document_loaders/lakefs/)\n",
      "    - [LangSmith](/docs/integrations/document_loaders/langsmith/)\n",
      "    - [LarkSuite (FeiShu)](/docs/integrations/document_loaders/larksuite/)\n",
      "    - [LLM Sherpa](/docs/integrations/document_loaders/llmsherpa/)\n",
      "    - [Mastodon](/docs/integrations/document_loaders/mastodon/)\n",
      "    - [MathPixPDFLoader](/docs/integrations/document_loaders/mathpix/)\n",
      "    - [MediaWiki Dump](/docs/integrations/document_loaders/mediawikidump/)\n",
      "    - [Merge Documents Loader](/docs/integrations/document_loaders/merge_doc/)\n",
      "    - [mhtml](/docs/integrations/document_loaders/mhtml/)\n",
      "    - [Microsoft Excel](/docs/integrations/document_loaders/microsoft_excel/)\n",
      "    - [Microsoft OneDrive](/docs/integrations/document_loaders/microsoft_onedrive/)\n",
      "    - [Microsoft OneNote](/docs/integrations/document_loaders/microsoft_onenote/)\n",
      "    - [Microsoft PowerPoint](/docs/integrations/document_loaders/microsoft_powerpoint/)\n",
      "    - [Microsoft SharePoint](/docs/integrations/document_loaders/microsoft_sharepoint/)\n",
      "    - [Microsoft Word](/docs/integrations/document_loaders/microsoft_word/)\n",
      "    - [Near Blockchain](/docs/integrations/document_loaders/mintbase/)\n",
      "    - [Modern Treasury](/docs/integrations/document_loaders/modern_treasury/)\n",
      "    - [MongoDB](/docs/integrations/document_loaders/mongodb/)\n",
      "    - [News URL](/docs/integrations/document_loaders/news/)\n",
      "    - [Notion DB 2/2](/docs/integrations/document_loaders/notion/)\n",
      "    - [Nuclia](/docs/integrations/document_loaders/nuclia/)\n",
      "    - [Obsidian](/docs/integrations/document_loaders/obsidian/)\n",
      "    - [Open Document Format (ODT)](/docs/integrations/document_loaders/odt/)\n",
      "    - [Open City Data](/docs/integrations/document_loaders/open_city_data/)\n",
      "    - [Oracle Autonomous Database](/docs/integrations/document_loaders/oracleadb_loader/)\n",
      "    - [Oracle AI Vector Search: Document Processing](/docs/integrations/document_loaders/oracleai/)\n",
      "    - [Org-mode](/docs/integrations/document_loaders/org_mode/)\n",
      "    - [Pandas DataFrame](/docs/integrations/document_loaders/pandas_dataframe/)\n",
      "    - [PDFMiner](/docs/integrations/document_loaders/pdfminer/)\n",
      "    - [PDFPlumber](/docs/integrations/document_loaders/pdfplumber/)\n",
      "    - [Pebblo Safe DocumentLoader](/docs/integrations/document_loaders/pebblo/)\n",
      "    - [Polars DataFrame](/docs/integrations/document_loaders/polars_dataframe/)\n",
      "    - [Psychic](/docs/integrations/document_loaders/psychic/)\n",
      "    - [PubMed](/docs/integrations/document_loaders/pubmed/)\n",
      "    - [PyMuPDF](/docs/integrations/document_loaders/pymupdf/)\n",
      "    - [PyPDFDirectoryLoader](/docs/integrations/document_loaders/pypdfdirectory/)\n",
      "    - [PyPDFium2Loader](/docs/integrations/document_loaders/pypdfium2/)\n",
      "    - [PyPDFLoader](/docs/integrations/document_loaders/pypdfloader/)\n",
      "    - [PySpark](/docs/integrations/document_loaders/pyspark_dataframe/)\n",
      "    - [Quip](/docs/integrations/document_loaders/quip/)\n",
      "    - [ReadTheDocs Documentation](/docs/integrations/document_loaders/readthedocs_documentation/)\n",
      "    - [Recursive URL](/docs/integrations/document_loaders/recursive_url/)\n",
      "    - [Reddit](/docs/integrations/document_loaders/reddit/)\n",
      "    - [Roam](/docs/integrations/document_loaders/roam/)\n",
      "    - [Rockset](/docs/integrations/document_loaders/rockset/)\n",
      "    - [rspace](/docs/integrations/document_loaders/rspace/)\n",
      "    - [RSS Feeds](/docs/integrations/document_loaders/rss/)\n",
      "    - [RST](/docs/integrations/document_loaders/rst/)\n",
      "    - [scrapfly](/docs/integrations/document_loaders/scrapfly/)\n",
      "    - [ScrapingAnt](/docs/integrations/document_loaders/scrapingant/)\n",
      "    - [Sitemap](/docs/integrations/document_loaders/sitemap/)\n",
      "    - [Slack](/docs/integrations/document_loaders/slack/)\n",
      "    - [Snowflake](/docs/integrations/document_loaders/snowflake/)\n",
      "    - [Source Code](/docs/integrations/document_loaders/source_code/)\n",
      "    - [Spider](/docs/integrations/document_loaders/spider/)\n",
      "    - [Spreedly](/docs/integrations/document_loaders/spreedly/)\n",
      "    - [Stripe](/docs/integrations/document_loaders/stripe/)\n",
      "    - [Subtitle](/docs/integrations/document_loaders/subtitle/)\n",
      "    - [SurrealDB](/docs/integrations/document_loaders/surrealdb/)\n",
      "    - [Telegram](/docs/integrations/document_loaders/telegram/)\n",
      "    - [Tencent COS Directory](/docs/integrations/document_loaders/tencent_cos_directory/)\n",
      "    - [Tencent COS File](/docs/integrations/document_loaders/tencent_cos_file/)\n",
      "    - [TensorFlow Datasets](/docs/integrations/document_loaders/tensorflow_datasets/)\n",
      "    - [TiDB](/docs/integrations/document_loaders/tidb/)\n",
      "    - [2Markdown](/docs/integrations/document_loaders/tomarkdown/)\n",
      "    - [TOML](/docs/integrations/document_loaders/toml/)\n",
      "    - [Trello](/docs/integrations/document_loaders/trello/)\n",
      "    - [TSV](/docs/integrations/document_loaders/tsv/)\n",
      "    - [Twitter](/docs/integrations/document_loaders/twitter/)\n",
      "    - [Unstructured](/docs/integrations/document_loaders/unstructured_file/)\n",
      "    - [UnstructuredMarkdownLoader](/docs/integrations/document_loaders/unstructured_markdown/)\n",
      "    - [UnstructuredPDFLoader](/docs/integrations/document_loaders/unstructured_pdfloader/)\n",
      "    - [Upstage](/docs/integrations/document_loaders/upstage/)\n",
      "    - [URL](/docs/integrations/document_loaders/url/)\n",
      "    - [Vsdx](/docs/integrations/document_loaders/vsdx/)\n",
      "    - [Weather](/docs/integrations/document_loaders/weather/)\n",
      "    - [WebBaseLoader](/docs/integrations/document_loaders/web_base/)\n",
      "    - [WhatsApp Chat](/docs/integrations/document_loaders/whatsapp_chat/)\n",
      "    - [Wikipedia](/docs/integrations/document_loaders/wikipedia/)\n",
      "    - [UnstructuredXMLLoader](/docs/integrations/document_loaders/xml/)\n",
      "    - [Xorbits Pandas DataFrame](/docs/integrations/document_loaders/xorbits/)\n",
      "    - [YouTube audio](/docs/integrations/document_loaders/youtube_audio/)\n",
      "    - [YouTube transcripts](/docs/integrations/document_loaders/youtube_transcript/)\n",
      "    - [Yuque](/docs/integrations/document_loaders/yuque/)\n",
      "  - [Vector stores](/docs/integrations/vectorstores/)\n",
      "\n",
      "    - [Vectorstores](/docs/integrations/vectorstores/)\n",
      "    - [Activeloop Deep Lake](/docs/integrations/vectorstores/activeloop_deeplake/)\n",
      "    - [Aerospike](/docs/integrations/vectorstores/aerospike/)\n",
      "    - [Alibaba Cloud OpenSearch](/docs/integrations/vectorstores/alibabacloud_opensearch/)\n",
      "    - [AnalyticDB](/docs/integrations/vectorstores/analyticdb/)\n",
      "    - [Annoy](/docs/integrations/vectorstores/annoy/)\n",
      "    - [Apache Doris](/docs/integrations/vectorstores/apache_doris/)\n",
      "    - [ApertureDB](/docs/integrations/vectorstores/aperturedb/)\n",
      "    - [Astra DB Vector Store](/docs/integrations/vectorstores/astradb/)\n",
      "    - [Atlas](/docs/integrations/vectorstores/atlas/)\n",
      "    - [AwaDB](/docs/integrations/vectorstores/awadb/)\n",
      "    - [Azure Cosmos DB Mongo vCore](/docs/integrations/vectorstores/azure_cosmos_db/)\n",
      "    - [Azure Cosmos DB No SQL](/docs/integrations/vectorstores/azure_cosmos_db_no_sql/)\n",
      "    - [Azure AI Search](/docs/integrations/vectorstores/azuresearch/)\n",
      "    - [Bagel](/docs/integrations/vectorstores/bagel/)\n",
      "    - [BagelDB](/docs/integrations/vectorstores/bageldb/)\n",
      "    - [Baidu Cloud ElasticSearch VectorSearch](/docs/integrations/vectorstores/baiducloud_vector_search/)\n",
      "    - [Baidu VectorDB](/docs/integrations/vectorstores/baiduvectordb/)\n",
      "    - [Apache Cassandra](/docs/integrations/vectorstores/cassandra/)\n",
      "    - [Chroma](/docs/integrations/vectorstores/chroma/)\n",
      "    - [Clarifai](/docs/integrations/vectorstores/clarifai/)\n",
      "    - [ClickHouse](/docs/integrations/vectorstores/clickhouse/)\n",
      "    - [Couchbase](/docs/integrations/vectorstores/couchbase/)\n",
      "    - [DashVector](/docs/integrations/vectorstores/dashvector/)\n",
      "    - [Databricks](/docs/integrations/vectorstores/databricks_vector_search/)\n",
      "    - [DingoDB](/docs/integrations/vectorstores/dingo/)\n",
      "    - [DocArray HnswSearch](/docs/integrations/vectorstores/docarray_hnsw/)\n",
      "    - [DocArray InMemorySearch](/docs/integrations/vectorstores/docarray_in_memory/)\n",
      "    - [Amazon Document DB](/docs/integrations/vectorstores/documentdb/)\n",
      "    - [DuckDB](/docs/integrations/vectorstores/duckdb/)\n",
      "    - [China Mobile ECloud ElasticSearch VectorSearch](/docs/integrations/vectorstores/ecloud_vector_search/)\n",
      "    - [Elasticsearch](/docs/integrations/vectorstores/elasticsearch/)\n",
      "    - [Epsilla](/docs/integrations/vectorstores/epsilla/)\n",
      "    - [Faiss](/docs/integrations/vectorstores/faiss/)\n",
      "    - [Faiss (Async)](/docs/integrations/vectorstores/faiss_async/)\n",
      "    - [Google AlloyDB for PostgreSQL](/docs/integrations/vectorstores/google_alloydb/)\n",
      "    - [Google BigQuery Vector Search](/docs/integrations/vectorstores/google_bigquery_vector_search/)\n",
      "    - [Google Cloud SQL for MySQL](/docs/integrations/vectorstores/google_cloud_sql_mysql/)\n",
      "    - [Google Cloud SQL for PostgreSQL](/docs/integrations/vectorstores/google_cloud_sql_pg/)\n",
      "    - [Firestore](/docs/integrations/vectorstores/google_firestore/)\n",
      "    - [Google Memorystore for Redis](/docs/integrations/vectorstores/google_memorystore_redis/)\n",
      "    - [Google Spanner](/docs/integrations/vectorstores/google_spanner/)\n",
      "    - [Google Vertex AI Feature Store](/docs/integrations/vectorstores/google_vertex_ai_feature_store/)\n",
      "    - [Google Vertex AI Vector Search](/docs/integrations/vectorstores/google_vertex_ai_vector_search/)\n",
      "    - [Hippo](/docs/integrations/vectorstores/hippo/)\n",
      "    - [Hologres](/docs/integrations/vectorstores/hologres/)\n",
      "    - [Infinispan](/docs/integrations/vectorstores/infinispanvs/)\n",
      "    - [Jaguar Vector Database](/docs/integrations/vectorstores/jaguar/)\n",
      "    - [KDB.AI](/docs/integrations/vectorstores/kdbai/)\n",
      "    - [Kinetica](/docs/integrations/vectorstores/kinetica/)\n",
      "    - [LanceDB](/docs/integrations/vectorstores/lancedb/)\n",
      "    - [Lantern](/docs/integrations/vectorstores/lantern/)\n",
      "    - [LLMRails](/docs/integrations/vectorstores/llm_rails/)\n",
      "    - [ManticoreSearch VectorStore](/docs/integrations/vectorstores/manticore_search/)\n",
      "    - [Marqo](/docs/integrations/vectorstores/marqo/)\n",
      "    - [Meilisearch](/docs/integrations/vectorstores/meilisearch/)\n",
      "    - [Amazon MemoryDB](/docs/integrations/vectorstores/memorydb/)\n",
      "    - [Milvus](/docs/integrations/vectorstores/milvus/)\n",
      "    - [Momento Vector Index (MVI)](/docs/integrations/vectorstores/momento_vector_index/)\n",
      "    - [MongoDB Atlas](/docs/integrations/vectorstores/mongodb_atlas/)\n",
      "    - [MyScale](/docs/integrations/vectorstores/myscale/)\n",
      "    - [Neo4j Vector Index](/docs/integrations/vectorstores/neo4jvector/)\n",
      "    - [NucliaDB](/docs/integrations/vectorstores/nucliadb/)\n",
      "    - [OpenSearch](/docs/integrations/vectorstores/opensearch/)\n",
      "    - [Oracle AI Vector Search: Vector Store](/docs/integrations/vectorstores/oracle/)\n",
      "    - [Pathway](/docs/integrations/vectorstores/pathway/)\n",
      "    - [Postgres Embedding](/docs/integrations/vectorstores/pgembedding/)\n",
      "    - [PGVecto.rs](/docs/integrations/vectorstores/pgvecto_rs/)\n",
      "    - [PGVector](/docs/integrations/vectorstores/pgvector/)\n",
      "    - [Pinecone](/docs/integrations/vectorstores/pinecone/)\n",
      "    - [Qdrant](/docs/integrations/vectorstores/qdrant/)\n",
      "    - [Redis](/docs/integrations/vectorstores/redis/)\n",
      "    - [Relyt](/docs/integrations/vectorstores/relyt/)\n",
      "    - [Rockset](/docs/integrations/vectorstores/rockset/)\n",
      "    - [SAP HANA Cloud Vector Engine](/docs/integrations/vectorstores/sap_hanavector/)\n",
      "    - [ScaNN](/docs/integrations/vectorstores/scann/)\n",
      "    - [SemaDB](/docs/integrations/vectorstores/semadb/)\n",
      "    - [SingleStoreDB](/docs/integrations/vectorstores/singlestoredb/)\n",
      "    - [scikit-learn](/docs/integrations/vectorstores/sklearn/)\n",
      "    - [SQLite-VSS](/docs/integrations/vectorstores/sqlitevss/)\n",
      "    - [StarRocks](/docs/integrations/vectorstores/starrocks/)\n",
      "    - [Supabase (Postgres)](/docs/integrations/vectorstores/supabase/)\n",
      "    - [SurrealDB](/docs/integrations/vectorstores/surrealdb/)\n",
      "    - [Tair](/docs/integrations/vectorstores/tair/)\n",
      "    - [Tencent Cloud VectorDB](/docs/integrations/vectorstores/tencentvectordb/)\n",
      "    - [ThirdAI NeuralDB](/docs/integrations/vectorstores/thirdai_neuraldb/)\n",
      "    - [TiDB Vector](/docs/integrations/vectorstores/tidb_vector/)\n",
      "    - [Tigris](/docs/integrations/vectorstores/tigris/)\n",
      "    - [TileDB](/docs/integrations/vectorstores/tiledb/)\n",
      "    - [Timescale Vector (Postgres)](/docs/integrations/vectorstores/timescalevector/)\n",
      "    - [Typesense](/docs/integrations/vectorstores/typesense/)\n",
      "    - [Upstash Vector](/docs/integrations/vectorstores/upstash/)\n",
      "    - [USearch](/docs/integrations/vectorstores/usearch/)\n",
      "    - [Vald](/docs/integrations/vectorstores/vald/)\n",
      "    - [Intel's Visual Data Management System (VDMS)](/docs/integrations/vectorstores/vdms/)\n",
      "    - [Vearch](/docs/integrations/vectorstores/vearch/)\n",
      "    - [Vectara](/docs/integrations/vectorstores/vectara/)\n",
      "    - [Vespa](/docs/integrations/vectorstores/vespa/)\n",
      "    - [viking DB](/docs/integrations/vectorstores/vikingdb/)\n",
      "    - [vlite](/docs/integrations/vectorstores/vlite/)\n",
      "    - [Weaviate](/docs/integrations/vectorstores/weaviate/)\n",
      "    - [Xata](/docs/integrations/vectorstores/xata/)\n",
      "    - [Yellowbrick](/docs/integrations/vectorstores/yellowbrick/)\n",
      "    - [Zep](/docs/integrations/vectorstores/zep/)\n",
      "    - [Zep Cloud](/docs/integrations/vectorstores/zep_cloud/)\n",
      "    - [Zilliz](/docs/integrations/vectorstores/zilliz/)\n",
      "  - [Retrievers](/docs/integrations/retrievers/)\n",
      "\n",
      "    - [Retrievers](/docs/integrations/retrievers/)\n",
      "    - [Activeloop Deep Memory](/docs/integrations/retrievers/activeloop/)\n",
      "    - [Amazon Kendra](/docs/integrations/retrievers/amazon_kendra_retriever/)\n",
      "    - [Arcee](/docs/integrations/retrievers/arcee/)\n",
      "    - [Arxiv](/docs/integrations/retrievers/arxiv/)\n",
      "    - [AskNews](/docs/integrations/retrievers/asknews/)\n",
      "    - [Azure AI Search](/docs/integrations/retrievers/azure_ai_search/)\n",
      "    - [Bedrock (Knowledge Bases)](/docs/integrations/retrievers/bedrock/)\n",
      "    - [BM25](/docs/integrations/retrievers/bm25/)\n",
      "    - [Box](/docs/integrations/retrievers/box/)\n",
      "    - [BREEBS (Open Knowledge)](/docs/integrations/retrievers/breebs/)\n",
      "    - [Chaindesk](/docs/integrations/retrievers/chaindesk/)\n",
      "    - [ChatGPT plugin](/docs/integrations/retrievers/chatgpt-plugin/)\n",
      "    - [Cohere reranker](/docs/integrations/retrievers/cohere-reranker/)\n",
      "    - [Cohere RAG](/docs/integrations/retrievers/cohere/)\n",
      "    - [DocArray](/docs/integrations/retrievers/docarray_retriever/)\n",
      "    - [Dria](/docs/integrations/retrievers/dria_index/)\n",
      "    - [ElasticSearch BM25](/docs/integrations/retrievers/elastic_search_bm25/)\n",
      "    - [Elasticsearch](/docs/integrations/retrievers/elasticsearch_retriever/)\n",
      "    - [Embedchain](/docs/integrations/retrievers/embedchain/)\n",
      "    - [FlashRank reranker](/docs/integrations/retrievers/flashrank-reranker/)\n",
      "    - [Fleet AI Context](/docs/integrations/retrievers/fleet_context/)\n",
      "    - [Google Drive](/docs/integrations/retrievers/google_drive/)\n",
      "    - [Google Vertex AI Search](/docs/integrations/retrievers/google_vertex_ai_search/)\n",
      "    - [JaguarDB Vector Database](/docs/integrations/retrievers/jaguar/)\n",
      "    - [Kay.ai](/docs/integrations/retrievers/kay/)\n",
      "    - [Kinetica Vectorstore based Retriever](/docs/integrations/retrievers/kinetica/)\n",
      "    - [kNN](/docs/integrations/retrievers/knn/)\n",
      "    - [LLMLingua Document Compressor](/docs/integrations/retrievers/llmlingua/)\n",
      "    - [LOTR (Merger Retriever)](/docs/integrations/retrievers/merger_retriever/)\n",
      "    - [Metal](/docs/integrations/retrievers/metal/)\n",
      "    - [Milvus Hybrid Search](/docs/integrations/retrievers/milvus_hybrid_search/)\n",
      "    - [NanoPQ (Product Quantization)](/docs/integrations/retrievers/nanopq/)\n",
      "    - [Outline](/docs/integrations/retrievers/outline/)\n",
      "    - [Pinecone Hybrid Search](/docs/integrations/retrievers/pinecone_hybrid_search/)\n",
      "    - [PubMed](/docs/integrations/retrievers/pubmed/)\n",
      "    - [Qdrant Sparse Vector](/docs/integrations/retrievers/qdrant-sparse/)\n",
      "    - [RAGatouille](/docs/integrations/retrievers/ragatouille/)\n",
      "    - [RePhraseQuery](/docs/integrations/retrievers/re_phrase/)\n",
      "    - [Rememberizer](/docs/integrations/retrievers/rememberizer/)\n",
      "    - [SEC filing](/docs/integrations/retrievers/sec_filings/)\n",
      "    - [Self-querying retrievers](/docs/integrations/retrievers/self_query/)\n",
      "\n",
      "    - [SingleStoreDB](/docs/integrations/retrievers/singlestoredb/)\n",
      "    - [SVM](/docs/integrations/retrievers/svm/)\n",
      "    - [TavilySearchAPI](/docs/integrations/retrievers/tavily/)\n",
      "    - [TF-IDF](/docs/integrations/retrievers/tf_idf/)\n",
      "    - [\\*\\*NeuralDB\\*\\*](/docs/integrations/retrievers/thirdai_neuraldb/)\n",
      "    - [Vespa](/docs/integrations/retrievers/vespa/)\n",
      "    - [Weaviate Hybrid Search](/docs/integrations/retrievers/weaviate-hybrid/)\n",
      "    - [Wikipedia](/docs/integrations/retrievers/wikipedia/)\n",
      "    - [You.com](/docs/integrations/retrievers/you-retriever/)\n",
      "    - [Zep Cloud](/docs/integrations/retrievers/zep_cloud_memorystore/)\n",
      "    - [Zep Open Source](/docs/integrations/retrievers/zep_memorystore/)\n",
      "    - [Zilliz Cloud Pipeline](/docs/integrations/retrievers/zilliz_cloud_pipeline/)\n",
      "  - [Tools/Toolkits](/docs/integrations/tools/)\n",
      "\n",
      "    - [Tools](/docs/integrations/tools/)\n",
      "    - [AINetwork Toolkit](/docs/integrations/tools/ainetwork/)\n",
      "    - [Alpha Vantage](/docs/integrations/tools/alpha_vantage/)\n",
      "    - [Amadeus Toolkit](/docs/integrations/tools/amadeus/)\n",
      "    - [ArXiv](/docs/integrations/tools/arxiv/)\n",
      "    - [AskNews](/docs/integrations/tools/asknews/)\n",
      "    - [AWS Lambda](/docs/integrations/tools/awslambda/)\n",
      "    - [Azure AI Services Toolkit](/docs/integrations/tools/azure_ai_services/)\n",
      "    - [Azure Cognitive Services Toolkit](/docs/integrations/tools/azure_cognitive_services/)\n",
      "    - [Azure Container Apps dynamic sessions](/docs/integrations/tools/azure_dynamic_sessions/)\n",
      "    - [Shell (bash)](/docs/integrations/tools/bash/)\n",
      "    - [Bearly Code Interpreter](/docs/integrations/tools/bearly/)\n",
      "    - [Bing Search](/docs/integrations/tools/bing_search/)\n",
      "    - [Brave Search](/docs/integrations/tools/brave_search/)\n",
      "    - [Cassandra Database Toolkit](/docs/integrations/tools/cassandra_database/)\n",
      "    - [ChatGPT Plugins](/docs/integrations/tools/chatgpt_plugins/)\n",
      "    - [ClickUp Toolkit](/docs/integrations/tools/clickup/)\n",
      "    - [Cogniswitch Toolkit](/docs/integrations/tools/cogniswitch/)\n",
      "    - [Connery Toolkit and Tools](/docs/integrations/tools/connery/)\n",
      "    - [Dall-E Image Generator](/docs/integrations/tools/dalle_image_generator/)\n",
      "    - [Databricks Unity Catalog (UC)](/docs/integrations/tools/databricks/)\n",
      "    - [DataForSEO](/docs/integrations/tools/dataforseo/)\n",
      "    - [Dataherald](/docs/integrations/tools/dataherald/)\n",
      "    - [DuckDuckGo Search](/docs/integrations/tools/ddg/)\n",
      "    - [E2B Data Analysis](/docs/integrations/tools/e2b_data_analysis/)\n",
      "    - [Eden AI](/docs/integrations/tools/edenai_tools/)\n",
      "    - [Eleven Labs Text2Speech](/docs/integrations/tools/eleven_labs_tts/)\n",
      "    - [Exa Search](/docs/integrations/tools/exa_search/)\n",
      "    - [File System](/docs/integrations/tools/filesystem/)\n",
      "    - [FinancialDatasets Toolkit](/docs/integrations/tools/financial_datasets/)\n",
      "    - [Github Toolkit](/docs/integrations/tools/github/)\n",
      "    - [Gitlab Toolkit](/docs/integrations/tools/gitlab/)\n",
      "    - [Gmail Toolkit](/docs/integrations/tools/gmail/)\n",
      "    - [Golden Query](/docs/integrations/tools/golden_query/)\n",
      "    - [Google Cloud Text-to-Speech](/docs/integrations/tools/google_cloud_texttospeech/)\n",
      "    - [Google Drive](/docs/integrations/tools/google_drive/)\n",
      "    - [Google Finance](/docs/integrations/tools/google_finance/)\n",
      "    - [Google Imagen](/docs/integrations/tools/google_imagen/)\n",
      "    - [Google Jobs](/docs/integrations/tools/google_jobs/)\n",
      "    - [Google Lens](/docs/integrations/tools/google_lens/)\n",
      "    - [Google Places](/docs/integrations/tools/google_places/)\n",
      "    - [Google Scholar](/docs/integrations/tools/google_scholar/)\n",
      "    - [Google Search](/docs/integrations/tools/google_search/)\n",
      "    - [Google Serper](/docs/integrations/tools/google_serper/)\n",
      "    - [Google Trends](/docs/integrations/tools/google_trends/)\n",
      "    - [Gradio](/docs/integrations/tools/gradio_tools/)\n",
      "    - [GraphQL](/docs/integrations/tools/graphql/)\n",
      "    - [HuggingFace Hub Tools](/docs/integrations/tools/huggingface_tools/)\n",
      "    - [Human as a tool](/docs/integrations/tools/human_tools/)\n",
      "    - [IFTTT WebHooks](/docs/integrations/tools/ifttt/)\n",
      "    - [Infobip](/docs/integrations/tools/infobip/)\n",
      "    - [Ionic Shopping Tool](/docs/integrations/tools/ionic_shopping/)\n",
      "    - [Jina Search](/docs/integrations/tools/jina_search/)\n",
      "    - [Jira Toolkit](/docs/integrations/tools/jira/)\n",
      "    - [JSON Toolkit](/docs/integrations/tools/json/)\n",
      "    - [Lemon Agent](/docs/integrations/tools/lemonai/)\n",
      "    - [Memorize](/docs/integrations/tools/memorize/)\n",
      "    - [Mojeek Search](/docs/integrations/tools/mojeek_search/)\n",
      "    - [MultiOn Toolkit](/docs/integrations/tools/multion/)\n",
      "    - [NASA Toolkit](/docs/integrations/tools/nasa/)\n",
      "    - [Nuclia Understanding](/docs/integrations/tools/nuclia/)\n",
      "    - [NVIDIA Riva: ASR and TTS](/docs/integrations/tools/nvidia_riva/)\n",
      "    - [Office365 Toolkit](/docs/integrations/tools/office365/)\n",
      "    - [OpenAPI Toolkit](/docs/integrations/tools/openapi/)\n",
      "    - [Natural Language API Toolkits](/docs/integrations/tools/openapi_nla/)\n",
      "    - [OpenWeatherMap](/docs/integrations/tools/openweathermap/)\n",
      "    - [Oracle AI Vector Search: Generate Summary](/docs/integrations/tools/oracleai/)\n",
      "    - [Pandas Dataframe](/docs/integrations/tools/pandas/)\n",
      "    - [Passio NutritionAI](/docs/integrations/tools/passio_nutrition_ai/)\n",
      "    - [PlayWright Browser Toolkit](/docs/integrations/tools/playwright/)\n",
      "    - [Polygon IO Toolkit and Tools](/docs/integrations/tools/polygon/)\n",
      "    - [PowerBI Toolkit](/docs/integrations/tools/powerbi/)\n",
      "    - [PubMed](/docs/integrations/tools/pubmed/)\n",
      "    - [Python REPL](/docs/integrations/tools/python/)\n",
      "    - [Reddit Search](/docs/integrations/tools/reddit_search/)\n",
      "    - [Requests Toolkit](/docs/integrations/tools/requests/)\n",
      "    - [Riza Code Interpreter](/docs/integrations/tools/riza/)\n",
      "    - [Robocorp Toolkit](/docs/integrations/tools/robocorp/)\n",
      "    - [SceneXplain](/docs/integrations/tools/sceneXplain/)\n",
      "    - [SearchApi](/docs/integrations/tools/searchapi/)\n",
      "    - [SearxNG Search](/docs/integrations/tools/searx_search/)\n",
      "    - [Semantic Scholar API Tool](/docs/integrations/tools/semanticscholar/)\n",
      "    - [SerpAPI](/docs/integrations/tools/serpapi/)\n",
      "    - [Slack Toolkit](/docs/integrations/tools/slack/)\n",
      "    - [Spark SQL Toolkit](/docs/integrations/tools/spark_sql/)\n",
      "    - [SQLDatabase Toolkit](/docs/integrations/tools/sql_database/)\n",
      "    - [StackExchange](/docs/integrations/tools/stackexchange/)\n",
      "    - [Steam Toolkit](/docs/integrations/tools/steam/)\n",
      "    - [Tavily Search](/docs/integrations/tools/tavily_search/)\n",
      "    - [Twilio](/docs/integrations/tools/twilio/)\n",
      "    - [Upstage](/docs/integrations/tools/upstage_groundedness_check/)\n",
      "    - [Wikidata](/docs/integrations/tools/wikidata/)\n",
      "    - [Wikipedia](/docs/integrations/tools/wikipedia/)\n",
      "    - [Wolfram Alpha](/docs/integrations/tools/wolfram_alpha/)\n",
      "    - [Yahoo Finance News](/docs/integrations/tools/yahoo_finance_news/)\n",
      "    - [You.com Search](/docs/integrations/tools/you/)\n",
      "    - [YouTube](/docs/integrations/tools/youtube/)\n",
      "    - [Zapier Natural Language Actions](/docs/integrations/tools/zapier/)\n",
      "    - [ZenGuard AI](/docs/integrations/tools/zenguard/)\n",
      "  - [Key-value stores](/docs/integrations/stores/)\n",
      "\n",
      "    - [AstraDB](/docs/integrations/stores/astradb/)\n",
      "    - [Cassandra](/docs/integrations/stores/cassandra/)\n",
      "    - [Elasticsearch](/docs/integrations/stores/elasticsearch/)\n",
      "    - [Local Filesystem](/docs/integrations/stores/file_system/)\n",
      "    - [In-memory](/docs/integrations/stores/in_memory/)\n",
      "    - [Key-value stores](/docs/integrations/stores/)\n",
      "    - [Redis](/docs/integrations/stores/redis/)\n",
      "    - [Upstash Redis](/docs/integrations/stores/upstash_redis/)\n",
      "  - Other\n",
      "\n",
      "- [Home page](/)\n",
      "- [Components](/docs/integrations/components/)\n",
      "- [Document loaders](/docs/integrations/document_loaders/)\n",
      "- FireCrawl\n",
      "\n",
      "On this page\n",
      "\n",
      "# FireCrawl\n",
      "\n",
      "[FireCrawl](https://firecrawl.dev/?ref=langchain) crawls and convert any website into LLM-ready data. It crawls all accessible subpages and give you clean markdown and metadata for each. No sitemap required.\n",
      "\n",
      "FireCrawl handles complex tasks such as reverse proxies, caching, rate limits, and content blocked by JavaScript. Built by the [mendable.ai](https://mendable.ai) team.\n",
      "\n",
      "## Overview [​](\\#overview \"Direct link to Overview\")\n",
      "\n",
      "### Integration details [​](\\#integration-details \"Direct link to Integration details\")\n",
      "\n",
      "| Class | Package | Local | Serializable | [JS support](https://js.langchain.com/docs/integrations/document_loaders/web_loaders/firecrawl/) |\n",
      "| :-- | :-- | :-: | :-: | :-: |\n",
      "| [FireCrawlLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.firecrawl.FireCrawlLoader.html) | [langchain\\_community](https://python.langchain.com/api_reference/community/index.html) | ✅ | ❌ | ✅ |\n",
      "\n",
      "### Loader features [​](\\#loader-features \"Direct link to Loader features\")\n",
      "\n",
      "| Source | Document Lazy Loading | Native Async Support |\n",
      "| :-: | :-: | :-: |\n",
      "| FireCrawlLoader | ✅ | ❌ |\n",
      "\n",
      "## Setup [​](\\#setup \"Direct link to Setup\")\n",
      "\n",
      "### Credentials [​](\\#credentials \"Direct link to Credentials\")\n",
      "\n",
      "You will need to get your own API key. Go to [this page](https://firecrawl.dev) to learn more.\n",
      "\n",
      "```codeBlockLines_e6Vv\n",
      "import getpass\n",
      "import os\n",
      "\n",
      "if \"FIRECRAWL_API_KEY\" not in os.environ:\n",
      "    os.environ[\"FIRECRAWL_API_KEY\"] = getpass.getpass(\"Enter your Firecrawl API key: \")\n",
      "\n",
      "```\n",
      "\n",
      "### Installation [​](\\#installation \"Direct link to Installation\")\n",
      "\n",
      "You will need to install both the `langchain_community` and `firecrawl-py` pacakges:\n",
      "\n",
      "```codeBlockLines_e6Vv\n",
      "%pip install -qU firecrawl-py==0.0.20 langchain_community\n",
      "\n",
      "```\n",
      "\n",
      "## Initialization [​](\\#initialization \"Direct link to Initialization\")\n",
      "\n",
      "### Modes [​](\\#modes \"Direct link to Modes\")\n",
      "\n",
      "- `scrape`: Scrape single url and return the markdown.\n",
      "- `crawl`: Crawl the url and all accessible sub pages and return the markdown for each one.\n",
      "\n",
      "```codeBlockLines_e6Vv\n",
      "from langchain_community.document_loaders import FireCrawlLoader\n",
      "\n",
      "loader = FireCrawlLoader(url=\"https://firecrawl.dev\", mode=\"crawl\")\n",
      "\n",
      "```\n",
      "\n",
      "**API Reference:** [FireCrawlLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.firecrawl.FireCrawlLoader.html)\n",
      "\n",
      "## Load [​](\\#load \"Direct link to Load\")\n",
      "\n",
      "```codeBlockLines_e6Vv\n",
      "docs = loader.load()\n",
      "\n",
      "docs[0]\n",
      "\n",
      "```\n",
      "\n",
      "```codeBlockLines_e6Vv\n",
      "Document(metadata={'ogUrl': 'https://www.firecrawl.dev/', 'title': 'Home - Firecrawl', 'robots': 'follow, index', 'ogImage': 'https://www.firecrawl.dev/og.png?123', 'ogTitle': 'Firecrawl', 'sitemap': {'lastmod': '2024-08-12T00:28:16.681Z', 'changefreq': 'weekly'}, 'keywords': 'Firecrawl,Markdown,Data,Mendable,Langchain', 'sourceURL': 'https://www.firecrawl.dev/', 'ogSiteName': 'Firecrawl', 'description': 'Firecrawl crawls and converts any website into clean markdown.', 'ogDescription': 'Turn any website into LLM-ready data.', 'pageStatusCode': 200, 'ogLocaleAlternate': []}, page_content='Introducing [Smart Crawl!](https://www.firecrawl.dev/smart-crawl)\\n Join the waitlist to turn any website into an API with AI\\n\\n\\n\\n[🔥 Firecrawl](/)\\n\\n*   [Playground](/playground)\\n    \\n*   [Docs](https://docs.firecrawl.dev)\\n    \\n*   [Pricing](/pricing)\\n    \\n*   [Blog](/blog)\\n    \\n*   Beta Features\\n\\n[Log In](/signin)\\n[Log In](/signin)\\n[Sign Up](/signin/signup)\\n 8.9k\\n\\n[💥 Get 2 months free with yearly plan](/pricing)\\n\\nTurn websites into  \\n_LLM-ready_ data\\n=====================================\\n\\nPower your AI apps with clean data crawled from any website. It\\'s also open-source.\\n\\nStart for free (500 credits)Start for free[Talk to us](https://calendly.com/d/cj83-ngq-knk/meet-firecrawl)\\n\\nA product by\\n\\n[![Mendable Logo](https://www.firecrawl.dev/images/mendable_logo_transparent.png)Mendable](https://mendable.ai)\\n\\n![Example Webpage](https://www.firecrawl.dev/multiple-websites.png)\\n\\nCrawl, Scrape, Clean\\n--------------------\\n\\nWe crawl all accessible subpages and give you clean markdown for each. No sitemap required.\\n\\n    \\n      [\\\\\\n        {\\\\\\n          \"url\": \"https://www.firecrawl.dev/\",\\\\\\n          \"markdown\": \"## Welcome to Firecrawl\\\\\\n            Firecrawl is a web scraper that allows you to extract the content of a webpage.\"\\\\\\n        },\\\\\\n        {\\\\\\n          \"url\": \"https://www.firecrawl.dev/features\",\\\\\\n          \"markdown\": \"## Features\\\\\\n            Discover how Firecrawl\\'s cutting-edge features can \\\\\\n            transform your data operations.\"\\\\\\n        },\\\\\\n        {\\\\\\n          \"url\": \"https://www.firecrawl.dev/pricing\",\\\\\\n          \"markdown\": \"## Pricing Plans\\\\\\n            Choose the perfect plan that fits your needs.\"\\\\\\n        },\\\\\\n        {\\\\\\n          \"url\": \"https://www.firecrawl.dev/about\",\\\\\\n          \"markdown\": \"## About Us\\\\\\n            Learn more about Firecrawl\\'s mission and the \\\\\\n            team behind our innovative platform.\"\\\\\\n        }\\\\\\n      ]\\n      \\n\\nNote: The markdown has been edited for display purposes.\\n\\nTrusted by Top Companies\\n------------------------\\n\\n[![Customer Logo](https://www.firecrawl.dev/logos/zapier.png)](https://www.zapier.com)\\n\\n[![Customer Logo](https://www.firecrawl.dev/logos/gamma.svg)](https://gamma.app)\\n\\n[![Customer Logo](https://www.firecrawl.dev/logos/nvidia-com.png)](https://www.nvidia.com)\\n\\n[![Customer Logo](https://www.firecrawl.dev/logos/teller-io.svg)](https://www.teller.io)\\n\\n[![Customer Logo](https://www.firecrawl.dev/logos/stackai.svg)](https://www.stack-ai.com)\\n\\n[![Customer Logo](https://www.firecrawl.dev/logos/palladiumdigital-co-uk.svg)](https://www.palladiumdigital.co.uk)\\n\\n[![Customer Logo](https://www.firecrawl.dev/logos/worldwide-casting-com.svg)](https://www.worldwide-casting.com)\\n\\n[![Customer Logo](https://www.firecrawl.dev/logos/open-gov-sg.png)](https://www.open.gov.sg)\\n\\n[![Customer Logo](https://www.firecrawl.dev/logos/bain-com.svg)](https://www.bain.com)\\n\\n[![Customer Logo](https://www.firecrawl.dev/logos/demand-io.svg)](https://www.demand.io)\\n\\n[![Customer Logo](https://www.firecrawl.dev/logos/nocodegarden-io.png)](https://www.nocodegarden.io)\\n\\n[![Customer Logo](https://www.firecrawl.dev/logos/cyberagent-co-jp.svg)](https://www.cyberagent.co.jp)\\n\\nIntegrate today\\n---------------\\n\\nEnhance your applications with top-tier web scraping and crawling capabilities.\\n\\n#### Scrape\\n\\nExtract markdown or structured data from websites quickly and efficiently.\\n\\n#### Crawling\\n\\nNavigate and retrieve data from all accessible subpages, even without a sitemap.\\n\\nNode.js\\n\\nPython\\n\\ncURL\\n\\n1\\n\\n2\\n\\n3\\n\\n4\\n\\n5\\n\\n6\\n\\n7\\n\\n8\\n\\n9\\n\\n10\\n\\n// npm install @mendable/firecrawl-js  \\n  \\nimport FirecrawlApp from \\'@mendable/firecrawl-js\\';  \\n  \\nconst app \\\\= new FirecrawlApp({ apiKey: \"fc-YOUR\\\\_API\\\\_KEY\" });  \\n  \\n// Scrape a website:  \\nconst scrapeResult \\\\= await app.scrapeUrl(\\'firecrawl.dev\\');  \\n  \\nconsole.log(scrapeResult.data.markdown)\\n\\n#### Use well-known tools\\n\\nAlready fully integrated with the greatest existing tools and workflows.\\n\\n[![LlamaIndex](https://www.firecrawl.dev/logos/llamaindex.svg)](https://docs.llamaindex.ai/en/stable/examples/data_connectors/WebPageDemo/#using-firecrawl-reader/)\\n[![Langchain](https://www.firecrawl.dev/integrations/langchain.png)](https://python.langchain.com/docs/integrations/document_loaders/firecrawl/)\\n[![Dify](https://www.firecrawl.dev/logos/dify.png)](https://dify.ai/blog/dify-ai-blog-integrated-with-firecrawl/)\\n[![Dify](https://www.firecrawl.dev/integrations/langflow_2.png)](https://www.langflow.org/)\\n[![Flowise](https://www.firecrawl.dev/integrations/flowise.png)](https://flowiseai.com/)\\n[![CrewAI](https://www.firecrawl.dev/integrations/crewai.png)](https://crewai.com/)\\n\\n#### Start for free, scale easily\\n\\nKick off your journey for free and scale seamlessly as your project expands.\\n\\n[Try it out](/signin/signup)\\n\\n#### Open-source\\n\\nDeveloped transparently and collaboratively. Join our community of contributors.\\n\\n[Check out our repo](https://github.com/mendableai/firecrawl)\\n\\nWe handle the hard stuff\\n------------------------\\n\\nRotating proxies, caching, rate limits, js-blocked content and more\\n\\n#### Crawling\\n\\nFirecrawl crawls all accessible subpages, even without a sitemap.\\n\\n#### Dynamic content\\n\\nFirecrawl gathers data even if a website uses javascript to render content.\\n\\n#### To Markdown\\n\\nFirecrawl returns clean, well formatted markdown - ready for use in LLM applications\\n\\n#### Crawling Orchestration\\n\\nFirecrawl orchestrates the crawling process in parallel for the fastest results.\\n\\n#### Caching\\n\\nFirecrawl caches content, so you don\\'t have to wait for a full scrape unless new content exists.\\n\\n#### Built for AI\\n\\nBuilt by LLM engineers, for LLM engineers. Giving you clean data the way you want it.\\n\\nOur wall of love\\n\\nDon\\'t take our word for it\\n--------------------------\\n\\n![Greg Kamradt](https://www.firecrawl.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Ftestimonial-02.0afeb750.jpg&w=96&q=75)\\n\\nGreg Kamradt\\n\\n[@GregKamradt](https://twitter.com/GregKamradt/status/1780300642197840307)\\n\\nLLM structured data via API, handling requests, cleaning, and crawling. Enjoyed the early preview.\\n\\n![Amit Naik](https://www.firecrawl.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Ftestimonial-03.ff5dbe11.jpg&w=96&q=75)\\n\\nAmit Naik\\n\\n[@suprgeek](https://twitter.com/suprgeek/status/1780338213351035254)\\n\\n#llm success with RAG relies on Retrieval. Firecrawl by @mendableai structures web content for processing. 👏\\n\\n![Jerry Liu](https://www.firecrawl.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Ftestimonial-04.76bef0df.jpg&w=96&q=75)\\n\\nJerry Liu\\n\\n[@jerryjliu0](https://twitter.com/jerryjliu0/status/1781122933349572772)\\n\\nFirecrawl is awesome 🔥 Turns web pages into structured markdown for LLM apps, thanks to @mendableai.\\n\\n![Bardia Pourvakil](https://www.firecrawl.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Ftestimonial-01.025350bc.jpeg&w=96&q=75)\\n\\nBardia Pourvakil\\n\\n[@thepericulum](https://twitter.com/thepericulum/status/1781397799487078874)\\n\\nThese guys ship. I wanted types for their node SDK, and less than an hour later, I got them. Can\\'t recommend them enough.\\n\\n![latentsauce 🧘🏽](https://www.firecrawl.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Ftestimonial-07.c2285d35.jpeg&w=96&q=75)\\n\\nlatentsauce 🧘🏽\\n\\n[@latentsauce](https://twitter.com/latentsauce/status/1781738253927735331)\\n\\nFirecrawl simplifies data preparation significantly, exactly what I was hoping for. Thank you for creating Firecrawl ❤️❤️❤️\\n\\n![Greg Kamradt](https://www.firecrawl.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Ftestimonial-02.0afeb750.jpg&w=96&q=75)\\n\\nGreg Kamradt\\n\\n[@GregKamradt](https://twitter.com/GregKamradt/status/1780300642197840307)\\n\\nLLM structured data via API, handling requests, cleaning, and crawling. Enjoyed the early preview.\\n\\n![Amit Naik](https://www.firecrawl.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Ftestimonial-03.ff5dbe11.jpg&w=96&q=75)\\n\\nAmit Naik\\n\\n[@suprgeek](https://twitter.com/suprgeek/status/1780338213351035254)\\n\\n#llm success with RAG relies on Retrieval. Firecrawl by @mendableai structures web content for processing. 👏\\n\\n![Jerry Liu](https://www.firecrawl.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Ftestimonial-04.76bef0df.jpg&w=96&q=75)\\n\\nJerry Liu\\n\\n[@jerryjliu0](https://twitter.com/jerryjliu0/status/1781122933349572772)\\n\\nFirecrawl is awesome 🔥 Turns web pages into structured markdown for LLM apps, thanks to @mendableai.\\n\\n![Bardia Pourvakil](https://www.firecrawl.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Ftestimonial-01.025350bc.jpeg&w=96&q=75)\\n\\nBardia Pourvakil\\n\\n[@thepericulum](https://twitter.com/thepericulum/status/1781397799487078874)\\n\\nThese guys ship. I wanted types for their node SDK, and less than an hour later, I got them. Can\\'t recommend them enough.\\n\\n![latentsauce 🧘🏽](https://www.firecrawl.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Ftestimonial-07.c2285d35.jpeg&w=96&q=75)\\n\\nlatentsauce 🧘🏽\\n\\n[@latentsauce](https://twitter.com/latentsauce/status/1781738253927735331)\\n\\nFirecrawl simplifies data preparation significantly, exactly what I was hoping for. Thank you for creating Firecrawl ❤️❤️❤️\\n\\n![Michael Ning](https://www.firecrawl.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Ftestimonial-05.76d7cd3e.png&w=96&q=75)\\n\\nMichael Ning\\n\\n[](#)\\n\\nFirecrawl is impressive, saving us 2/3 the tokens and allowing gpt3.5turbo use over gpt4. Major savings in time and money.\\n\\n![Alex Reibman 🖇️](https://www.firecrawl.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Ftestimonial-06.4ee7cf5a.jpeg&w=96&q=75)\\n\\nAlex Reibman 🖇️\\n\\n[@AlexReibman](https://twitter.com/AlexReibman/status/1780299595484131836)\\n\\nMoved our internal agent\\'s web scraping tool from Apify to Firecrawl because it benchmarked 50x faster with AgentOps.\\n\\n![Michael](https://www.firecrawl.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Ftestimonial-08.0bed40be.jpeg&w=96&q=75)\\n\\nMichael\\n\\n[@michael\\\\_chomsky](#)\\n\\nI really like some of the design decisions Firecrawl made, so I really want to share with others.\\n\\n![Paul Scott](https://www.firecrawl.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Ftestimonial-09.d303b5b4.png&w=96&q=75)\\n\\nPaul Scott\\n\\n[@palebluepaul](https://twitter.com/palebluepaul)\\n\\nAppreciating your lean approach, Firecrawl ticks off everything on our list without the cost prohibitive overkill.\\n\\n![Michael Ning](https://www.firecrawl.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Ftestimonial-05.76d7cd3e.png&w=96&q=75)\\n\\nMichael Ning\\n\\n[](#)\\n\\nFirecrawl is impressive, saving us 2/3 the tokens and allowing gpt3.5turbo use over gpt4. Major savings in time and money.\\n\\n![Alex Reibman 🖇️](https://www.firecrawl.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Ftestimonial-06.4ee7cf5a.jpeg&w=96&q=75)\\n\\nAlex Reibman 🖇️\\n\\n[@AlexReibman](https://twitter.com/AlexReibman/status/1780299595484131836)\\n\\nMoved our internal agent\\'s web scraping tool from Apify to Firecrawl because it benchmarked 50x faster with AgentOps.\\n\\n![Michael](https://www.firecrawl.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Ftestimonial-08.0bed40be.jpeg&w=96&q=75)\\n\\nMichael\\n\\n[@michael\\\\_chomsky](#)\\n\\nI really like some of the design decisions Firecrawl made, so I really want to share with others.\\n\\n![Paul Scott](https://www.firecrawl.dev/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Ftestimonial-09.d303b5b4.png&w=96&q=75)\\n\\nPaul Scott\\n\\n[@palebluepaul](https://twitter.com/palebluepaul)\\n\\nAppreciating your lean approach, Firecrawl ticks off everything on our list without the cost prohibitive overkill.\\n\\nFlexible Pricing\\n----------------\\n\\nStart for free, then scale as you grow\\n\\nYearly (17% off)Yearly (2 months free)Monthly\\n\\nFree Plan\\n---------\\n\\n500 credits\\n\\n$0/month\\n\\n*   Scrape 500 pages\\n*   5 /scrape per min\\n*   1 /crawl per min\\n\\nGet Started\\n\\nHobby\\n-----\\n\\n3,000 credits\\n\\n$16/month\\n\\n*   Scrape 3,000 pages\\n*   10 /scrape per min\\n*   3 /crawl per min\\n\\nSubscribe\\n\\nStandardMost Popular\\n--------------------\\n\\n100,000 credits\\n\\n$83/month\\n\\n*   Scrape 100,000 pages\\n*   50 /scrape per min\\n*   10 /crawl per min\\n\\nSubscribe\\n\\nGrowth\\n------\\n\\n500,000 credits\\n\\n$333/month\\n\\n*   Scrape 500,000 pages\\n*   500 /scrape per min\\n*   50 /crawl per min\\n*   Priority Support\\n\\nSubscribe\\n\\nEnterprise Plan\\n---------------\\n\\nUnlimited credits. Custom RPMs.\\n\\nTalk to us\\n\\n*   Top priority support\\n*   Feature Acceleration\\n*   SLAs\\n*   Account Manager\\n*   Custom rate limits volume\\n*   Custom concurrency limits\\n*   Beta features access\\n*   CEO\\'s number\\n\\n\\\\* a /scrape refers to the [scrape](https://docs.firecrawl.dev/api-reference/endpoint/scrape)\\n API endpoint.\\n\\n\\\\* a /crawl refers to the [crawl](https://docs.firecrawl.dev/api-reference/endpoint/crawl)\\n API endpoint.\\n\\nScrape Credits\\n--------------\\n\\nScrape credits are consumed for each API request, varying by endpoint and feature.\\n\\n| Features | Credits per page |\\n| --- | --- |\\n| Scrape(/scrape) | 1   |\\n| Crawl(/crawl) | 1   |\\n| Search(/search) | 1   |\\n| Scrape + LLM extraction (/scrape) | 50  |\\n\\n[🔥](/)\\n\\nReady to _Build?_\\n-----------------\\n\\nStart scraping web data for your AI apps today.  \\nNo credit card needed.\\n\\n[Get Started](/signin)\\n\\n[Talk to us](https://calendly.com/d/cj83-ngq-knk/meet-firecrawl)\\n\\nFAQ\\n---\\n\\nFrequently asked questions about Firecrawl\\n\\n#### General\\n\\nWhat is Firecrawl?\\n\\nFirecrawl turns entire websites into clean, LLM-ready markdown or structured data. Scrape, crawl and extract the web with a single API. Ideal for AI companies looking to empower their LLM applications with web data.\\n\\nWhat sites work?\\n\\nFirecrawl is best suited for business websites, docs and help centers. We currently don\\'t support social media platforms.\\n\\nWho can benefit from using Firecrawl?\\n\\nFirecrawl is tailored for LLM engineers, data scientists, AI researchers, and developers looking to harness web data for training machine learning models, market research, content aggregation, and more. It simplifies the data preparation process, allowing professionals to focus on insights and model development.\\n\\nIs Firecrawl open-source?\\n\\nYes, it is. You can check out the repository on GitHub. Keep in mind that this repository is currently in its early stages of development. We are in the process of merging custom modules into this mono repository.\\n\\n#### Scraping & Crawling\\n\\nHow does Firecrawl handle dynamic content on websites?\\n\\nUnlike traditional web scrapers, Firecrawl is equipped to handle dynamic content rendered with JavaScript. It ensures comprehensive data collection from all accessible subpages, making it a reliable tool for scraping websites that rely heavily on JS for content delivery.\\n\\nWhy is it not crawling all the pages?\\n\\nThere are a few reasons why Firecrawl may not be able to crawl all the pages of a website. Some common reasons include rate limiting, and anti-scraping mechanisms, disallowing the crawler from accessing certain pages. If you\\'re experiencing issues with the crawler, please reach out to our support team at hello@firecrawl.com.\\n\\nCan Firecrawl crawl websites without a sitemap?\\n\\nYes, Firecrawl can access and crawl all accessible subpages of a website, even in the absence of a sitemap. This feature enables users to gather data from a wide array of web sources with minimal setup.\\n\\nWhat formats can Firecrawl convert web data into?\\n\\nFirecrawl specializes in converting web data into clean, well-formatted markdown. This format is particularly suited for LLM applications, offering a structured yet flexible way to represent web content.\\n\\nHow does Firecrawl ensure the cleanliness of the data?\\n\\nFirecrawl employs advanced algorithms to clean and structure the scraped data, removing unnecessary elements and formatting the content into readable markdown. This process ensures that the data is ready for use in LLM applications without further preprocessing.\\n\\nIs Firecrawl suitable for large-scale data scraping projects?\\n\\nAbsolutely. Firecrawl offers various pricing plans, including a Scale plan that supports scraping of millions of pages. With features like caching and scheduled syncs, it\\'s designed to efficiently handle large-scale data scraping and continuous updates, making it ideal for enterprises and large projects.\\n\\nDoes it respect robots.txt?\\n\\nYes, Firecrawl crawler respects the rules set in a website\\'s robots.txt file. If you notice any issues with the way Firecrawl interacts with your website, you can adjust the robots.txt file to control the crawler\\'s behavior. Firecrawl user agent name is \\'FirecrawlAgent\\'. If you notice any behavior that is not expected, please let us know at hello@firecrawl.com.\\n\\nWhat measures does Firecrawl take to handle web scraping challenges like rate limits and caching?\\n\\nFirecrawl is built to navigate common web scraping challenges, including reverse proxies, rate limits, and caching. It smartly manages requests and employs caching techniques to minimize bandwidth usage and avoid triggering anti-scraping mechanisms, ensuring reliable data collection.\\n\\nDoes Firecrawl handle captcha or authentication?\\n\\nFirecrawl avoids captcha by using stealth proxyies. When it encounters captcha, it attempts to solve it automatically, but this is not always possible. We are working to add support for more captcha solving methods. Firecrawl can handle authentication by providing auth headers to the API.\\n\\n#### API Related\\n\\nWhere can I find my API key?\\n\\nClick on the dashboard button on the top navigation menu when logged in and you will find your API key in the main screen and under API Keys.\\n\\n#### Billing\\n\\nIs Firecrawl free?\\n\\nFirecrawl is free for the first 500 scraped pages (500 free credits). After that, you can upgrade to our Standard or Scale plans for more credits.\\n\\nIs there a pay per use plan instead of monthly?\\n\\nNo we do not currently offer a pay per use plan, instead you can upgrade to our Standard or Growth plans for more credits and higher rate limits.\\n\\nHow many credit does scraping, crawling, and extraction cost?\\n\\nScraping costs 1 credit per page. Crawling costs 1 credit per page.\\n\\nDo you charge for failed requests (scrape, crawl, extract)?\\n\\nWe do not charge for any failed requests (scrape, crawl, extract). Please contact support at help@firecrawl.dev if you have any questions.\\n\\nWhat payment methods do you accept?\\n\\nWe accept payments through Stripe which accepts most major credit cards, debit cards, and PayPal.\\n\\n[🔥](/)\\n\\n© A product by Mendable.ai - All rights reserved.\\n\\n[StatusStatus](https://firecrawl.betteruptime.com)\\n[Terms of ServiceTerms of Service](/terms-of-service)\\n[Privacy PolicyPrivacy Policy](/privacy-policy)\\n\\n[Twitter](https://twitter.com/mendableai)\\n[GitHub](https://github.com/mendableai)\\n[Discord](https://discord.gg/gSmWdAkdwd)\\n\\n###### Helpful Links\\n\\n*   [Status](https://firecrawl.betteruptime.com/)\\n    \\n*   [Pricing](/pricing)\\n    \\n*   [Blog](https://www.firecrawl.dev/blog)\\n    \\n*   [Docs](https://docs.firecrawl.dev)\\n    \\n\\nBacked by![Y Combinator Logo](https://www.firecrawl.dev/images/yc.svg)\\n\\n![SOC 2 Type II](https://www.firecrawl.dev/soc2type2badge.png)\\n\\n###### Resources\\n\\n*   [Community](#0)\\n    \\n*   [Terms of service](#0)\\n    \\n*   [Collaboration features](#0)\\n    \\n\\n###### Legals\\n\\n*   [Refund policy](#0)\\n    \\n*   [Terms & Conditions](#0)\\n    \\n*   [Privacy policy](#0)\\n    \\n*   [Brand Kit](#0)')\n",
      "\n",
      "```\n",
      "\n",
      "```codeBlockLines_e6Vv\n",
      "print(docs[0].metadata)\n",
      "\n",
      "```\n",
      "\n",
      "```codeBlockLines_e6Vv\n",
      "{'ogUrl': 'https://www.firecrawl.dev/', 'title': 'Home - Firecrawl', 'robots': 'follow, index', 'ogImage': 'https://www.firecrawl.dev/og.png?123', 'ogTitle': 'Firecrawl', 'sitemap': {'lastmod': '2024-08-12T00:28:16.681Z', 'changefreq': 'weekly'}, 'keywords': 'Firecrawl,Markdown,Data,Mendable,Langchain', 'sourceURL': 'https://www.firecrawl.dev/', 'ogSiteName': 'Firecrawl', 'description': 'Firecrawl crawls and converts any website into clean markdown.', 'ogDescription': 'Turn any website into LLM-ready data.', 'pageStatusCode': 200, 'ogLocaleAlternate': []}\n",
      "\n",
      "```\n",
      "\n",
      "## Lazy Load [​](\\#lazy-load \"Direct link to Lazy Load\")\n",
      "\n",
      "You can use lazy loading to minimize memory requirements.\n",
      "\n",
      "```codeBlockLines_e6Vv\n",
      "pages = []\n",
      "for doc in loader.lazy_load():\n",
      "    pages.append(doc)\n",
      "    if len(pages) >= 10:\n",
      "        # do some paged operation, e.g.\n",
      "        # index.upsert(page)\n",
      "\n",
      "        pages = []\n",
      "\n",
      "```\n",
      "\n",
      "```codeBlockLines_e6Vv\n",
      "len(pages)\n",
      "\n",
      "```\n",
      "\n",
      "```codeBlockLines_e6Vv\n",
      "8\n",
      "\n",
      "```\n",
      "\n",
      "```codeBlockLines_e6Vv\n",
      "print(pages[0].page_content[:100])\n",
      "print(pages[0].metadata)\n",
      "\n",
      "```\n",
      "\n",
      "```codeBlockLines_e6Vv\n",
      "Introducing [Smart Crawl!](https://www.firecrawl.dev/smart-crawl)\n",
      " Join the waitlist to turn any web\n",
      "{'ogUrl': 'https://www.firecrawl.dev/blog/introducing-fire-engine-for-firecrawl', 'title': 'Introducing Fire Engine for Firecrawl', 'robots': 'follow, index', 'ogImage': 'https://www.firecrawl.dev/images/blog/fire-engine-launch.png', 'ogTitle': 'Introducing Fire Engine for Firecrawl', 'sitemap': {'lastmod': '2024-08-06T00:00:00.000Z', 'changefreq': 'weekly'}, 'keywords': 'firecrawl,fireengine,web crawling,dashboard,web scraping,LLM,data extraction', 'sourceURL': 'https://www.firecrawl.dev/blog/introducing-fire-engine-for-firecrawl', 'ogSiteName': 'Firecrawl', 'description': 'The most scalable, reliable, and fast way to get web data for Firecrawl.', 'ogDescription': 'The most scalable, reliable, and fast way to get web data for Firecrawl.', 'pageStatusCode': 200, 'ogLocaleAlternate': []}\n",
      "\n",
      "```\n",
      "\n",
      "## Crawler Options [​](\\#crawler-options \"Direct link to Crawler Options\")\n",
      "\n",
      "You can also pass `params` to the loader. This is a dictionary of options to pass to the crawler. See the [FireCrawl API documentation](https://github.com/mendableai/firecrawl-py) for more information.\n",
      "\n",
      "## API reference [​](\\#api-reference \"Direct link to API reference\")\n",
      "\n",
      "For detailed documentation of all `FireCrawlLoader` features and configurations head to the API reference: [https://python.langchain.com/api\\_reference/community/document\\_loaders/langchain\\_community.document\\_loaders.firecrawl.FireCrawlLoader.html](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.firecrawl.FireCrawlLoader.html)\n",
      "\n",
      "## Related [​](\\#related \"Direct link to Related\")\n",
      "\n",
      "- Document loader [conceptual guide](/docs/concepts/#document-loaders)\n",
      "- Document loader [how-to guides](/docs/how_to/#document-loaders)\n",
      "\n",
      "[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/integrations/document_loaders/firecrawl.ipynb)\n",
      "\n",
      "* * *\n",
      "\n",
      "#### Was this page helpful?\n",
      "\n",
      "#### You can also leave detailed feedback [on GitHub](https://github.com/langchain-ai/langchain/issues/new?assignees=&labels=03+-+Documentation&projects=&template=documentation.yml&title=DOC%3A+%3CIssue+related+to+/docs/integrations/document_loaders/firecrawl/%3E&url=https://python.langchain.com/docs/integrations/document_loaders/firecrawl/).\n",
      "\n",
      "[Previous\\\\\n",
      "\\\\\n",
      "Figma](/docs/integrations/document_loaders/figma/) [Next\\\\\n",
      "\\\\\n",
      "Geopandas](/docs/integrations/document_loaders/geopandas/)\n",
      "\n",
      "- [Overview](#overview)\n",
      "  - [Integration details](#integration-details)\n",
      "  - [Loader features](#loader-features)\n",
      "- [Setup](#setup)\n",
      "  - [Credentials](#credentials)\n",
      "  - [Installation](#installation)\n",
      "- [Initialization](#initialization)\n",
      "  - [Modes](#modes)\n",
      "- [Load](#load)\n",
      "- [Lazy Load](#lazy-load)\n",
      "- [Crawler Options](#crawler-options)\n",
      "- [API reference](#api-reference)\n",
      "- [Related](#related)\n",
      "\n",
      "Community\n",
      "\n",
      "- [Twitter](https://twitter.com/LangChainAI)\n",
      "\n",
      "GitHub\n",
      "\n",
      "- [Organization](https://github.com/langchain-ai)\n",
      "- [Python](https://github.com/langchain-ai/langchain)\n",
      "- [JS/TS](https://github.com/langchain-ai/langchainjs)\n",
      "\n",
      "More\n",
      "\n",
      "- [Homepage](https://langchain.com)\n",
      "- [Blog](https://blog.langchain.dev)\n",
      "- [YouTube](https://www.youtube.com/@LangChain)\n",
      "\n",
      "Copyright © 2024 LangChain, Inc.\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ArxivLoader\n",
    "\n",
    "arXiv is an open-access archive for 2 million scholarly articles in the fields of physics, mathematics, computer science, quantitative biology, quantitative finance, statistics, electrical engineering and systems science, and economics.\n",
    "\n",
    "To access Arxiv document loader you'll need to install the arxiv, PyMuPDF and langchain-community integration packages. PyMuPDF transforms PDF files downloaded from the arxiv.org site into the text format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='SOD-YOLOv8 - Enhancing YOLOv8 for Small Object Detection in\n",
      "Traffic Scenes\n",
      "Boshra Khalili1 and Andrew W.Smyth2\n",
      "Abstract— Object detection as part of computer vision can\n",
      "be crucial for traffic management, emergency response, au-\n",
      "tonomous vehicles, and smart cities. Despite significant ad-\n",
      "vances in object detection, detecting small objects in images\n",
      "captured by distant cameras remains challenging due to their\n",
      "size, distance from the camera, varied shapes, and cluttered\n",
      "backgrounds. To address these challenges, we propose Small\n",
      "Object Detection YOLOv8 (SOD-YOLOv8), a novel model\n",
      "specifically designed for scenarios involving numerous small\n",
      "objects. Inspired by Efficient Generalized Feature Pyramid Net-\n",
      "works (GFPN), we enhance multi-path fusion within YOLOv8\n",
      "to integrate features across different levels, preserving details\n",
      "from shallower layers and improving small object detection\n",
      "accuracy. Additionally, a fourth detection layer is introduced\n",
      "to utilize high-resolution spatial information effectively. The\n",
      "Efficient Multi-Scale Attention Module (EMA) in the C2f-EMA\n",
      "module enhances feature extraction by redistributing weights\n",
      "and prioritizing relevant features. We introduce Powerful-IoU\n",
      "(PIoU) as a replacement for CIoU, focusing on moderate-\n",
      "quality anchor boxes and adding a penalty based on differences\n",
      "between predicted and ground truth bounding box corners.\n",
      "This approach simplifies calculations, speeds up convergence,\n",
      "and enhances detection accuracy. SOD-YOLOv8 significantly\n",
      "improves small object detection, surpassing widely used models\n",
      "in various metrics, without substantially increasing computa-\n",
      "tional cost or latency compared to YOLOv8s. Specifically, it\n",
      "increases recall from 40.1% to 43.9%, precision from 51.2%\n",
      "to 53.9%, mAP0.5 from 40.6% to 45.1%, and mAP0.5:0.95\n",
      "from 24% to 26.6%. In dynamic real-world traffic scenes,\n",
      "SOD-YOLOv8 demonstrated notable improvements in diverse\n",
      "conditions, proving its reliability and effectiveness in detecting\n",
      "small objects even in challenging environments.\n",
      "I. INTRODUCTION\n",
      "Object detection in computer vision plays a crucial role\n",
      "across various fields, including Autonomous Vehicles [1],\n",
      "[2], [3], traffic scene monitoring [4], [5], enhancing intelli-\n",
      "gent driving systems [6], and facilitating search and rescue\n",
      "missions [7]. Accurate detection of small objects such as\n",
      "pedestrians, vehicles, motorcycles, bicycles, traffic signs, and\n",
      "lights is crucial for safe navigation and decision-making\n",
      "in autonomous vehicles and intelligent driving systems [1],\n",
      "[3]. Furthermore, detecting small objects enhances traffic\n",
      "flow management, pedestrian safety, and overall traffic scene\n",
      "analysis. This capability is essential for improving urban\n",
      "planning and transportation systems [4], [5].\n",
      "1Boshra Khalili is Graduate Research Assistant in the Department of\n",
      "Civil Engineering and Engineering Mechanics, Columbia University, New\n",
      "York, NY 10027, USA bk2898@columbia.edu\n",
      "2Andrew W. Smyth (corresponding author) is the Robert A. W. and Chris-\n",
      "tine S. Carleton Professor of Civil Engineering and Engineering Mechanics\n",
      "and the Director of the Center for the Smart Streetscapes (CS3), Columbia\n",
      "University, New York, NY 10027, USA aws16@columbia.edu\n",
      "As the cost of UAV production decreases and flight\n",
      "control techniques advance, these small, flexible devices are\n",
      "increasingly used for intelligent traffic monitoring [8]. UAVs\n",
      "typically operate at higher altitudes to capture broader views,\n",
      "which reduces the apparent size of ground objects due to\n",
      "greater distances. This distance complicates object detection\n",
      "within captured images [8]. Despite significant progress in\n",
      "object detection, detecting small objects such as pedestrians,\n",
      "motorcycles, bicycles, and vehicles in urban traffic remains\n",
      "challenging due to their size, varied shapes, and cluttered\n",
      "backgrounds. This challenge is further amplified when work-\n",
      "ing with limited hardware resources in computer vision and\n",
      "object detection.\n",
      "Small objects, which occupy a small portion of an image\n",
      "and have lower resolution and less distinct visual charac-\n",
      "teristics compared to larger objects, are more challenging to\n",
      "detect accurately. Moreover, shallow layers in networks such\n",
      "as YOLOv8 may filter out essential spatial details required\n",
      "for detecting these small objects, resulting in data loss.\n",
      "Additionally, smaller objects can be overshadowed by larger\n",
      "ones during feature extraction, potentially causing the loss of\n",
      "relevant details crucial for accurate detection. Overcoming\n",
      "these challenges is crucial for improving overall detection\n",
      "accuracy and reliability in real-world scenarios.\n",
      "To address small object detection in UAV aerial pho-\n",
      "tography and traffic scenes, we introduce a novel model\n",
      "based on YOLOv8. Our model integrates multi-scale spatial\n",
      "and contextual information using an enhanced GFPN [9].We\n",
      "integrate EMA Attention [10] into the C2f module to ensure\n",
      "that small object features are given sufficient emphasis. We\n",
      "also include a fourth detection layer to utilize high-resolution\n",
      "spatial details effectively. Furthermore, due to the significant\n",
      "impact of bounding box regression in object detection, we\n",
      "used the PIoU method, which enhances performance and\n",
      "reduces convergence time by incorporating an improved\n",
      "penalty term and attention mechanism. Our key contributions\n",
      "are as follows:\n",
      "• Inspired by the Efficient RepGFPN in DAMO-YOLO\n",
      "models [11], we enhance multi-path fusion within\n",
      "the YOLOv8 architecture. This enhancement facilitates\n",
      "better fusion of features across different levels and\n",
      "simplifies the GFPN structure through reparameteriza-\n",
      "tion. By preserving crucial information from shallower\n",
      "layers, our approach significantly improves detection\n",
      "accuracy, especially for small objects. Additionally, we\n",
      "add a fourth detection layer to effectively leverage high-\n",
      "resolution and detailed spatial information.\n",
      "• We integrate a C2f-EMA structure into the network,\n",
      "arXiv:2408.04786v1  [cs.CV]  8 Aug 2024\n",
      "leveraging the Efficient Multi-Scale Attention Module\n",
      "to replace C2f in the neck layers. This enhancement\n",
      "improves feature extraction by redistributing feature\n",
      "weights, prioritizing relevant features and spatial details\n",
      "across image channels. Consequently, it enhances the\n",
      "network’s ability to detect targets of different sizes.\n",
      "• We use PIoU to replace CIoU in the original network.\n",
      "PIoU enhances existing IoU-based loss functions by\n",
      "better balancing difficult and easy samples, with a\n",
      "specific focus on anchor boxes of moderate quality.\n",
      "It incorporates a penalty term based on differences\n",
      "between predicted and ground truth bounding box cor-\n",
      "ners, improving the efficiency and accuracy of bounding\n",
      "box regression. PIoU also simplifies calculations by\n",
      "requiring only one hyperparameter, leading to faster\n",
      "convergence and enhanced object detection accuracy.\n",
      "• We conduct visual analyses on various challenging sce-\n",
      "narios to demonstrate the effectiveness of our proposed\n",
      "approach in enhancing small object detection. Addi-\n",
      "tionally, we perform experiments in real-world traffic\n",
      "scenes using images captured from cameras mounted on\n",
      "buildings. These images contain numerous small objects\n",
      "and help validate our enhanced model for small object\n",
      "detection.\n",
      "The structure of the remaining sections of this paper is as\n",
      "follows: Section 2 discusses related work. Section 3 provides\n",
      "an overview of the YOLOv8 network architecture. Section\n",
      "4 details the proposed enhanced YOLOv8. Section 5 covers\n",
      "our experimental setup and result analysis. Finally, Section\n",
      "6 concludes the paper.\n",
      "II. RELATED WORK\n",
      "Small object detection has been a significant challenge in\n",
      "the field of computer vision, particularly in traffic scenarios.\n",
      "This section reviews mainstream object detection algorithms,\n",
      "recent advancements in small object detection, and specific\n",
      "enhancements made to the YOLO framework.\n",
      "Mainstream object detection algorithms predominantly use\n",
      "deep learning techniques, categorized into two types: two-\n",
      "stage and one-stage methods. Two-stage methods process\n",
      "candidate frames with a classifier and perform deep learning\n",
      "on corresponding frames [13]. Typical two-stage detection\n",
      "algorithms include R-CNN [13], Fast R-CNN [14], and\n",
      "Faster R-CNN [15]. The R-CNN family is a classic two-\n",
      "stage algorithm known for high detection accuracy but faces\n",
      "challenges such as slow speed, training complexity, and\n",
      "optimization [16]. One-stage detectors, like the YOLO series\n",
      "[17], [18] and SSD [19], use a single neural network to\n",
      "predict box coordinates and categories in one pass. Conse-\n",
      "quently, single-stage networks excel in applications where\n",
      "speed is crucial. However, they sacrifice some accuracy.\n",
      "Despite advancements in speed, these methods struggle with\n",
      "accuracy due to the multi-scale nature of objects and the\n",
      "prevalence of small objects in UAV and traffic scenes.\n",
      "Recent research has focused on improving small object\n",
      "detection in UAV aerial photography and traffic scenarios,\n",
      "which is challenging due to their lower resolution and less\n",
      "distinct visual characteristics compared to larger objects.\n",
      "Studies have explored diverse backbone architectures to\n",
      "enhance feature representation, reduce false positives, and\n",
      "extract relevant features from complex backgrounds in UAV\n",
      "imagery.\n",
      "Liu et al. [20] introduced a model for small target detection\n",
      "in UAV images, addressing leakage and false positives by\n",
      "integrating ResNet units and optimizing convolutional op-\n",
      "erations to expand the network’s receptive field. Liu et al.\n",
      "[21] proposed CBSSD, a specialized detector for small object\n",
      "detection in UAV traffic images. By integrating ResNet50’s\n",
      "lower-level features with VGG16, CBSSD improves feature\n",
      "representation, enhances object recognition accuracy, and\n",
      "reduces false positives under challenging lighting condi-\n",
      "tions. Additionally, Liu et al. [22] utilized Multi-branch\n",
      "Parallel Feature Pyramid Networks (MPFPN) and SSAM\n",
      "for detecting small objects in UAV images. Their approach\n",
      "enhances feature extraction through MPFPN for deep layer\n",
      "detail recovery, while SSAM reduces background noise,\n",
      "significantly boosting accuracy. Experimental results on the\n",
      "VisDrone-DET dataset [23] showcase their method’s com-\n",
      "petitive performance.\n",
      "Adaptations and optimizations within the YOLO frame-\n",
      "work have also been explored to address challenges in small\n",
      "object detection. Lai et al. [24] introduced STC-YOLO, a\n",
      "specialized variant of YOLOv5 designed for challenging\n",
      "traffic sign detection. Their improvements include refined\n",
      "down-sampling, a dedicated small object detection layer,\n",
      "and a CNN-based feature extraction module with multi-\n",
      "head attention. STC-YOLO demonstrated a significant 9.3%\n",
      "improvement in mean Average Precision (mAP) compared\n",
      "to YOLOv5 on benchmark datasets.\n",
      "Further enhancements have been made to YOLOv8, focus-\n",
      "ing on improving backbone architectures, integrating atten-\n",
      "tion mechanisms to focus on relevant features and suppress\n",
      "irrelevant ones, and modifying loss functions. Shen et al. [25]\n",
      "introduced DS-YOLOv8 to enhance small object detection\n",
      "within images by integrating Deformable Convolution C2f\n",
      "(DCN C2f) and Self-Calibrating Shuffle Attention (SC SA)\n",
      "for adaptive feature adjustment, alongside Wise-IoU [26] and\n",
      "position regression loss to boost performance. Experimental\n",
      "results across diverse datasets show significant enhancements\n",
      "in mAP0.5. Wang et al. [8] improved YOLOv8 for UAV\n",
      "aerial photography with a BiFormer attention mechanism for\n",
      "focusing on important information and FFNB for effective\n",
      "multiscale feature fusion. This resulted in a significant 7.7%\n",
      "increase in mean detection accuracy over baseline mod-\n",
      "els, surpassing widely-used alternatives in detecting small\n",
      "objects. This marks substantial progress in UAV object\n",
      "detection, though it necessitates further optimization due to\n",
      "increased computational complexity from additional detec-\n",
      "tion layers.\n",
      "Wang et al. [27] improved YOLOv8 for detecting targets\n",
      "in remote sensing images, focusing on complex backgrounds\n",
      "and diverse small targets. They introduced a small target\n",
      "detection layer and incorporated a C2f-E structure using\n",
      "the EMA attention module. Experimental results on the\n",
      "DOTAv1.0 dataset [28] demonstrate a notable 1.3% increase\n",
      "in mAP0.5 to 82.7%, highlighting significant advancements\n",
      "in target detection accuracy. However, their approach intro-\n",
      "duces increased computational complexity. Xu et al. [29] in-\n",
      "troduced YOLOv8-MPEB, specialized for small target detec-\n",
      "tion in UAV images, addressing scale variations and complex\n",
      "scenes. Enhancements include replacing CSPDarknet53 [30]\n",
      "with MobileNetV3 for efficiency, integrating Efficient Multi-\n",
      "Scale Attention in C2f for better feature extraction, and in-\n",
      "corporating Bidirectional Feature Pyramid Network (BiFPN)\n",
      "[31] in the Neck segment for enhanced adaptability. Exper-\n",
      "imental results on a custom dataset demonstrate YOLOv8-\n",
      "MPEB achieving a 91.9% mAP, a 2.2% improvement over\n",
      "standard YOLOv8, while reducing parameters by 34% and\n",
      "model size by 32%. However, accurately detecting dense\n",
      "small targets remains a challenge.\n",
      "Despite advancements in the reviewed studies, small ob-\n",
      "ject detection methods still face challenges in UAV aerial\n",
      "photography and traffic scenarios. These methods primarily\n",
      "focus on feature fusion but often neglect inner block con-\n",
      "nections. In contrast, our approach integrates an optimized\n",
      "GFPN, inspired by Efficient-RepGFPN, into YOLOv8. This\n",
      "enhancement incorporates skip connections and queen fusion\n",
      "structures to improve efficacy without significantly increas-\n",
      "ing computational complexity or latency. Additionally, the\n",
      "introduced C2f-EMA module enhances feature extraction\n",
      "by redistributing feature weights using the EMA attention\n",
      "mechanism. Unlike other attention mechanisms, it overcomes\n",
      "limitations such as neglecting interactions among spatial de-\n",
      "tails and the limited receptive field of 1x1 kernel convolution,\n",
      "which limits local cross-channel interaction and contextual\n",
      "information modeling.\n",
      "Furthermore, our method avoids the enlargement issues\n",
      "common in other bounding box regression methods. The\n",
      "used PIoU loss function effectively guides anchor boxes\n",
      "during training, resulting in faster convergence and demon-\n",
      "strating its effectiveness. While existing methods perform\n",
      "well in controlled datasets, they often struggle to generalize\n",
      "across diverse environments and dynamic lighting conditions\n",
      "in real-world settings. In this paper, we experiment with\n",
      "real-world traffic scenes captured by building-mounted cam-\n",
      "eras, assessing diverse environments, lighting conditions, and\n",
      "dynamic scenarios such as nighttime and crowded scenes.\n",
      "This challenges the generalization capabilities of small object\n",
      "detection method beyond controlled datasets.\n",
      "III. INTRODUCTION OF YOLOV8 DETECTION NETWORK\n",
      "As shown in Figure 1, the YOLOv8 architecture consists\n",
      "of three main elements: the backbone, neck, and detection\n",
      "layers. Each of these components will be introduced in the\n",
      "subsequent sections.\n",
      "A. Backbone Layer\n",
      "The architecture of YOLOv8 is based on the CSPDark-\n",
      "net53 [30] backbone, employing five downsampling stages to\n",
      "extract distinct scale features. It improves information flow\n",
      "and stays lightweight by using the C2f module instead of\n",
      "Fig. 1.\n",
      "The network structure of YOLOv8.\n",
      "the Cross Stage Partial (CSP) module [32]. The C2f module\n",
      "includes dense and residual structures for better gradient flow\n",
      "and feature representation. The backbone also includes the\n",
      "Spatial Pyramid Pooling Fast (SPPF) module, which captures\n",
      "features at multiple scales to boost detection performance.\n",
      "The SPPF layer reduces computational complexity and la-\n",
      "tency while optimizing feature extraction [34].\n",
      "B. Neck Layer\n",
      "For multi-scale feature fusion, YOLOv8’s neck uses Fea-\n",
      "ture Pyramid Network (FPN) [35] and Path Aggregation\n",
      "Networks (PANet) [36]. FPN enhances hierarchical feature\n",
      "fusion, improving object detection across various scales\n",
      "through a top-down pathway, while PANet enhances fea-\n",
      "ture representation and information reuse with a bottom-up\n",
      "pathway, though it increases computational cost. Combining\n",
      "FPN-PANet structures and C2f modules integrates feature\n",
      "maps of various scales, merging both shallow and deep\n",
      "information.\n",
      "C.\n",
      "Detection Head Layer\n",
      "YOLOv8, a state-of-the-art object detection model, en-\n",
      "hances accuracy and robustness by using the Task-Aligned\n",
      "Assigner [37] instead of traditional anchors. This assigner\n",
      "dynamically categorizes samples as positives or negatives,\n",
      "refining the model’s ability to detect objects accurately. The\n",
      "detection head features a decoupled structure with sepa-\n",
      "rate branches for object classification and bounding box\n",
      "regression. For classification, it employs binary cross-entropy\n",
      "loss (BCE Loss). For regression, it uses a combination of\n",
      "distribution focal loss (DFL) [38] and Complete Intersection\n",
      "over Union (CIoU) [39] loss. These efficient loss functions\n",
      "Fig. 2.\n",
      "Proposed improved YOLOv8 for small object detection\n",
      "are crucial for precise object localization, further boosting\n",
      "the model’s performance.\n",
      "Bounding box loss functions aim to accurately localize\n",
      "objects by penalizing differences between predicted and\n",
      "ground truth bounding boxes. IoU-based loss functions [40]\n",
      "are crucial for bounding box regression in the detection layer.\n",
      "IoU Loss measures the overlap between predicted and ground\n",
      "truth boxes by comparing the ratio of their intersection\n",
      "area to their union area. However, its gradient diminishes\n",
      "when there is no overlap, making it less effective in such\n",
      "cases. Various IoU-based loss functions have been developed\n",
      "with different methodologies and constraints. CIoU, used in\n",
      "YOLOv8, minimizes the normalized distance between the\n",
      "center points of predicted and ground truth boxes and in-\n",
      "cludes an aspect ratio penalty term. This approach improves\n",
      "convergence speed and overall performance.\n",
      "IV. METHOD\n",
      "In this section, we introduce three pivotal enhancements\n",
      "to improve small object detection in our study. First, we\n",
      "enhance feature fusion within the YOLOv8 architecture’s\n",
      "neck to better retain crucial spatial details typically filtered\n",
      "out by shallower layers. This modification aims to mitigate\n",
      "information loss, especially for smaller objects overshadowed\n",
      "by larger ones during feature extraction. Second, we pro-\n",
      "pose the C2f-EMA module, integrating an EMA attention\n",
      "mechanism to prioritize relevant features and spatial details\n",
      "across different channels. This method enhances feature\n",
      "extraction efficiency by redistributing feature weights ef-\n",
      "fectively. Finally, we use PIoU as an improved bounding\n",
      "box regression metric, replacing CIoU. PIoU incorporates a\n",
      "penalty term that minimizes the Euclidean distance between\n",
      "corresponding corners of predicted and ground truth boxes,\n",
      "offering a more intuitive measure of similarity and stability in\n",
      "box regression tasks. These methods contribute to enhancing\n",
      "the accuracy and robustness of our small object detection\n",
      "framework. The enhanced structure depicted in Figure 2 is\n",
      "utilized in this paper.\n",
      "A. Improved GFPN for Multilevel Feature Integration\n",
      "In YOLOv8, crucial spatial details are primarily encoded\n",
      "in the network’s shallower layers. However, these layers\n",
      "often filter out less prominent details, leading to significant\n",
      "data loss for small object detection. Additionally, smaller\n",
      "objects may be overshadowed by larger ones during feature\n",
      "extraction, resulting in a gradual loss of information and the\n",
      "potential disappearance of relevant details. To address these\n",
      "challenges, this study introduces an enhanced feature fusion\n",
      "method in the neck of the YOLOv8 architecture. This method\n",
      "focuses on preserving and effectively utilizing important\n",
      "information from the shallower layers, thereby enhancing\n",
      "overall detection accuracy, especially for small objects.\n",
      "The FPN merges features of different resolutions ex-\n",
      "tracted from a backbone network. It begins with the highest-\n",
      "resolution feature map and progressively combines features\n",
      "from higher to lower resolutions using a top-down approach.\n",
      "PAFPN improves FPN by adding a bottom-up approach that\n",
      "enhances bidirectional information flow. It merges features\n",
      "from lower to higher network layers, prioritizing spatial\n",
      "detail preservation, even with increased computational de-\n",
      "mands.\n",
      "The BiFPN [31] enhances object detection by integrating\n",
      "features across different resolutions bidirectionally, using\n",
      "both bottom-up and top-down pathways. This method op-\n",
      "timizes multi-scale feature utilization, simplifies the network\n",
      "by reducing computational complexity, and includes skip\n",
      "connections at each level. These connections allow for\n",
      "adaptable use of input features, enhancing feature fusion\n",
      "across scales and details [31]. However, deep stacking of\n",
      "BiFPN blocks may cause gradient vanishing during training,\n",
      "potentially affecting overall network performance [33].\n",
      "Prior methods focused mainly on combining features with-\n",
      "out considering inner block connections. In contrast, GFPN\n",
      "introduces skip connections and queen fusion structures. It\n",
      "employs skip-layer and cross-scale connections to enhance\n",
      "feature combination. GFPN implements skip connections in\n",
      "two forms: log2(n)-link and dense-link.\n",
      "The log2(n)-link method optimizes information transmis-\n",
      "sion by allowing the lth layer at level k to receive feature\n",
      "maps from up to log2(l) + 1 preceding layers. These skip\n",
      "connections help mitigate gradient vanishing during back-\n",
      "propagation by extending the shortest gradient distance from\n",
      "one layer to approximately 1 + log2(n) layers [9]. This\n",
      "extension facilitates more effective gradient propagation over\n",
      "longer distances, potentially enhancing the scalability of\n",
      "deeper networks.\n",
      "In contrast, the dense-link method ensures that each scale\n",
      "feature P l\n",
      "k at level k receives feature maps from all preceding\n",
      "Fig. 3.\n",
      "skip-layer links: (a) dense-link: concatenates features from all\n",
      "preceding layers; (b) log2 n-link: concatenates features from up to log2(l)+\n",
      "1 layers at each level.\n",
      "Fig. 4.\n",
      "Different Feature Pyramid Network designs: (a) FPN uses a top-\n",
      "down strategy; (b) PANet enhances FPN with a bottom-up pathway; (c)\n",
      "BiFPN integrates cross-scale pathways bidirectionally; (d) GFPN includes\n",
      "a queen-fusion style pathway and skip-layer connections.\n",
      "layers up to the lth layer. This promotes robust information\n",
      "flow and integration of features across multiple scales. In\n",
      "GFPN, this structure facilitates seamless connectivity be-\n",
      "tween layers, enhancing feature reuse and improving the\n",
      "network’s efficiency in tasks like object detection. During\n",
      "back-propagation, the dense connectivity in GFPN supports\n",
      "efficient transmission of feature information across the net-\n",
      "work hierarchy. Dense-link and log2(n)-link configurations\n",
      "are illustrated in Figure 3.\n",
      "Another significant improvement in GFPN is the Queen-\n",
      "Fusion module, which facilitates cross-scale connections to\n",
      "enhance adaptability to multi-scale variations. This module\n",
      "utilizes a 3x3 convolution to merge features across different\n",
      "scales, gathering input features from diagonally adjacent\n",
      "nodes above and below to minimize information loss during\n",
      "feature fusion. Implementing this approach enhances the\n",
      "network’s capability to handle multi-scale variations, po-\n",
      "tentially improving overall performance robustness. Figure\n",
      "4 illustrates various methods for integrating features across\n",
      "different layers, including FPN, PANet, BiFPN, and GFPN.\n",
      "In YOLOv8, integrating PAFPN with C2f modules effec-\n",
      "tively combines feature maps across scales, thereby enhanc-\n",
      "ing object detection capabilities. This study aims to enhance\n",
      "YOLOv8’s small object detection using advanced feature\n",
      "fusion techniques. However, replacing PAFPN with GFPN\n",
      "in YOLOv8 improves precision while introducing higher\n",
      "latency compared to the PAFPN-based model.\n",
      "This paper introduces an enhanced and efficient GFPN,\n",
      "depicted in Figure 5, inspired by Efficient-RepGFPN [11].\n",
      "By integrating it into YOLOv8, the model achieves superior\n",
      "performance without significantly increasing computational\n",
      "complexity or latency. Efficient-RepGFPN simplifies com-\n",
      "plexity by parameterizing and eliminating additional upsam-\n",
      "pling operations in queen-fusion. Furthermore, it upgrades\n",
      "the feature fusion module to CSPNet, enhancing the merging\n",
      "of features across different scales [11].\n",
      "In the feature fusion block of the GFPN architecture, we\n",
      "replace the conventional 3x3 convolution-based feature fu-\n",
      "sion with C2f-EMA, incorporating an attention mechanism.\n",
      "Fig. 5.\n",
      "Enhanced and efficient GPFN structure\n",
      "This module merges high-level semantic features with low-\n",
      "level spatial details, thereby enhancing the representation\n",
      "and detection accuracy of small objects. These modifications\n",
      "maintain GFPN’s ability to improve feature interaction and\n",
      "efficiency by effectively managing both types of informa-\n",
      "tion in the neck section. Inspired by Efficient-RepGFPN,\n",
      "we also reparametrize and eliminate additional upsampling\n",
      "operations in queen-fusion. Ultimately, these enhancements\n",
      "improve the efficiency and effectiveness of YOLOv8 for\n",
      "object detection tasks without significantly increasing com-\n",
      "putational complexity or latency.\n",
      "We enhance the network’s capability by adding a detection\n",
      "layer, which involves integrating feature maps from P2 along-\n",
      "side those from P3 to P5, which are used in YOLOv8. This\n",
      "enhancement significantly improves the network’s ability to\n",
      "detect small objects. As depicted in Figure 5, P2 with a\n",
      "resolution of 320x320 plays a crucial role in preserving finer\n",
      "spatial details essential for improving small object detection.\n",
      "Additionally, a detection head is introduced, prompting the\n",
      "network structure to focus more on features related to small\n",
      "objects.\n",
      "This approach not only provides higher-resolution de-\n",
      "tails but also enhances feature fusion, offers comprehensive\n",
      "contextual information, and enables precise localization. By\n",
      "leveraging features from both fine and coarse scales simulta-\n",
      "neously, the network achieves accurate detection of small\n",
      "objects, effectively capturing finer details. The enhanced\n",
      "architectural design illustrated in Figure 5 is implemented\n",
      "and evaluated in this study.\n",
      "B. Embedding Efficient Multi-scale Attention Mechanism in\n",
      "C2f\n",
      "The C2f module in YOLOv8 enhances gradient flow\n",
      "and detection accuracy by dynamically adjusting channel\n",
      "numbers through split and concatenate operations, optimizing\n",
      "feature extraction while managing computational complexity\n",
      "[41]. It incorporates convolutional and residual structures to\n",
      "deepen network training and address the vanishing gradient\n",
      "problem, thereby improving feature extraction.\n",
      "The C2f-EMA module introduced in this paper enhances\n",
      "feature extraction by redistributing feature weights using\n",
      "the EMA attention mechanism. This mechanism prioritizes\n",
      "relevant features and spatial details across different channels\n",
      "within the image. Figure 6 illustrates the EMA structure,\n",
      "which partitions input features into groups, processes them\n",
      "through parallel subnetworks, and integrates them with ad-\n",
      "vanced aggregation techniques. This enhancement signifi-\n",
      "cantly boosts the representation of small, challenging objects\n",
      "Fig. 6.\n",
      "Efficient Multi-scale Attention Mechanism\n",
      "and enhances the efficiency of the network backbone.\n",
      "The EMA module employs feature grouping to partition\n",
      "the input feature map X along the channel dimension into G\n",
      "sub-features, denoted as X = [X1, X2, . . . , XG], where each\n",
      "Xi ∈R\n",
      "C\n",
      "G ×H×W . This approach enables specialized feature\n",
      "extraction and representation by allowing the network to\n",
      "learn different semantics or characteristics within each group.\n",
      "Additionally, it optimizes CNNs by reducing computation.\n",
      "The EMA module uses a Parallel Subnetworks approach to\n",
      "efficiently capture multi-scale spatial information and cross-\n",
      "channel dependencies. It features two parallel branches: the\n",
      "1x1 Branch, with two routes, and the 3x3 Branch, with one\n",
      "route. In the 1x1 Branch, each route employs 1D global\n",
      "average pooling to encode channel information along the\n",
      "horizontal and vertical spatial directions. These operations\n",
      "produce two encoded feature vectors representing global\n",
      "information, which are then concatenated along the height\n",
      "direction. A 1x1 convolution layer is subsequently applied\n",
      "to the concatenated output to maintain channel integrity and\n",
      "capture cross-channel interactions by blending information\n",
      "across different channels. The outputs are split into two\n",
      "vectors, refined using non-linear Sigmoid functions to adjust\n",
      "attention weights in a 2D Binomial distribution. Channel-\n",
      "wise attention maps are then combined through multiplica-\n",
      "tion within each group, enhancing interactive features across\n",
      "channels.\n",
      "EMA differs from traditional attention methods by ad-\n",
      "dressing issues such as neglecting interactions among spatial\n",
      "details and the limited scope of 1x1 kernel convolution,\n",
      "accomplished through the inclusion of a 3x3 convolution\n",
      "branch. This branch uses a single route with a 3x3 con-\n",
      "volution kernel to capture multi-scale spatial information.\n",
      "Additionally, the output of the 1x1 branch undergoes 2D\n",
      "global average pooling to encode global spatial information.\n",
      "The pooled output integrates with the transformed output\n",
      "from the 3x3 branch, aligning dimensions to enhance feature\n",
      "aggregation through both spatial information sources.\n",
      "Unlike traditional attention methods that use basic aver-\n",
      "aging, EMA integrates attention maps from parallel subnet-\n",
      "works using a cross-spatial learning approach. It uses matrix\n",
      "dot product operations to capture relationships between in-\n",
      "dividual pixels, enriching global context across all pixels.\n",
      "Fig. 7.\n",
      "C2f-EMA\n",
      "Specifically, the EMA module combines global and local\n",
      "spatial information from its parallel 1x1 and 3x3 branches\n",
      "to enhance feature representation. This approach effectively\n",
      "captures long-range dependencies and multi-scale spatial\n",
      "details, improving overall feature aggregation.\n",
      "SoftMax is then applied to the outputs to generate 2D\n",
      "Gaussian maps, highlighting relevant features and model-\n",
      "ing long-range dependencies. This process is repeated for\n",
      "the second spatial attention map, using 2D global average\n",
      "pooling and Sigmoid functions to preserve precise spatial\n",
      "positional information. Finally, feature maps obtained from\n",
      "spatial attention weights within each group are aggregated.\n",
      "The resulting output feature map retains the original input\n",
      "size, ensuring efficiency and effectiveness for integration into\n",
      "architectures. The final output is a redistributed feature map\n",
      "that captures pixel-level pairwise relationships and highlights\n",
      "the global context across all pixels and channels, allocating\n",
      "higher weights to more relevant features and spatial details.\n",
      "In this paper, we introduce the C2f-EMA as a replacement\n",
      "for C2f, redistributing the feature map using the EMA\n",
      "structure to assign higher weights to more relevant features\n",
      "and spatial details within images. This enhancement aims to\n",
      "improve detection performance, especially for small objects\n",
      "with very fine details due to their size. C2f-EMA includes\n",
      "initial convolution, split function, EMA module, and parallel\n",
      "processing, collectively enhancing the network’s overall per-\n",
      "formance. As shown in Figure 7, this mechanism operates\n",
      "within the second residual block of the C2f.\n",
      "C. Improved Bounding Box Loss Function\n",
      "Bounding box loss functions penalize discrepancies be-\n",
      "tween predicted and ground truth bounding box parameters\n",
      "to enhance object localization. IoU-based loss functions are\n",
      "essential for this purpose, measuring overlap as the ratio of\n",
      "intersection over union. However, their effectiveness dimin-\n",
      "ishes when there is no overlap, resulting in negligible gradi-\n",
      "ents. Several IoU-based loss functions have been developed\n",
      "to address this limitation, each presenting unique approaches\n",
      "and specific limitations. CIoU, implemented in YOLOv8,\n",
      "considers the distance between box centers and differences\n",
      "in aspect ratios. This refinement enhances convergence speed\n",
      "and overall performance. However, the aspect ratio penalty\n",
      "term in CIoU may not sufficiently account for size variations\n",
      "between boxes of the same aspect ratio but different dimen-\n",
      "sions. Moreover, CIoU involves computationally intensive\n",
      "inverse trigonometric functions, which could pose drawbacks\n",
      "in real-time applications. The equation for CIoU is presented\n",
      "in Equation 1:\n",
      "LCIOU = LIOU + d2\n",
      "c2 + v,\n",
      "v = 4\n",
      "π2\n",
      "\u0012\n",
      "arctan\n",
      "\u0012wgt\n",
      "hgt\n",
      "\u0013\n",
      "−arctan\n",
      "\u0010w\n",
      "h\n",
      "\u0011\u00132\n",
      "(1)\n",
      "Where d represents the Euclidean distance between the\n",
      "center points of the predicted and ground truth bounding\n",
      "boxes. Additionally, c represents a normalization factor,\n",
      "typically representing the diagonal length of the smallest\n",
      "enclosing box that contains both the predicted and ground\n",
      "truth bounding boxes. v represents the aspect ratio penalty,\n",
      "which accounts for discrepancies in aspect ratios between\n",
      "the predicted and ground truth boxes. (w, h) and (wgt, hgt)\n",
      "represent the width and height of the predicted and ground\n",
      "truth bounding box, respectively.\n",
      "Efficient Intersection over Union (EIoU) adjusts CIoU\n",
      "by using distinct penalty terms for width and height rather\n",
      "than a shared aspect ratio penalty, aiming for more precise\n",
      "measurement of differences between anchor box and target\n",
      "box dimensions. Equation 2 presents the EIoU formula.\n",
      "LEIOU = LIOU + d2\n",
      "c2 + (wpred −wgt)2\n",
      "w2c\n",
      "+ (hpred −hgt)2\n",
      "h2c\n",
      "(2)\n",
      "where wc and hc represent the width and height of\n",
      "the smallest enclosing bounding box, respectively. Despite\n",
      "addressing size discrepancies, EIoU encounters challenges\n",
      "such as anchor box enlargement during regression and slow\n",
      "convergence. This issue is critical in object detection models\n",
      "that use IoU-based loss functions, where optimization may\n",
      "inadvertently enlarge anchor boxes instead of precisely con-\n",
      "verging them to target sizes, thereby reducing localization\n",
      "precision [42]. CIoU and EIoU losses use the term RD = d2\n",
      "c2 ,\n",
      "where d is the diagonal length of the intersection between the\n",
      "anchor and target boxes, and c is the diagonal length of the\n",
      "smallest enclosing box covering both. The gradient of RD\n",
      "with respect to d is 2d\n",
      "c2 , meaning RD decreases as c increases.\n",
      "The problem is when the boxes do not overlap, enlarging the\n",
      "anchor box increases c, reducing RD and thus lowering the\n",
      "CIoU and EIoU losses without improving the overlap. Using\n",
      "c as the denominator in the penalty term is flawed, allowing\n",
      "loss reduction through anchor box size manipulation rather\n",
      "than overlap improvement, indicating the need for a revised\n",
      "penalty term to better handle non-overlapping boxes [42].\n",
      "Wise Intersection over Union (WIoU) [26] introduces a\n",
      "dynamic, non-monotonic focusing mechanism in bounding\n",
      "box regression. This mechanism prioritizes anchor boxes\n",
      "with moderate quality and reduces harmful gradients from\n",
      "low-quality examples. WIoU uses aspect ratio and the dis-\n",
      "tance between predicted and ground truth boxes as penalty\n",
      "terms. It evaluates anchor box quality dynamically by com-\n",
      "paring each box’s quality to the average quality of all boxes\n",
      "in the batch, giving more attention to those with moderate\n",
      "quality. The WIoU calculation is given by Equation 3.\n",
      "LWIoUv3 =\n",
      "β\n",
      "δαβ−δ · e\n",
      "\u0012\n",
      "(x−xgt)2+(y−ygt)2\n",
      "w2\n",
      "gt+h2\n",
      "gt\n",
      "\u0013\n",
      ",\n",
      "β = L∗\n",
      "IOU\n",
      "LIOU\n",
      "(3)\n",
      "The non-monotonic attention function of WIoU is denoted\n",
      "by β, while δ and α serve as hyper-parameters that regulate\n",
      "its gradient. The operation ∗denotes the detach operation,\n",
      "and LIOU indicates the average LIOU value across all an-\n",
      "chor boxes within a batch. WIoU introduces attention-based\n",
      "predicted box loss and focusing coefficients. However, it\n",
      "relies on multiple hyper-parameters, posing challenges in\n",
      "optimization for diverse datasets.\n",
      "We use PIoU [42] as a replacement for CIoU in the\n",
      "original network. The details of the PIoU method are outlined\n",
      "in Algorithm 1. The penalty term in PIoU enhanced bound-\n",
      "ing box regression by minimizing the Euclidean distance\n",
      "between corresponding corners of predicted and ground truth\n",
      "boxes. This approach offers a more intuitive measure of\n",
      "similarity and proves effective for both overlapping and\n",
      "non-overlapping boxes. Unlike traditional IoU-based loss\n",
      "functions, PIoU mitigates the issue of box enlargement,\n",
      "ensuring precise and stable box regression. Simulated results\n",
      "in Figure 8 demonstrate its effectiveness. Figure 8 illustrates\n",
      "an experiment evaluating anchor box regression using various\n",
      "loss functions. The CIoU loss function exhibits continuous\n",
      "enlargement of anchor boxes from epochs 25 to 75 and fails\n",
      "to achieve full convergence to the ground truth anchor box by\n",
      "epoch 150. In contrast, the anchor box guided solely by the\n",
      "penalty term in the PIoU loss function, without considera-\n",
      "tion of the attention function, does not show enlargement\n",
      "issues during training, as observed in epochs 25 and 75.\n",
      "By epoch 75, it demonstrates almost complete convergence\n",
      "to the ground truth bounding box, reaching a perfect fit\n",
      "by epoch 150. PIoU loss uses a non-monotonic attention\n",
      "layer to enhance focus on medium and high-quality anchor\n",
      "boxes. By prioritizing moderate-quality stages in anchor box\n",
      "regression, PIoU improves object detector performance. The\n",
      "non-monotonic attention function u(λq), controlled by the\n",
      "parameter λ. PIoU simplifies the tuning process by requiring\n",
      "only one hyperparameter. Replace the penalty factor P with\n",
      "q, which indicates anchor box quality on a scale from 0 to\n",
      "1. When q = 1 (meaning P = 0), the anchor box perfectly\n",
      "aligns with the target box. As P increases, q decreases,\n",
      "signifying lower-quality anchor boxes.\n",
      "V. RESULTS\n",
      "This section begins with an introduction to the dataset\n",
      "utilized in this paper, followed by detailing the experimental\n",
      "environment and training strategy. It further outlines the\n",
      "evaluation metrics employed to assess the model’s perfor-\n",
      "mance. The effectiveness of the proposed approach is then\n",
      "demonstrated through a comparative analysis with state-of-\n",
      "the-art models, using YOLOv8 as the baseline. Furthermore,\n",
      "the section includes an evaluation of the model’s performance\n",
      "Algorithm 1: : PIoU Bounding Box Regression\n",
      "1 Input:\n",
      "2 - Two arbitrary convex shapes: A, B ⊆Rn\n",
      "3 - Width and height of input image: w, h\n",
      "4 - Width and height of ground truth box: wgt, hgt\n",
      "5 - Coordinates of bounding box 1:\n",
      "b1x1, b1x2, b1y1, b1y2\n",
      "6 - Coordinates of bounding box 2:\n",
      "b2x1, b2x2, b2y1, b2y2\n",
      "7 - IoU (Intersection over Union)\n",
      "8 Output:\n",
      "9 - Powerful-IoU\n",
      "10 Steps:\n",
      "1) Calculate Absolute Differences for Widths:\n",
      "• dw1 = | min(b1x2 −b1x1) −min(b2x2 −b2x1)|\n",
      "• dw2 = | min(b1x2 −b1x1) −min(b2x2 −b2x1)|\n",
      "2) Calculate Absolute Differences for Heights:\n",
      "• dh1 = | min(b1y2 −b1y1) −min(b2y2 −b2y1)|\n",
      "• dh2 = | min(b1y2 −b1y1) −min(b2y2 −b2y1)|\n",
      "3) Compute Parameter P:\n",
      "P = 1\n",
      "4(dw1 + dw2\n",
      "wgt\n",
      "+ dh1 + dh2\n",
      "hgt\n",
      ")\n",
      "4) Calculate Custom Loss L:\n",
      "L = 1 −IoU −e−P 2\n",
      "5) Calculate the Focal Loss by Adding an Attention\n",
      "Layer:\n",
      "q = e−P ,\n",
      "q ∈(0, 1]\n",
      "u(x) = 3x · e−x2\n",
      "LP IoU = u(λq) · L\n",
      "LP IoU = 3 · (λq) · e−(λq)2 · L\n",
      "Fig. 8.\n",
      "Anchor box regression process guided by (a) Complete IoU-based\n",
      "loss function (CIoU) (b) penalty term in Powerful-IoU (PIoU) loss function\n",
      "without attention function.\n",
      "Fig. 9.\n",
      "Information regarding the manual annotation process for objects\n",
      "in the VisDrone2019 dataset\n",
      "in challenging real-world scenarios, such as detecting distant\n",
      "objects and small objects positioned far from the camera.\n",
      "A. Dataset\n",
      "The VisDrone2019 dataset [23], a prominent collection\n",
      "of UAV aerial photography, was developed by Tianjin Uni-\n",
      "versity’s Lab of Machine Learning and Data Mining in\n",
      "collaboration with the AISKYEYE data mining team. It com-\n",
      "prises 288 video clips totaling 261,908 frames and 10,209\n",
      "static images. These visuals were captured using various\n",
      "drone-mounted cameras, showcasing diverse scenarios across\n",
      "more than a dozen cities throughout China. The dataset is\n",
      "exceptionally rich, featuring a wide range of geographic\n",
      "locations, environmental settings, and object types. Geo-\n",
      "graphically, the dataset covers footage from 14 different\n",
      "cities across China, offering a comprehensive spectrum of\n",
      "scenes from urban to rural landscapes. It includes a diverse\n",
      "array of objects such as pedestrians, cars, bicycles, and more.\n",
      "Additionally, the dataset spans various population densities,\n",
      "ranging from sparse to densely crowded areas, and captures\n",
      "images under different lighting conditions, including both\n",
      "daytime and nighttime scenes. One distinguishing feature of\n",
      "the VisDrone2019 dataset is its inclusion of numerous small\n",
      "objects of varying sizes, depicted from different angles and\n",
      "within various scenes. This diversity increases the dataset’s\n",
      "complexity and difficulty compared to other computer vision\n",
      "datasets. Figure 9 illustrates the process of manually anno-\n",
      "tating objects in the VisDrone2019 dataset.\n",
      "B. Experimental Environment and Training Strategies\n",
      "In this study, YOLOv8s was selected as the baseline model\n",
      "for investigation and further enhancements. The model was\n",
      "trained on the VisDrone dataset using an NVIDIA RTX\n",
      "A6000 GPU (48 GB) on Linux, utilizing PyTorch 2.2.1 and\n",
      "CUDA 12.1. Training involved optimizing key parameters,\n",
      "running for 200 epochs with the Stochastic Gradient Descent\n",
      "(SGD) optimizer [43] set to a momentum of 0.932. The\n",
      "initial learning rate started at 0.01 and decayed gradually to\n",
      "0.0001. A batch size of 32 was chosen for efficient memory\n",
      "usage and stable training, with input images resized to\n",
      "640x640 pixels. A weight decay of 0.0005 was also applied\n",
      "to prevent overfitting and improve model generalization.\n",
      "C. Evaluation metrics\n",
      "To assess the detection performance of our enhanced\n",
      "model, we utilize several evaluation metrics: precision, recall,\n",
      "mAP0.5, mAP0.5 : 0.95, and the number of model parame-\n",
      "ters. The specific formulas for these metrics are provided in\n",
      "this section.\n",
      "Precision: is the metric that represents the ratio of true\n",
      "positives to the total predicted positives, as defined by\n",
      "Equation 4:\n",
      "Precision =\n",
      "TP\n",
      "TP + FP\n",
      "(4)\n",
      "True Positives (TP) is the number of instances where the\n",
      "model accurately predicts a positive instance. False Positives\n",
      "(FP) is the number of instances where the model incorrectly\n",
      "predicts a positive instance. False Negatives (FN) is the\n",
      "number of instances where the model fails to predict a\n",
      "positive instance.\n",
      "Recall: Measures the ratio of correctly predicted positive\n",
      "samples to all actual positive samples, as defined by Equation\n",
      "5:\n",
      "Recall =\n",
      "TP\n",
      "TP + FN\n",
      "(5)\n",
      "Average Precision (AP): represents the area under the\n",
      "precision-recall curve, calculated using Equation 6:\n",
      "AP =\n",
      "Z 1\n",
      "0\n",
      "Precision(Recall) d(Recall)\n",
      "(6)\n",
      "Mean Average Precision (mAP): represents the average\n",
      "AP value across all categories, indicating the model’s overall\n",
      "detection performance across the entire dataset. This calcu-\n",
      "lation is defined by Equation 7:\n",
      "mAP = 1\n",
      "N\n",
      "N\n",
      "X\n",
      "i=1\n",
      "APi\n",
      "(7)\n",
      "where APi represents the average precision value for the\n",
      "category indexed by i, and N denotes the total number of\n",
      "categories in the training dataset.\n",
      "mAP0.5: is the average precision calculated at an IoU\n",
      "threshold of 0.5.\n",
      "mAP0.5:0.95: is calculated across IoU thresholds from 0.5\n",
      "to 0.95, with values computed at intervals of 0.05.\n",
      "D. Experiment Results\n",
      "This section presents a comprehensive evaluation of the\n",
      "SOD-YOLOv8 model through targeted experiments. We be-\n",
      "gin by comparing the PIoU loss function with other common\n",
      "loss functions on YOLOv8s. Next, we assess the integration\n",
      "of the GFPN structure with the EMA and other attention\n",
      "modules. We then evaluate SOD-YOLOv8s against various\n",
      "YOLO variants (YOLOv3, YOLOv5s, YOLOv7) and widely\n",
      "used models (Faster R-CNN, CenterNet, Cascade R-CNN,\n",
      "SSD). Ablation studies validate the contributions of each\n",
      "enhancement. Visual experiments with the VisDrone2019\n",
      "dataset demonstrate the model’s effectiveness in diverse\n",
      "scenarios, including distant, high-density, and nighttime con-\n",
      "ditions. Finally, real-world traffic scene evaluations highlight\n",
      "the model’s applicability and performance in challenging en-\n",
      "vironments with cameras mounted on buildings at significant\n",
      "distances from the objects of interest.\n",
      "1) Comparative Evaluation of Bounding Box Regression:\n",
      "To evaluate the impact of PIoU, we conducted comparative\n",
      "experiments on YOLOv8s using PIoU and other common\n",
      "loss functions under consistent training conditions. As shown\n",
      "in Table I, PIoU achieves the best detection performance.\n",
      "Specifically, it improves mAP0.5 by 1.1%, mAP0.5:0.95 by\n",
      "0.2%, precision by 1.6%, and recall by 0.4% compared\n",
      "to CIoU. Additionally, the simpler loss function of PIoU\n",
      "makes model tuning easier, demonstrating its potential as\n",
      "an efficient and effective bounding box regression method.\n",
      "2) Comparative experiment of attention mechanisms: To\n",
      "evaluate the effectiveness of integrating the GFPN structure\n",
      "with the EMA attention mechanism, we incorporated three\n",
      "widely used attention modules—CBAM [45], CA [46], and\n",
      "SE [47]—at the same position within the GFPN structure.\n",
      "This setup allowed for a direct comparison in our exper-\n",
      "iments, as detailed in Table II. The experimental results\n",
      "demonstrate that training with the GFPN-EMA combination\n",
      "consistently outperforms GFPN configurations with CBAM,\n",
      "CA, and SE in terms of mAP0.5 values. Specifically, the\n",
      "GFPN-EMA model exhibits significant improvements across\n",
      "most object categories, particularly in Pedestrian, People,\n",
      "Car, Bus, and Motor classes, as well as overall mAP0.5. As\n",
      "shown in Table II, these findings highlight EMA’s efficacy in\n",
      "improving small object detection accuracy in the VisDrone\n",
      "dataset. EMA achieves this by addressing spatial interactions,\n",
      "overcoming 1x1 kernel convolution limitations through the\n",
      "integration of a 3x3 convolution for multi-scale spatial in-\n",
      "formation, and employing cross-spatial learning to merge\n",
      "attention maps from parallel subnetworks. This approach\n",
      "effectively combines global and local spatial contexts.\n",
      "3) Comparison with different mainstream model: Com-\n",
      "paring SOD-YOLOv8s with other YOLO variants such as\n",
      "YOLOv3, YOLOv5s, and YOLOv7, SOD-YOLOv8s is re-\n",
      "markably efficient. Despite having a small model size of 11.5\n",
      "million parameters, SOD-YOLOv8s achieves the highest\n",
      "accuracy metrics. It outperforms YOLOv3, YOLOv5s, and\n",
      "YOLOv7, despite YOLOv3 and YOLOv5s having larger\n",
      "model sizes ranging from 12.0 million to 18.3 million\n",
      "TABLE I\n",
      "DETECTION RESULTS OF YOLOV8S WITH DIFFERENT BOUNDING BOX LOSS FUNCTIONS, SHOWN AS PERCENTAGES (BEST OUTCOMES IN BOLD).\n",
      "Metrics\n",
      "Precision%\n",
      "Recall%\n",
      "mAP0.5%\n",
      "mAP0.5:0.95%\n",
      "CIOU\n",
      "51.2\n",
      "40.1\n",
      "40.6\n",
      "24\n",
      "EIOU\n",
      "49.8\n",
      "39.9\n",
      "40\n",
      "23\n",
      "WIOU v1\n",
      "51.4\n",
      "40\n",
      "40.7\n",
      "23.7\n",
      "WIOU v2\n",
      "51.9\n",
      "40\n",
      "40.7\n",
      "23.8\n",
      "WIOU v3\n",
      "52.6\n",
      "40\n",
      "41.2\n",
      "24.2\n",
      "MPDIOU [44]\n",
      "52.1\n",
      "39\n",
      "40.7\n",
      "23.9\n",
      "PIoU (λ = 1.2)\n",
      "52.8\n",
      "40.5\n",
      "41.7\n",
      "24.2\n",
      "TABLE II\n",
      "DETECTION RESULTS OF GFPN-YOLOV8S WITH VARIOUS ATTENTION MECHANISMS ARE PRESENTED AS PERCENTAGES. (BEST RESULTS ARE\n",
      "HIGHLIGHTED IN BOLD))\n",
      "Models\n",
      "Pedestrian\n",
      "People\n",
      "Bicycle\n",
      "Car\n",
      "Van\n",
      "Truck\n",
      "Bus\n",
      "Motorcycle\n",
      "mAP0.5\n",
      "YOLO-GFPN-SE\n",
      "50.9\n",
      "42.6\n",
      "15.0\n",
      "83.5\n",
      "45.1\n",
      "39.2\n",
      "53.4\n",
      "51.3\n",
      "42.9\n",
      "YOLO-GFPN-CBAM\n",
      "48.8\n",
      "40.3\n",
      "14.6\n",
      "82.9\n",
      "46.0\n",
      "38.8\n",
      "55.4\n",
      "49.3\n",
      "42.3\n",
      "YOLO-GFPN-CA\n",
      "51.0\n",
      "42.3\n",
      "17.8\n",
      "83.8\n",
      "47.5\n",
      "40.1\n",
      "57.3\n",
      "52.7\n",
      "43.9\n",
      "YOLO-GFPN-EMA\n",
      "51.0\n",
      "43.3\n",
      "16.7\n",
      "83.8\n",
      "46.7\n",
      "39.1\n",
      "60.9\n",
      "52.8\n",
      "44.2\n",
      "parameters. The YOLOv8 model has been adapted into var-\n",
      "ious scales (YOLOv8n, YOLOv8s, YOLOv8m, YOLOv8l,\n",
      "and YOLOv8x) by adjusting width and depth, each pro-\n",
      "gressively consuming more resources to improve detection\n",
      "performance. We conducted comparisons between SOD-\n",
      "YOLOv8s and different scales of YOLOv8 to further validate\n",
      "the performance of our proposed approach. Based on the\n",
      "information presented in Table III, Despite its lower param-\n",
      "eter count of 11.5 million, SOD-YOLOv8s achieves highest\n",
      "recall (43.9%), mAP0.5 (45.1%), and mAP0.5:0.95 (26.6%).\n",
      "In contrast, YOLOv8m, which has 25.9 million parameters,\n",
      "achieves lower accuracy metrics. This indicates that SOD-\n",
      "YOLOv8s is efficient in terms of computing capacity and\n",
      "model size, while also performing well in object detection\n",
      "tasks.\n",
      "This study conducted a comparative experiment to evaluate\n",
      "the performance of SOD-YOLOv8s against widely adopted\n",
      "models, including Faster R-CNN, CenterNet, Cascade R-\n",
      "CNN, and SSD. In Faster R-CNN [48], the Region Proposal\n",
      "Network (RPN) [49] relies on backbone network features\n",
      "to generate region proposals. However, due to lower feature\n",
      "map resolution for small objects, the RPN may struggle\n",
      "to accurately localize them, leading to potential missed\n",
      "detections. Cascade R-CNN [52] enhances detection perfor-\n",
      "mance through a multilevel architecture, albeit at the cost of\n",
      "increased computational complexity and training difficulty.\n",
      "CenterNet [53] simplifies architecture with an anchor-free,\n",
      "center-point approach but faces challenges in precisely lo-\n",
      "cating small objects in crowded or obscured scenes. These\n",
      "challenges arise from ambiguous object centers, interference\n",
      "from larger objects, complex backgrounds obscuring object\n",
      "centers, and sensitivity to pixel-level inaccuracies. Addi-\n",
      "tionally, SSD’s performance decreases on smaller objects\n",
      "compared to larger ones, as its shallow neural network layers\n",
      "may lack detailed high-level features necessary for accurate\n",
      "small object prediction. According to the data provided in\n",
      "Table IV, the SOD-YOLOv8s model achieves the highest\n",
      "performance in AP0.5 of 45.1% and AP0.5:0.95 of 26.6%\n",
      "compared to other models such as CenterNet, Cascade R-\n",
      "CNN, SSD, and Faster R-CNN.\n",
      "4) Ablation Experiments: To validate the efficacy of each\n",
      "proposed enhancement approach in this study, ablation ex-\n",
      "periments were conducted on the baseline model. The results\n",
      "from these tests, shown in Table V, demonstrate that each\n",
      "enhancement significantly improves detection performance\n",
      "across various categories. Introducing PIoU for bounding\n",
      "box regression enhances localization without enlargement\n",
      "issues, leading to a significant 1.1% increase in mAP0.5.\n",
      "This improvement is particularly beneficial for categories\n",
      "such as Pedestrian, People, and Bicycle. Integrating an en-\n",
      "hanced GFPN and incorporating a new small object detection\n",
      "layer into the YOLOv8 network results in a significant\n",
      "2.9% increase in mAP0.5. This enhancement demonstrates\n",
      "substantial performance improvements across all categories,\n",
      "including Pedestrian, Bicycle, Car, Van, and Motor, high-\n",
      "lighting GFPN’s effectiveness in capturing multi-scale fea-\n",
      "tures. Furthermore, integrating the C2f-EMA module, which\n",
      "utilizes the EMA attention mechanism, and replacing C2f\n",
      "with it within the neck layers increases mAP0.5 by 0.5%.\n",
      "This enhancement notably benefits categories such as Peo-\n",
      "ple, Motor, and Truck, demonstrating its effectiveness in\n",
      "improving detection across various categories. According to\n",
      "Table VI, our proposed efficient model enhances object de-\n",
      "tection performance significantly without adding significant\n",
      "computational cost or latency compared to YOLOv8s. It\n",
      "improves recall from 43% to 44%, precision from 45% to\n",
      "46%, mAP0.5 from 40% to 45.1%, and mAP0.5:0.95 from\n",
      "20% to 26.6%.\n",
      "Figure 10 depicts the evaluation metrics for SOD-\n",
      "YOLOv8 and YOLOv8s across 200 training epochs. Our\n",
      "model outperforms YOLOv8s in precision and mAP0.5\n",
      "starting around epoch 15 and stabilizes after 50 epochs.\n",
      "TABLE III\n",
      "DIFFERENT YOLO MODELS’ RESULTS, PRESENTED AS PERCENTAGES.(THE BEST-PERFORMING OUTCOMES ARE HIGHLIGHTED IN BOLD)\n",
      "Models\n",
      "Model’s Size\n",
      "Backbone\n",
      "Precision\n",
      "Recall\n",
      "mAP0.5\n",
      "mAP0.5:0.95\n",
      "Time/ms\n",
      "Parameter/106\n",
      "YOLOv3 [30]\n",
      "-\n",
      "Darknet-53\n",
      "53.6\n",
      "43.2\n",
      "42\n",
      "23.1\n",
      "209\n",
      "18.3\n",
      "YOLOv5s\n",
      "-\n",
      "CSP-Darknet-53\n",
      "46.7\n",
      "34.8\n",
      "34.7\n",
      "19.2\n",
      "13.9\n",
      "12.0\n",
      "YOLOv7 [50]\n",
      "-\n",
      "ELAN\n",
      "51.5\n",
      "42.3\n",
      "40.1\n",
      "21.8\n",
      "71.5\n",
      "1.7\n",
      "YOLOv8 [51]\n",
      "YOLOv8n\n",
      "CSP-Darknet-53\n",
      "44.0\n",
      "33.2\n",
      "33.5\n",
      "19.5\n",
      "6.7\n",
      "4.2\n",
      "YOLOv8s\n",
      "51.1\n",
      "39.1\n",
      "39.6\n",
      "23.8\n",
      "7.8\n",
      "11.1\n",
      "YOLOv8m\n",
      "55.8\n",
      "42.6\n",
      "44.5\n",
      "26.6\n",
      "16.8\n",
      "25.9\n",
      "SOD-YOLOv8s\n",
      "-\n",
      "CSP-Darknet-53\n",
      "53.9\n",
      "43.9\n",
      "45.1\n",
      "26.6\n",
      "17.7\n",
      "11.5\n",
      "TABLE IV\n",
      "RESULTS FROM DIFFERENT WIDELY USED MODELS, PRESENTED AS\n",
      "PERCENTAGES.(THE BEST-PERFORMING OUTCOMES ARE HIGHLIGHTED\n",
      "IN BOLD)\n",
      "Models\n",
      "Backbone\n",
      "AP0.5\n",
      "AP0.5:0.95\n",
      "Faster R-CNN [48]\n",
      "ResNet\n",
      "37.8\n",
      "21.5\n",
      "Cascade R-CNN [52]\n",
      "ResNet\n",
      "39.4\n",
      "24.2\n",
      "CenterNet [53]\n",
      "ResNet50 [54]\n",
      "39.1\n",
      "22.8\n",
      "SSD [19]\n",
      "MobileNetV2 [55]\n",
      "33.7\n",
      "19\n",
      "SOD-YOLOv8s\n",
      "CSP-Darknet-53\n",
      "45.1\n",
      "26.6\n",
      "Fig. 10.\n",
      "(a) Training progress plot comparing YOLOv8s-GFPN-EMA,\n",
      "YOLOv8s-GFPN, and YOLOv8s based on mAP0.5 (b) and precision\n",
      "This illustrates that SOD-YOLOv8 significantly enhances\n",
      "detection performance, particularly for small and challenging\n",
      "objects, without introducing significant complexity.\n",
      "5) Visual assessment: We conducted visual experiments\n",
      "to evaluate our model’s detection performance. Our analysis\n",
      "included various metrics such as confusion matrices and\n",
      "inference test results. To validate the effectiveness of our\n",
      "method in challenging real-world scenarios, we performed\n",
      "inference tests using images captured by a camera mounted\n",
      "on the 12th floor of a building. This scenario involves\n",
      "capturing\n",
      "images\n",
      "from\n",
      "a\n",
      "high\n",
      "vantage\n",
      "point,\n",
      "posing\n",
      "challenges in detecting numerous small objects in a crowded\n",
      "traffic scene at an intersection.\n",
      "VisDrone2019 dataset results\n",
      "To visualize the performance of SOD-YOLOv8s, we uti-\n",
      "lize a confusion matrix. This matrix organizes predictions\n",
      "into a format where each row corresponds to instances of a\n",
      "true class label, and each column corresponds to instances\n",
      "predicted by the model. Diagonal elements indicate correct\n",
      "predictions, where the predicted class matches the actual\n",
      "class. Off-diagonal elements represent incorrect predictions,\n",
      "Fig. 11.\n",
      "Confusion matrix of YOLOv8s; (b) confusion matrix of proposed\n",
      "model.\n",
      "Fig. 12.\n",
      "Inference results for (a) YOLOv8s and (b) SOD-YOLOv8s across\n",
      "diverse scenarios including distant and high-density objects, as well as\n",
      "nighttime scenarios, using the VisDrone2019 dataset.\n",
      "where the predicted class does not match the actual class.\n",
      "Figure 11 demonstrates improved detection performance\n",
      "of SOD-YOLOv8s across most object categories. The confu-\n",
      "sion matrix shows lighter shades in the last row compared to\n",
      "YOLOv8s, indicating reduced misclassifications of objects as\n",
      "background. However, challenges remain in accurately iden-\n",
      "tifying bicycles, tricycles, and awning-tricycles, which are\n",
      "often mislabeled as background. Despite these issues, SOD-\n",
      "YOLOv8s shows darker shades along the main diagonal,\n",
      "indicating an overall increase in correctly detected objects.\n",
      "As depicted in Figure 12, we assess the efficacy of\n",
      "TABLE V\n",
      "COMPARATIVE EXPERIMENTS BETWEEN THE ENHANCED MODEL AND YOLOV8S ACROSS VARIOUS CATEGORIES, WITH PERCENTAGES PRESENTED\n",
      "(BEST-PERFORMING OUTCOMES HIGHLIGHTED IN BOLD).\n",
      "Models\n",
      "Pedestrian\n",
      "People\n",
      "Bicycle\n",
      "Car\n",
      "Van\n",
      "Truck\n",
      "Bus\n",
      "Motorcycle\n",
      "mAP0.5\n",
      "YOLOv8s\n",
      "43.5\n",
      "34.2\n",
      "14.9\n",
      "79.5\n",
      "45.0\n",
      "40.3\n",
      "58.1\n",
      "45.4\n",
      "40.6\n",
      "YOLOv8s-PIoU\n",
      "46\n",
      "38.2\n",
      "15.6\n",
      "80.4\n",
      "45.8\n",
      "39\n",
      "60.2\n",
      "47\n",
      "41.7\n",
      "YOLOv8s-PIoU-GFPN\n",
      "52.8\n",
      "44.0\n",
      "17.7\n",
      "84.2\n",
      "47.7\n",
      "39.4\n",
      "60.8\n",
      "53.2\n",
      "44.6\n",
      "YOLOv8s-PIoU-GFPN-EMA\n",
      "53.1\n",
      "44.5\n",
      "18.2\n",
      "83.9\n",
      "47.1\n",
      "41.0\n",
      "60.9\n",
      "53.8\n",
      "45.1\n",
      "TABLE VI\n",
      "DETECTION RESULTS FOLLOWING THE ADOPTION OF DIFFERENT IMPROVEMENT STRATEGIES, PRESENTED AS PERCENTAGES.(THE\n",
      "BEST-PERFORMING OUTCOMES ARE HIGHLIGHTED IN BOLD)\n",
      "Baseline\n",
      "PIoU\n",
      "GFPN\n",
      "EMA\n",
      "Precision\n",
      "Recall\n",
      "mAP0.5\n",
      "mAP0.5:0.95\n",
      "Detection Time/ms\n",
      "Parameter/106\n",
      "✓\n",
      "51.2\n",
      "40.1\n",
      "40.6\n",
      "24\n",
      "7.8\n",
      "11.1\n",
      "✓\n",
      "✓\n",
      "52.8\n",
      "40.5\n",
      "41.7\n",
      "24.2\n",
      "7.4\n",
      "11.1\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "52.7\n",
      "44.3\n",
      "44.6\n",
      "26.3\n",
      "11.5\n",
      "11.5\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "53.9\n",
      "43.9\n",
      "45.1\n",
      "26.6\n",
      "11.6\n",
      "11.5\n",
      "SOD-YOLOv8 across three challenging scenarios within\n",
      "the\n",
      "Visdrone\n",
      "dataset:\n",
      "nighttime\n",
      "conditions,\n",
      "crowded\n",
      "scenes with high-density objects, and scenes with distant\n",
      "objects. Remarkably, across all three scenarios, notable\n",
      "improvements are observed. In the nighttime scenario,\n",
      "illustrated in the first row of Figure 12, objects are\n",
      "detected with higher IoU values, and a greater number\n",
      "of\n",
      "smaller\n",
      "objects\n",
      "are\n",
      "successfully\n",
      "identified.\n",
      "In\n",
      "the\n",
      "second scenario, depicted in the second row of Figure\n",
      "12,\n",
      "SOD-YOLOv8\n",
      "demonstrates\n",
      "superior\n",
      "performance\n",
      "by successfully detecting numerous small objects located\n",
      "at the corners of intersections, a task which YOLOv8s\n",
      "struggles with. Similarly, in the third scenario involving\n",
      "objects positioned farther from the camera, SOD-YOLOv8s\n",
      "excels in detecting objects with higher IoU values and\n",
      "successfully identifying a greater number of smaller objects.\n",
      "These results demonstrating the substantial improvements\n",
      "provided by SOD-YOLOv8s across different environmental\n",
      "conditions, indicating its reliability and effectiveness in\n",
      "detecting objects in challenging scenarios.\n",
      "Real dataset results\n",
      "This section evaluates the model’s performance in dy-\n",
      "namic, real-world challenging scenarios where the camera\n",
      "is mounted on a building at a significant distance from the\n",
      "objects of interest. To assess the applicability and general-\n",
      "ization of the proposed SOD-YOLOv8 model, we conducted\n",
      "inference experiments using real-world data from a traffic\n",
      "scene scenario. The image data were primarily captured\n",
      "by NSF PAWR COSMOS testbed cameras [56], [57], [58]\n",
      "mounted on the 12th floor of Columbia’s Mudd building (Fig.\n",
      "13), overlooking the intersection of 120th St. and Amsterdam\n",
      "Ave. in New York City. Images were specifically selected\n",
      "from this vantage point to utilize its elevated perspective\n",
      "and greater distance from the street. This viewpoint poses\n",
      "a unique challenge for object detection, requiring enhanced\n",
      "Fig. 13.\n",
      "The perspective captured by COSMOS cameras on the 12th floor\n",
      "of Columbia’s Mudd building overlooking the intersection [58].\n",
      "perception due to the reduced scale of objects, including\n",
      "various vehicle types and pedestrians.\n",
      "As depicted in Figure 14, we assess SOD-YOLOv8’s\n",
      "performance in three challenging real-world traffic scenarios\n",
      "using images from cameras on the 12th floor such as\n",
      "crowded scenes with high-density objects, distant objects,\n",
      "and nighttime conditions. Significant improvements are\n",
      "observed across all three scenarios. In the first scenario,\n",
      "shown\n",
      "in\n",
      "the\n",
      "top\n",
      "row\n",
      "of\n",
      "Figure\n",
      "14,\n",
      "SOD-YOLOv8\n",
      "outperforms YOLOv8s by successfully detecting numerous\n",
      "small-scale pedestrians at the corners of intersections, a task\n",
      "Fig. 14.\n",
      "Inference results for (a) YOLOv8s and (b) SOD-YOLOv8s across\n",
      "various scenarios, including scenes with distant and high-density objects, as\n",
      "well as nighttime scenarios, using the traffic scene dataset.\n",
      "where YOLOv8s struggles. In the second scenario, with\n",
      "distant objects, SOD-YOLOv8 shows superior performance,\n",
      "achieving higher IoU values and effectively detecting more\n",
      "small objects. In the nighttime scenario, shown in the\n",
      "third row of Figure 14, SOD-YOLOv8 achieves higher\n",
      "IoU values for detected objects and identifies more small\n",
      "objects compared to the YOLOv8s baseline model, despite\n",
      "challenging lighting conditions. These results illustrate the\n",
      "substantial improvements achieved by SOD-YOLOv8 across\n",
      "diverse environmental conditions, highlighting its reliability\n",
      "and effective object detection capabilities in challenging\n",
      "scenarios.\n",
      "VI. CONCLUSION\n",
      "Detecting small-scale objects in traffic scenarios presents\n",
      "significant challenges that can reduce overall effectiveness.\n",
      "To address these issues, we introduced SOD-YOLOv8, a\n",
      "specialized object detection model designed for aerial pho-\n",
      "tography and traffic scenes dominated by small objects. Built\n",
      "upon YOLOv8, this model integrates enhanced multi-path\n",
      "fusion inspired by the GFPN architecture of DAMO-YOLO\n",
      "models, facilitating effective feature fusion across layers\n",
      "and simplifying architecture through reparameterization. By\n",
      "leveraging a high-resolution fourth layer and incorporating a\n",
      "C2f-EMA structure, SOD-YOLOv8 prioritizes small objects,\n",
      "enhances feature fusion, and improves precise localization.\n",
      "Also PIoU is used as a replacement for CIoU, the IoU-based\n",
      "loss function in YOLOv8.\n",
      "The SOD-YOLOv8 model outperforms widely used mod-\n",
      "els such as CenterNet, Cascade R-CNN, SSD, and Faster R-\n",
      "CNN across various evaluation metrics. Our efficient model\n",
      "significantly enhances object detection performance without\n",
      "substantially increasing computational cost or detection time\n",
      "compared to YOLOv8s. It improves recall from 40.1%\n",
      "to 43.9%, precision from 51.2% to 53.9%, mAP0.5 from\n",
      "40.6% to 45.1%, and mAP0.5:0.95 from 24% to 26.6%. In\n",
      "real-world traffic scenarios captured by building-mounted\n",
      "cameras, SOD-YOLOv8 achieves higher IoU values and\n",
      "identifies more small objects than YOLOv8s, even under\n",
      "challenging conditions like poor lighting and crowded back-\n",
      "grounds. These capabilities make it ideal for applications\n",
      "such as UAV-based traffic monitoring.\n",
      "However, challenges remain in deploying small object de-\n",
      "tection methods in resource-constrained environments. While\n",
      "attention mechanisms and complex feature fusion improve\n",
      "performance in controlled settings, they may struggle with\n",
      "generalization across diverse environments and conditions,\n",
      "complicating real-world deployment and maintenance. In this\n",
      "study, given the promising results of the used PIoU method\n",
      "on the VisDrone dataset and real-world traffic scenes, which\n",
      "involve numerous small objects, future research will prior-\n",
      "itize evaluating PIoU across various datasets. Additionally,\n",
      "efforts will focus on refining the GFPN architecture, explor-\n",
      "ing alternative processing methods, and assessing the model’s\n",
      "performance in adverse weather conditions to enhance its\n",
      "adaptability and robustness across diverse scenarios.\n",
      "ACKNOWLEDGMENT\n",
      "This work was supported by the Center for Smart\n",
      "Streetscapes, an NSF Engineering Research Center, under\n",
      "grant agreement EEC-2133516. The authors are grateful to\n",
      "Eric Valasek and Nicholas D’Andre from Gridmatrix for\n",
      "motivating the small object detection problem through a\n",
      "number of discussions\n",
      "REFERENCES\n",
      "[1] Chen, X., Ma, H., Wan, J., Li, B., and Xia, T. (2017). Multi-view\n",
      "3D object detection network for autonomous driving. In Proceedings\n",
      "of the IEEE conference on computer vision and pattern recognition\n",
      "(CVPR) (pp. 6526–6534).\n",
      "[2] M. Alqarqaz, M. Bani Younes, and R. Qaddoura, “An Object Classi-\n",
      "fication Approach for Autonomous Vehicles Using Machine Learning\n",
      "Techniques,” World Electric Vehicle Journal, vol. 14, no. 2, p. 41,\n",
      "2023.\n",
      "[3] Y. Lim, S. S. Tiang, W. H. Lim, C. H. Wong, M. Mastaneh, K. S.\n",
      "Chong, and B. Sun, “Object Detection in Autonomous Vehicles: A\n",
      "Performance Analysis,” in International Conference on Mechatron-\n",
      "ics and Intelligent Robotics, Singapore, August 2023, pp. 277-291.\n",
      "Springer Nature Singapore.\n",
      "[4] Feng, J., Wang, J., and Qin, R. (2023). Lightweight detection network\n",
      "for arbitrary-oriented vehicles in UAV imagery via precise positional\n",
      "information encoding and bidirectional feature fusion. International\n",
      "Journal of Remote Sensing, 44(15), 4529-4558.\n",
      "[5] Q. Chuai, X. He, and Y. Li, “Improved Traffic Small Object Detection\n",
      "via Cross-Layer Feature Fusion and Channel Attention,” Electronics,\n",
      "vol. 12, no. 16, p. 3421, 2023.\n",
      "[6] Salimans, T., Goodfellow, I., Zaremba, W., Cheung, V., Radford,\n",
      "A., and Chen, X. (2016). Improved Techniques for Training GANs.\n",
      "Advances in Neural Information Processing Systems (NeurIPS).\n",
      "[7] Alsamhi, S. H., Shvetsov, A. V., Kumar, S., Shvetsova, S. V., Alhar-\n",
      "tomi, M. A., Hawbani, A., ... and Nyangaresi, V. O. (2022). UAV\n",
      "computing-assisted search and rescue mission framework for disaster\n",
      "and harsh environment mitigation. Drones, 6(7), 154.\n",
      "[8] Wang, G., Chen, Y., An, P., Hong, H., Hu, J., and Huang, T. (2023).\n",
      "UAV-YOLOv8: a small-object-detection model based on improved\n",
      "YOLOv8 for UAV aerial photography scenarios. Sensors, 23(16), 7190.\n",
      "[9] Jiang, Y., Tan, Z., Wang, J., Sun, X., Lin, M., and Li, H. (2022).\n",
      "Giraffedet: A heavy-neck paradigm for object detection. arXiv preprint\n",
      "arXiv:2202.04256.\n",
      "[10] D. Ouyang, S. He, G. Zhang, M. Luo, H. Guo, J. Zhan, and Z. Huang,\n",
      "”Efficient multi-scale attention module with cross-spatial learning,”\n",
      "in ICASSP 2023-2023 IEEE International Conference on Acoustics,\n",
      "Speech and Signal Processing (ICASSP), pp. 1-5, IEEE, June 2023.\n",
      "[11] Xu, X., Jiang, Y., Chen, W., Huang, Y., Zhang, Y., and Sun, X.\n",
      "(2022). Damo-yolo: A report on real-time object detection design.\n",
      "arXiv preprint arXiv:2211.15444.\n",
      "[12] Du, Y., and Jiang, X. (2024). A Real-Time Small Target Vehicle\n",
      "Detection Algorithm with an Improved YOLOv5m Network Model.\n",
      "Computers, Materials and Continua, 78(1).\n",
      "[13] Girshick, R., Donahue, J., Darrell, T., and Malik, J. (2014). Rich\n",
      "feature hierarchies for accurate object detection and semantic segmen-\n",
      "tation. In Proceedings of the IEEE Conference on Computer Vision and\n",
      "Pattern Recognition (pp. 580–587). Columbus, OH, USA.\n",
      "[14] Girshick, R. (2015). Fast R-CNN. arXiv preprint arXiv:1504.08083.\n",
      "[15] Ren, S., He, K., Girshick, R., and Sun, J. (2015). Faster R-CNN:\n",
      "Towards real-time object detection with region proposal networks. In\n",
      "Proceedings of the 28th International Conference on Neural Infor-\n",
      "mation Processing Systems (pp. 21–37). Montreal, QC, Canada: MIT\n",
      "Press.\n",
      "[16] Deng, Z., Sun, H., Zhou, S., Zhao, J., Lei, L., and Zou, H. (2018).\n",
      "Multi-scale object detection in remote sensing imagery with convolu-\n",
      "tional neural networks. ISPRS Journal of Photogrammetry and Remote\n",
      "Sensing, 145, 3-22.\n",
      "[17] Redmon, J., Divvala, S., Girshick, R., and Farhadi, A. (2016). You\n",
      "Only Look Once: Unified, Real-Time Object Detection. In Proceedings\n",
      "of the IEEE Computer Society Conference on Computer Vision and\n",
      "Pattern Recognition, Las Vegas, NV, USA, 27–30 June 2016 (pp.\n",
      "779–788). IEEE Computer Society: Washington DC, USA.\n",
      "[18] Redmon, J., and Farhadi, A. (2017). YOLO9000: Better, faster,\n",
      "stronger. In Proceedings of the IEEE Conference on Computer Vision\n",
      "and Pattern Recognition, Honolulu, HI, USA, 21–26 July 2017 (pp.\n",
      "7263–7271).\n",
      "[19] Liu, W., Anguelov, D., Erhan, D., Szegedy, C., Reed, S., Fu, C.\n",
      "Y., and Berg, A. C. (2016). SSD: Single shot multibox detector. In\n",
      "Proceedings of the 14th European Conference on Computer Vision,\n",
      "Amsterdam, The Netherlands, 11–14 October 2016 (pp. 21–37).\n",
      "Springer: Cham, Switzerland.\n",
      "[20] Liu, M., Wang, X., Zhou, A., Fu, X., Ma, Y., and Piao, C. (2020). UAV-\n",
      "YOLO: Small object detection on unmanned aerial vehicle perspective.\n",
      "Sensors, 20(8), 2238.\n",
      "[21] Liu, W., Qiang, J., Li, X., Guan, P., and Du, Y. (2022). UAV image\n",
      "small object detection based on composite backbone network. Mobile\n",
      "Information Systems, 2022(1), 7319529.\n",
      "[22] Liu, Y., Yang, F., and Hu, P. (2020). Small-object detection in UAV-\n",
      "captured images via multi-branch parallel feature pyramid networks.\n",
      "IEEE Access, 8, 145740-145750.\n",
      "[23] Zhu, P.; Wen, L.; Du, D.; Bian, X.; Fan, H.; Hu, Q.; Ling, H. Detection\n",
      "and Tracking Meet Drones Challenge. IEEE Transactions on Pattern\n",
      "Analysis and Machine Intelligence 2021, 44, 7380–7399.\n",
      "[24] Lai, H., Chen, L., Liu, W., Yan, Z., and Ye, S. (2023). STC-\n",
      "YOLO: Small object detection network for traffic signs in complex\n",
      "environments. Sensors, 23(11), 5307.\n",
      "[25] Shen, L., Lang, B., and Song, Z. (2023). DS-YOLOv8-based object de-\n",
      "tection method for remote sensing images. IEEE Access, 11, 125122-\n",
      "125137.\n",
      "[26] Tong, Z., Chen, Y., Xu, Z., and Yu, R. (2023). Wise-IoU: bounding\n",
      "box regression loss with dynamic focusing mechanism. arXiv preprint\n",
      "arXiv:2301.10051.\n",
      "[27] Wang, H., Yang, H., Chen, H., Wang, J., Zhou, X., and Xu, Y. (2024).\n",
      "A remote sensing image target detection algorithm based on improved\n",
      "YOLOv8. Applied Sciences, 14(4), 1557.\n",
      "[28] Xia, G. S., Bai, X., Ding, J., Zhu, Z., Belongie, S., Luo, J., ... and\n",
      "Zhang, L. (2018). DOTA: A large-scale dataset for object detection in\n",
      "aerial images. In Proceedings of the IEEE Conference on Computer\n",
      "Vision and Pattern Recognition (pp. 3974-3983).\n",
      "[29] Xu, W., Cui, C., Ji, Y., Li, X., and Li, S. (2024). YOLOv8-MPEB small\n",
      "target detection algorithm based on UAV images. Heliyon, 10(8).\n",
      "[30] Redmon, J., and Farhadi, A. (2018). Yolov3: An incremental improve-\n",
      "ment. arXiv preprint arXiv:1804.02767.\n",
      "[31] Tan, M., Pang, R., and Le, Q. V. (2020). Efficientdet: Scalable and\n",
      "efficient object detection. In Proceedings of the IEEE Conference\n",
      "on Computer Vision and Pattern Recognition (CVPR), 2020, pp.\n",
      "10781–10790.\n",
      "[32] Chien-Yao Wang, Hong-Yuan Mark Liao, Yueh-Hua Wu, Ping-Yang\n",
      "Chen, Jun-Wei Hsieh, and I-Hau Yeh. CSPNet: A new backbone\n",
      "that can enhance learning capability of CNN. In Proceedings of the\n",
      "IEEE/CVF Conference on Computer Vision and Pattern Recognition\n",
      "Workshops, pages 390–391, 2020.\n",
      "[33] Kang, M., Ting, C. M., Ting, F. F., and Phan, R. C. W. (2023). Bgf-\n",
      "yolo: Enhanced yolov8 with multiscale attentional feature fusion for\n",
      "brain tumor detection. arXiv preprint arXiv:2309.12585.\n",
      "[34] He, K., Zhang, X., Ren, S., and Sun, J. (2015). Spatial pyramid\n",
      "pooling in deep convolutional networks for visual recognition. IEEE\n",
      "Transactions on Pattern Analysis and Machine Intelligence, 37(9),\n",
      "1904-1916.\n",
      "[35] Lin, T.-Y., Dollar, P., Girshick, R., He, K., Hariharan, B., and Be-\n",
      "longie, S. (2017). Feature pyramid networks for object detection. In\n",
      "Proceedings of the IEEE Conference on Computer Vision and Pattern\n",
      "Recognition (CVPR), 2017, pp. 2117–2125.\n",
      "[36] Liu, S.; Qi, L.; Qin, H.; Shi, J.; Jia, J. Path Aggregation Network\n",
      "for Instance Segmentation. In Proceedings of the IEEE Conference on\n",
      "Computer Vision and Pattern Recognition, Salt Lake City, UT, USA,\n",
      "18–23 June 2018; pp. 8759–8768.\n",
      "[37] Feng, C.; Zhong, Y.; Gao, Y.; Scott, M.R.; Huang, W. TOOD: Task-\n",
      "Aligned One-Stage Object Detection. In Proceedings of the 2021 IEEE\n",
      "International Conference on Computer Vision (ICCV), Montreal, QC,\n",
      "Canada, 10–17 October 2021; pp. 3490–3499.\n",
      "[38] Li, X., Wang, W., Wu, L., Chen, S., Hu, X., Li, J., Tang, J.,\n",
      "and Yang, J. (2020). Generalized Focal Loss: Learning Qualified\n",
      "and Distributed Bounding Boxes for Dense Object Detection. arXiv,\n",
      "arXiv:2006.04388.\n",
      "[39] Zheng, Z., Wang, P., Liu, W., Li, J., Ye, R., and Ren, D. (2020).\n",
      "Distance-iou loss: Faster and better learning for bounding box regres-\n",
      "sion. In Proceedings of the AAAI Conference on Artificial Intelligence,\n",
      "Vol. 34 (07), (pp. 12993–13000).\n",
      "[40] Yu, J., Jiang, Y., Wang, Z., Cao, Z., and Huang, T. (2016). Unitbox:\n",
      "An advanced object detection network. In Proceedings of the 24th\n",
      "ACM International Conference on Multimedia (pp. 516–520).\n",
      "[41] Wang, H., Yang, H., Chen, H., Wang, J., Zhou, X., and Xu, Y. (2024).\n",
      "A remote sensing image target detection algorithm based on improved\n",
      "YOLOv8. Applied Sciences, 14(4), 1557.\n",
      "[42] Liu, C., Wang, K., Li, Q., Zhao, F., Zhao, K., and Ma, H. (2024).\n",
      "Powerful-IoU: More straightforward and faster bounding box regres-\n",
      "sion loss with a nonmonotonic focusing mechanism. Neural Networks,\n",
      "170, 276-284.\n",
      "[43] Loshchilov, I., Hutter, F. (2017). SGDR: Stochastic Gradient Descent\n",
      "with Warm Restarts. Proceedings of the 5th International Conference\n",
      "on Learning Representations (ICLR), Toulon, France, 24-26 April\n",
      "2017.\n",
      "[44] Siliang, M., and Yong, X. (2023). Mpdiou: a loss for efficient and\n",
      "accurate bounding box regression. arXiv preprint arXiv:2307.07662.\n",
      "[45] Woo, S., Park, J., Lee, J. Y., and Kweon, I. S. (2018). CBAM: Con-\n",
      "volutional Block Attention Module. In Proceedings of the European\n",
      "Conference on Computer Vision (ECCV) (pp. 3-19).\n",
      "[46] Hou, Q., Zhou, D., and Feng, J. (2021). Coordinate Attention for\n",
      "Efficient Mobile Network Design. In Proceedings of the IEEE/CVF\n",
      "Conference on Computer Vision and Pattern Recognition (pp. 13713-\n",
      "13722).\n",
      "[47] Hu, J., Shen, L., and Sun, G. (2018). Squeeze-and-Excitation Net-\n",
      "works. In Proceedings of the IEEE/CVF Conference on Computer\n",
      "Vision and Pattern Recognition (pp. 7132-7141).\n",
      "[48] Ren, S.; He, K.; Girshick, R.; Sun, J. (2017). Faster R-CNN: Towards\n",
      "Real-Time Object Detection with Region Proposal Networks. IEEE\n",
      "Trans. Pattern Anal. Mach. Intell., 39, 1137–1149.\n",
      "[49] Ren, S., He, K., Girshick, R., Sun, J. (2015). Faster R-CNN: Towards\n",
      "Real-Time Object Detection with Region Proposal Networks. In Pro-\n",
      "ceedings of the Advances in Neural Information Processing Systems\n",
      "28, Montreal, QC, Canada, 7–12 December 2015.\n",
      "[50] Chien-Yao Wang, Alexander Bochkovskiy, and Hong-Yuan Mark Liao.\n",
      "YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-\n",
      "time object detectors. In Proceedings of the IEEE/CVF Conference on\n",
      "Computer Vision and Pattern Recognition, pages 7464–7475, 2023.\n",
      "[51] Reis, D., Kupec, J., Hong, J., and Daoudi, A. (2023). Real-time flying\n",
      "object detection with YOLOv8. arXiv preprint arXiv:2305.09972.\n",
      "[52] Cai, Z.; Vasconcelos, N. (2018). Cascade R-CNN: Delving into High\n",
      "Quality Object Detection.\n",
      "In Proceedings of the 2018 IEEE/CVF\n",
      "Conference on Computer Vision and Pattern Recognition, Salt Lake\n",
      "City, UT, USA, 18–22 June 2018; pp. 6154–6162.\n",
      "[53] Duan, K., Bai, S., Xie, L., Qi, H., Huang, Q., and Tian, Q. (2019).\n",
      "CenterNet: Keypoint triplets for object detection. In Proceedings of the\n",
      "IEEE/CVF International Conference on Computer Vision, pp. 6569-\n",
      "6578.\n",
      "[54] He, K., Zhang, X., Ren, S., and Sun, J. (2016). Deep residual learning\n",
      "for image recognition.\n",
      "In Proceedings of the IEEE Conference on\n",
      "Computer Vision and Pattern Recognition (CVPR), pp. 770-778.\n",
      "[55] Sandler, M., Howard, A., Zhu, M., Zhmoginov, A., and Chen, L. C.\n",
      "(2018). MobileNetV2: Inverted residuals and linear bottlenecks. In\n",
      "Proceedings of the IEEE Conference on Computer Vision and Pattern\n",
      "Recognition (CVPR), pp. 4510-4520.\n",
      "[56] D. Raychaudhuri, I. Seskar, G. Zussman, T. Korakis, D. Kilper, T.\n",
      "Chen, J. Kolodziejski, M. Sherman, Z. Kostic, X. Gu, H. Krish-\n",
      "naswamy, S. Maheshwari, P. Skrimponis, and C. Gutterman, ”Chal-\n",
      "lenge: COSMOS: A city-scale programmable testbed for experimen-\n",
      "tation with advanced wireless,” in Proc. ACM MobiCom’20, 2020.\n",
      "[57] Z. Kostic, A. Angus, Z. Yang, Z. Duan, I. Seskar, G. Zussman, and D.\n",
      "Raychaudhuri, ”Smart city intersections: Intelligence nodes for future\n",
      "metropolises,” IEEE Comp., vol. 55, no. 12, pp. 74–85, 2022.\n",
      "[58] COSMOS Project, “Hardware: Cameras,” 2022. [Online]. Available:\n",
      "https://wiki.cosmos-lab.org/wiki/Hardware/\n",
      "Cameras.\n",
      "' metadata={'Published': '2024-08-08', 'Title': 'SOD-YOLOv8 -- Enhancing YOLOv8 for Small Object Detection in Traffic Scenes', 'Authors': 'Boshra Khalili, Andrew W. Smyth', 'Summary': 'Object detection as part of computer vision can be crucial for traffic\\nmanagement, emergency response, autonomous vehicles, and smart cities. Despite\\nsignificant advances in object detection, detecting small objects in images\\ncaptured by distant cameras remains challenging due to their size, distance\\nfrom the camera, varied shapes, and cluttered backgrounds. To address these\\nchallenges, we propose Small Object Detection YOLOv8 (SOD-YOLOv8), a novel\\nmodel specifically designed for scenarios involving numerous small objects.\\nInspired by Efficient Generalized Feature Pyramid Networks (GFPN), we enhance\\nmulti-path fusion within YOLOv8 to integrate features across different levels,\\npreserving details from shallower layers and improving small object detection\\naccuracy. Also, A fourth detection layer is added to leverage high-resolution\\nspatial information effectively. The Efficient Multi-Scale Attention Module\\n(EMA) in the C2f-EMA module enhances feature extraction by redistributing\\nweights and prioritizing relevant features. We introduce Powerful-IoU (PIoU) as\\na replacement for CIoU, focusing on moderate-quality anchor boxes and adding a\\npenalty based on differences between predicted and ground truth bounding box\\ncorners. This approach simplifies calculations, speeds up convergence, and\\nenhances detection accuracy. SOD-YOLOv8 significantly improves small object\\ndetection, surpassing widely used models in various metrics, without\\nsubstantially increasing computational cost or latency compared to YOLOv8s.\\nSpecifically, it increases recall from 40.1\\\\% to 43.9\\\\%, precision from 51.2\\\\%\\nto 53.9\\\\%, $\\\\text{mAP}_{0.5}$ from 40.6\\\\% to 45.1\\\\%, and\\n$\\\\text{mAP}_{0.5:0.95}$ from 24\\\\% to 26.6\\\\%. In dynamic real-world traffic\\nscenes, SOD-YOLOv8 demonstrated notable improvements in diverse conditions,\\nproving its reliability and effectiveness in detecting small objects even in\\nchallenging environments.'}\n"
     ]
    }
   ],
   "source": [
    "#%pip install -qU langchain-community arxiv pymupdf\n",
    "\n",
    "from langchain_community.document_loaders import ArxivLoader\n",
    "\n",
    "# Supports all arguments of `ArxivAPIWrapper`\n",
    "loader = ArxivLoader(\n",
    "    query=\"Yolov8\",\n",
    "    load_max_docs=2,\n",
    "    # doc_content_chars_max=1000,\n",
    "    # load_all_available_meta=False,\n",
    "    # ...\n",
    ")\n",
    "\n",
    "docs = loader.load()\n",
    "print(docs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use summaries of Arvix paper as documents rather than raw papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'Entry ID': 'http://arxiv.org/abs/2408.04786v1', 'Published': datetime.date(2024, 8, 8), 'Title': 'SOD-YOLOv8 -- Enhancing YOLOv8 for Small Object Detection in Traffic Scenes', 'Authors': 'Boshra Khalili, Andrew W. Smyth'}, page_content='Object detection as part of computer vision can be crucial for traffic\\nmanagement, emergency response, autonomous vehicles, and smart cities. Despite\\nsignificant advances in object detection, detecting small objects in images\\ncaptured by distant cameras remains challenging due to their size, distance\\nfrom the camera, varied shapes, and cluttered backgrounds. To address these\\nchallenges, we propose Small Object Detection YOLOv8 (SOD-YOLOv8), a novel\\nmodel specifically designed for scenarios involving numerous small objects.\\nInspired by Efficient Generalized Feature Pyramid Networks (GFPN), we enhance\\nmulti-path fusion within YOLOv8 to integrate features across different levels,\\npreserving details from shallower layers and improving small object detection\\naccuracy. Also, A fourth detection layer is added to leverage high-resolution\\nspatial information effectively. The Efficient Multi-Scale Attention Module\\n(EMA) in the C2f-EMA module enhances feature extraction by redistributing\\nweights and prioritizing relevant features. We introduce Powerful-IoU (PIoU) as\\na replacement for CIoU, focusing on moderate-quality anchor boxes and adding a\\npenalty based on differences between predicted and ground truth bounding box\\ncorners. This approach simplifies calculations, speeds up convergence, and\\nenhances detection accuracy. SOD-YOLOv8 significantly improves small object\\ndetection, surpassing widely used models in various metrics, without\\nsubstantially increasing computational cost or latency compared to YOLOv8s.\\nSpecifically, it increases recall from 40.1\\\\% to 43.9\\\\%, precision from 51.2\\\\%\\nto 53.9\\\\%, $\\\\text{mAP}_{0.5}$ from 40.6\\\\% to 45.1\\\\%, and\\n$\\\\text{mAP}_{0.5:0.95}$ from 24\\\\% to 26.6\\\\%. In dynamic real-world traffic\\nscenes, SOD-YOLOv8 demonstrated notable improvements in diverse conditions,\\nproving its reliability and effectiveness in detecting small objects even in\\nchallenging environments.')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = loader.get_summaries_as_docs()\n",
    "docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Document Doader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create an example of a standard document loader that loads a file and creates a document from each line in the file.\n",
    "\n",
    "Document - Contains text and metadata\n",
    "BaseLoader - Use to convert raw data into Documents\n",
    "Blob - A representation of binary data that's located either in a file or in memory\n",
    "BaseBlobParser - Logic to parse a Blob to yield Document objects\n",
    "\n",
    "Create a standard document Loader by sub-classing from BaseLoader.\n",
    "\n",
    "Create a parser using BaseBlobParser and use it in conjunction with Blob and BlobLoaders. This is useful primarily when working with files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<class 'langchain_core.documents.base.Document'>\n",
      "page_content='meow meow🐱 \n",
      "' metadata={'line_number': 0, 'source': './meow.txt'}\n",
      "\n",
      "<class 'langchain_core.documents.base.Document'>\n",
      "page_content=' meow meow🐱 \n",
      "' metadata={'line_number': 1, 'source': './meow.txt'}\n",
      "\n",
      "<class 'langchain_core.documents.base.Document'>\n",
      "page_content=' meow😻😻' metadata={'line_number': 2, 'source': './meow.txt'}\n"
     ]
    }
   ],
   "source": [
    "from typing import AsyncIterator, Iterator\n",
    "\n",
    "from langchain_core.document_loaders import BaseLoader\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "\n",
    "class CustomDocumentLoader(BaseLoader):\n",
    "    \"\"\"An example document loader that reads a file line by line.\"\"\"\n",
    "\n",
    "    def __init__(self, file_path: str) -> None:\n",
    "        \"\"\"Initialize the loader with a file path.\n",
    "\n",
    "        Args:\n",
    "            file_path: The path to the file to load.\n",
    "        \"\"\"\n",
    "        self.file_path = file_path\n",
    "\n",
    "    def lazy_load(self) -> Iterator[Document]:  # <-- Does not take any arguments\n",
    "        \"\"\"A lazy loader that reads a file line by line.\n",
    "\n",
    "        When you're implementing lazy load methods, you should use a generator\n",
    "        to yield documents one by one.\n",
    "        \"\"\"\n",
    "        with open(self.file_path, encoding=\"utf-8\") as f:\n",
    "            line_number = 0\n",
    "            for line in f:\n",
    "                yield Document(\n",
    "                    page_content=line,\n",
    "                    metadata={\"line_number\": line_number, \"source\": self.file_path},\n",
    "                )\n",
    "                line_number += 1\n",
    "\n",
    "with open(\"./meow.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    quality_content = \"meow meow🐱 \\n meow meow🐱 \\n meow😻😻\"\n",
    "    f.write(quality_content)\n",
    "\n",
    "loader = CustomDocumentLoader(\"./meow.txt\")\n",
    "\n",
    "for doc in loader.lazy_load():\n",
    "    print()\n",
    "    print(type(doc))\n",
    "    print(doc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
