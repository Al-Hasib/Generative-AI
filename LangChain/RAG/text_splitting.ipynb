{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mastering Text Splitting in Langchain\n",
    "\n",
    "Retrieval-Augmented Generation (RAG) has emerged as a powerful technique for enhancing the accuracy and relevance of AI-generated responses. At the heart of RAG lies a crucial step: “text splitting”. This process involves breaking down large documents into smaller, manageable chunks that can be efficiently processed and retrieved.\n",
    "\n",
    "Langchain, a popular framework for developing applications with large language models (LLMs), offers a variety of text splitting techniques. \n",
    "\n",
    "CharacterTextSplitter: The Simple Solution\n",
    "The CharacterTextSplitter is the most basic text splitting technique in Langchain. It divides text based on a specified number of characters, making it suitable for simple, uniform text splitting tasks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "text = \"Your long document text here...\"\n",
    "\n",
    "splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\\n\",  #used to avoid splitting in the middle of paragraphs.\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200\n",
    ")\n",
    "\n",
    "chunks = splitter.split_text(text) #you can also split documents using split_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Your long document text here...']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RecursiveCharacterTextSplitter: The Versatile Powerhouse\n",
    "\n",
    "The RecursiveCharacterTextSplitter is Langchain’s most versatile text splitter. It attempts to split text on a list of characters in order, falling back to the next option if the resulting chunks are too large.\n",
    "\n",
    "When to use:\n",
    "- As a default choice for most general-purpose text splitting tasks.\n",
    "- When dealing with various document types with different structures.\n",
    "- To maintain semantic coherence in splits as much as possible.\n",
    "\n",
    "This splitter tries to split on double newlines first, then single newlines, spaces, and finally individual characters if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text = \"Your long document text here...\"\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len\n",
    ")\n",
    "\n",
    "chunks = splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ". TokenTextSplitter: Precision for Token-Based Models\n",
    "\n",
    "The TokenTextSplitter is designed to split text based on the number of tokens, which is particularly useful when working with models that have specific token limits, such as GPT-3 or any other models.\n",
    "\n",
    "When to use:\n",
    "- When working with token-sensitive models\n",
    "- To ensure that chunks fit within model token limits\n",
    "- For more precise control over input size for language models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import TokenTextSplitter\n",
    "\n",
    "text = \"Your long document text here...\"\n",
    "\n",
    "splitter = TokenTextSplitter(\n",
    "    encoding_name=\"cl100k_base\",  # OpenAI's encoding\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=20\n",
    ")\n",
    "\n",
    "chunks = splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Your long document text here...']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " MarkdownHeaderTextSplitter: Structure-Aware Splitting for Markdown\n",
    "The MarkdownHeaderTextSplitter is specially designed to handle Markdown documents, respecting the header hierarchy and document structure.\n",
    "\n",
    "When to use:\n",
    "- Specifically for Markdown documents\n",
    "- To maintain the logical structure of documentation or articles\n",
    "- When header-based organization is crucial for your RAG application \n",
    "\n",
    "This splitter creates chunks based on Markdown headers, preserving the document’s hierarchical structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
    "\n",
    "markdown_text = \"\"\"\n",
    "# Title\n",
    "## Section 1\n",
    "Content of section 1\n",
    "## Section 2\n",
    "Content of section 2\n",
    "### Subsection 2.1\n",
    "Content of subsection 2.1\n",
    "\"\"\"\n",
    "\n",
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\"),\n",
    "    (\"###\", \"Header 3\"),\n",
    "]\n",
    "\n",
    "splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "chunks = splitter.split_text(markdown_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PythonCodeTextSplitter: Tailored for Code Splitting\n",
    "\n",
    "The PythonCodeTextSplitter is designed specifically for splitting Python source code, respecting function and class boundaries.\n",
    "\n",
    "When to use:\n",
    "- When working with Python codebases\n",
    "- For code documentation or analysis tasks\n",
    "- To maintain the integrity of code structures in your splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['def function1():\\n    print(\"Hello, World!\")',\n",
       " 'class MyClass:\\n    def __init__(self):\\n        self.value = 42',\n",
       " 'def method1(self):\\n        return self.value']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import PythonCodeTextSplitter\n",
    "\n",
    "python_code = \"\"\"\n",
    "def function1():\n",
    "    print(\"Hello, World!\")\n",
    "\n",
    "class MyClass:\n",
    "    def __init__(self):\n",
    "        self.value = 42\n",
    "\n",
    "    def method1(self):\n",
    "        return self.value\n",
    "\"\"\"\n",
    "\n",
    "splitter = PythonCodeTextSplitter(\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=20\n",
    ")\n",
    "\n",
    "chunks = splitter.split_text(python_code)\n",
    "\n",
    "chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HTMLTextSplitter: Structured Splitting for Web Content\n",
    "The HTMLTextSplitter is tailored for HTML documents, maintaining the structure and hierarchy of HTML elements.\n",
    "\n",
    "When to use:\n",
    "- For processing web pages or HTML-formatted documents\n",
    "- When HTML structure is important for your RAG task\n",
    "- To extract content while preserving HTML context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'Header 1': 'Main Title'}, page_content='Main Title \\n This is a paragraph.'),\n",
       " Document(metadata={'Header 2': 'Subsection'}, page_content='Subsection \\n Another paragraph.')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!pip install langchain_text_splitter\n",
    "from langchain_text_splitters import HTMLSectionSplitter\n",
    "\n",
    "html_text = \"\"\"\n",
    "<html>\n",
    "<body>\n",
    "<h1>Main Title</h1>\n",
    "<p>This is a paragraph.</p>\n",
    "<div>\n",
    "    <h2>Subsection</h2>\n",
    "    <p>Another paragraph.</p>\n",
    "</div>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "headers_to_split_on = [(\"h1\", \"Header 1\"), (\"h2\", \"Header 2\")]\n",
    "splitter = HTMLSectionSplitter(\n",
    "    headers_to_split_on=headers_to_split_on,\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=20\n",
    ")\n",
    "\n",
    "chunks = splitter.split_text(html_text)\n",
    "\n",
    "chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTKTextSplitter: Linguistic-Based Splitting\n",
    "The NLTKTextSplitter uses the Natural Language Toolkit (NLTK) to split text based on linguistic features, such as sentences or paragraphs.\n",
    "\n",
    "When to use:\n",
    "- For more linguistically aware text splitting\n",
    "- When working with well-formatted, natural language text\n",
    "- To maintain sentence or paragraph integrity in your splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you’ll need to have NLTK installed and the necessary NLTK data downloaded for this splitter to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\hp/nltk_data'\n    - 'f:\\\\Youtube\\\\Generative AI\\\\Langchain\\\\env\\\\nltk_data'\n    - 'f:\\\\Youtube\\\\Generative AI\\\\Langchain\\\\env\\\\share\\\\nltk_data'\n    - 'f:\\\\Youtube\\\\Generative AI\\\\Langchain\\\\env\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\hp\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 10\u001b[0m\n\u001b[0;32m      3\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYour long document text here. It contains multiple sentences. Each sentence will be considered for splitting.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      5\u001b[0m splitter \u001b[38;5;241m=\u001b[39m NLTKTextSplitter(\n\u001b[0;32m      6\u001b[0m     chunk_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[0;32m      7\u001b[0m     chunk_overlap\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m\n\u001b[0;32m      8\u001b[0m )\n\u001b[1;32m---> 10\u001b[0m chunks \u001b[38;5;241m=\u001b[39m \u001b[43msplitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m chunks\n",
      "File \u001b[1;32mf:\\Youtube\\Generative AI\\Langchain\\env\\lib\\site-packages\\langchain_text_splitters\\nltk.py:30\u001b[0m, in \u001b[0;36mNLTKTextSplitter.split_text\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Split incoming text and return chunks.\"\"\"\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# First we naively split the large input into a bunch of smaller ones.\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m splits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_language\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_splits(splits, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_separator)\n",
      "File \u001b[1;32mf:\\Youtube\\Generative AI\\Langchain\\env\\lib\\site-packages\\nltk\\tokenize\\__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32mf:\\Youtube\\Generative AI\\Langchain\\env\\lib\\site-packages\\nltk\\tokenize\\__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[1;34m(language)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_punkt_tokenizer\u001b[39m(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;124;03m    a lru cache for performance.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;124;03m    :type language: str\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mf:\\Youtube\\Generative AI\\Langchain\\env\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m-> 1744\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mf:\\Youtube\\Generative AI\\Langchain\\env\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1747\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[1;32m-> 1749\u001b[0m     lang_dir \u001b[38;5;241m=\u001b[39m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1750\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[0;32m   1751\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang \u001b[38;5;241m=\u001b[39m lang\n",
      "File \u001b[1;32mf:\\Youtube\\Generative AI\\Langchain\\env\\lib\\site-packages\\nltk\\data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\hp/nltk_data'\n    - 'f:\\\\Youtube\\\\Generative AI\\\\Langchain\\\\env\\\\nltk_data'\n    - 'f:\\\\Youtube\\\\Generative AI\\\\Langchain\\\\env\\\\share\\\\nltk_data'\n    - 'f:\\\\Youtube\\\\Generative AI\\\\Langchain\\\\env\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\hp\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import NLTKTextSplitter\n",
    "\n",
    "text = \"Your long document text here. It contains multiple sentences. Each sentence will be considered for splitting.\"\n",
    "\n",
    "splitter = NLTKTextSplitter(\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=20\n",
    ")\n",
    "\n",
    "chunks = splitter.split_text(text)\n",
    "chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SpacyTextSplitter: Advanced Linguistic Splitting\n",
    "The SpacyTextSplitter leverages the spaCy library for more advanced, language-aware text splitting.\n",
    "\n",
    "When to use:\n",
    "- For highly accurate, linguistically informed splitting\n",
    "- When working with multiple languages\n",
    "- For tasks requiring deep language understanding\n",
    "\n",
    "Remember to install spaCy and the appropriate language model before using this splitter.\n",
    "\n",
    "Remember to install spaCy and the appropriate language model before using this splitter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Spacy is not installed, please install it with `pip install spacy`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32mf:\\Youtube\\Generative AI\\Langchain\\env\\lib\\site-packages\\langchain_text_splitters\\spacy.py:48\u001b[0m, in \u001b[0;36m_make_spacy_pipeline_for_splitting\u001b[1;34m(pipeline, max_length)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 48\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'spacy'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext_splitter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SpacyTextSplitter\n\u001b[0;32m      3\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYour long document text here. It can be in various languages. SpaCy will handle the linguistic nuances.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 5\u001b[0m splitter \u001b[38;5;241m=\u001b[39m \u001b[43mSpacyTextSplitter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunk_overlap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\n\u001b[0;32m      8\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m chunks \u001b[38;5;241m=\u001b[39m splitter\u001b[38;5;241m.\u001b[39msplit_text(text)\n\u001b[0;32m     11\u001b[0m chunks\n",
      "File \u001b[1;32mf:\\Youtube\\Generative AI\\Langchain\\env\\lib\\site-packages\\langchain_text_splitters\\spacy.py:29\u001b[0m, in \u001b[0;36mSpacyTextSplitter.__init__\u001b[1;34m(self, separator, pipeline, max_length, strip_whitespace, **kwargs)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Initialize the spacy text splitter.\"\"\"\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 29\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43m_make_spacy_pipeline_for_splitting\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpipeline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_separator \u001b[38;5;241m=\u001b[39m separator\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_strip_whitespace \u001b[38;5;241m=\u001b[39m strip_whitespace\n",
      "File \u001b[1;32mf:\\Youtube\\Generative AI\\Langchain\\env\\lib\\site-packages\\langchain_text_splitters\\spacy.py:50\u001b[0m, in \u001b[0;36m_make_spacy_pipeline_for_splitting\u001b[1;34m(pipeline, max_length)\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m---> 50\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m     51\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpacy is not installed, please install it with `pip install spacy`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     52\u001b[0m     )\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pipeline \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentencizer\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlang\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01men\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m English\n",
      "\u001b[1;31mImportError\u001b[0m: Spacy is not installed, please install it with `pip install spacy`."
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import SpacyTextSplitter\n",
    "\n",
    "text = \"Your long document text here. It can be in various languages. SpaCy will handle the linguistic nuances.\"\n",
    "\n",
    "splitter = SpacyTextSplitter(\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=20\n",
    ")\n",
    "\n",
    "chunks = splitter.split_text(text)\n",
    "chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SentenceTransformersTextSplitter: Semantic-Aware Splitting\n",
    "The SentenceTransformersTextSplitter uses sentence embeddings to create semantically coherent chunks.\n",
    "\n",
    "When to use:\n",
    "- For applications requiring semantic understanding\n",
    "- In conjunction with sentence embedding models\n",
    "- When semantic similarity between chunks is crucial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'SentenceTransformersTextSplitter' from 'langchain.text_splitter' (f:\\Youtube\\Generative AI\\Langchain\\env\\lib\\site-packages\\langchain\\text_splitter.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext_splitter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformersTextSplitter\n\u001b[0;32m      3\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYour long document text here. This splitter will attempt to create semantically coherent chunks.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      5\u001b[0m splitter \u001b[38;5;241m=\u001b[39m SentenceTransformersTextSplitter(\n\u001b[0;32m      6\u001b[0m     chunk_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[0;32m      7\u001b[0m     chunk_overlap\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m\n\u001b[0;32m      8\u001b[0m )\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'SentenceTransformersTextSplitter' from 'langchain.text_splitter' (f:\\Youtube\\Generative AI\\Langchain\\env\\lib\\site-packages\\langchain\\text_splitter.py)"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import SentenceTransformersTextSplitter\n",
    "\n",
    "text = \"Your long document text here. This splitter will attempt to create semantically coherent chunks.\"\n",
    "\n",
    "splitter = SentenceTransformersTextSplitter(\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=20\n",
    ")\n",
    "\n",
    "chunks = splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LatexTextSplitter: Specialized for LaTeX Documents\n",
    "The LatexTextSplitter is designed to handle LaTeX documents, respecting the unique structure and commands of LaTeX syntax.\n",
    "\n",
    "When to use:\n",
    "- Specifically for LaTeX documents\n",
    "- In academic or scientific document processing\n",
    "- To maintain LaTeX formatting and structure in splits\n",
    "\n",
    "This splitter attempts to maintain the integrity of LaTeX commands and environments while creating chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\\\documentclass{article}\\n\\\\begin{document}\\n\\\\section{Introduction}\\nThis is the',\n",
       " 'is the introduction.\\n\\\\section{Methodology}\\nThis is the methodology section.\\n\\\\end{document}']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import LatexTextSplitter\n",
    "\n",
    "latex_text = r\"\"\"\n",
    "\\documentclass{article}\n",
    "\\begin{document}\n",
    "\\section{Introduction}\n",
    "This is the introduction.\n",
    "\\section{Methodology}\n",
    "This is the methodology section.\n",
    "\\end{document}\n",
    "\"\"\"\n",
    "\n",
    "splitter = LatexTextSplitter(\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=20\n",
    ")\n",
    "\n",
    "chunks = splitter.split_text(latex_text)\n",
    "chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosing the right text splitter is crucial for optimizing your RAG pipeline in Langchain. Each splitter offers unique advantages suited to different document types and use cases. The RecursiveCharacterTextSplitter serves as an excellent default choice for general purposes, while specialized splitters like MarkdownHeaderTextSplitter or PythonCodeTextSplitter offer tailored solutions for specific document formats.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
