{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mastering Text Splitting in Langchain\n",
    "\n",
    "Retrieval-Augmented Generation (RAG) has emerged as a powerful technique for enhancing the accuracy and relevance of AI-generated responses. At the heart of RAG lies a crucial step: “text splitting”. This process involves breaking down large documents into smaller, manageable chunks that can be efficiently processed and retrieved.\n",
    "\n",
    "Langchain, a popular framework for developing applications with large language models (LLMs), offers a variety of text splitting techniques. \n",
    "\n",
    "CharacterTextSplitter: The Simple Solution\n",
    "The CharacterTextSplitter is the most basic text splitting technique in Langchain. It divides text based on a specified number of characters, making it suitable for simple, uniform text splitting tasks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLMs have limits on context window size in terms of token numbers. Even if the context size is infinite, more input tokens will lead to higher costs, and money is not infinite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text will be split only at new lines since we are using the new line (“\\n”) as the separator. \n",
    "\n",
    "Recursive\n",
    "Rather than using a single separator, we use multiple separators. This method will use each separator sequentially to split the data until the chunk reaches less than chunk_size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "text = \"Your long document text here...\"\n",
    "\n",
    "splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\\n\",  #used to avoid splitting in the middle of paragraphs.\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200\n",
    ")\n",
    "\n",
    "chunks = splitter.split_text(text) #you can also split documents using split_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Your long document text here...']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Text:\n",
      "Md Abdullah Al Hasib\n",
      "MACHINE LEARNING ENGINEER\n",
      "+8801741813559 |\n",
      "Mail |\n",
      "Abdullah Al Hasib |\n",
      "Al-Hasib |\n",
      "YouTube |\n",
      "Medium\n",
      "Gopalpur,Tangail, Bangladesh\n",
      "EXPERIENCE\n",
      "Machine Learning Engineer - remote (KBY-AI)\n",
      "SEP 2023 – MAY 2024\n",
      "• Design, develop, and optimize computer vision algorithms and models through GPU Servers\n",
      "• Train, fine-tune, and optimize deep learning models and neural networks for monitoring the cow farms such\n",
      "as incident detection, animal counting, feed lane detection etc\n",
      "• Integrate computer vision models into larger software systems or applications and deploy them into\n",
      "production environments. Engage in continuous learning and professional development activities.\n",
      "Jr. ML Engineer - remote (Namespace IT)\n",
      "JAN 2023 – FEB 2024\n",
      "• Content Writer at aionlinecourse in Machine Learning, Deep Leanring, Computer Vision related articles\n",
      "• Explore the updated technologies in the field of AI. Making projects in different domains of AI.\n",
      "PERSONAL PROJECTS\n",
      "License Plate Detection & Recognition (Project Demo)\n",
      "• Developed License plate detection through YOLOv8, extract the region of the license plate.\n",
      "• Read the license plate by easyocr and track the car with it’s license plate with bytetracker in a video.\n",
      "Sentiment Analysis (Project Demo)\n",
      "• Developed an app through streamlit that is able to predict the sentiment of a text through textblob, Vader\n",
      "& transformers. Also comparison the performance of the models.\n",
      "Forest Cover Type Prediction (Project Demo)\n",
      "• Developed machine learning multi class classification problem, experimenting with different models as well\n",
      "as perform hyperparameter tuning to get the best parameters.\n",
      "TECHNICAL SKILLS\n",
      "Domain Expertise: Machine Learning, Deep Learning, Computer Vision, NLP, Generative AI, MLOPs\n",
      "Languages: Python, R, HTML, SQL\n",
      "Libraries: Numpy, Pandas, Matplotlib, Seaborn, Scikit-Learn, XGboost, OpenCV, Ultralytics, Pillow, Nltk,\n",
      "Gensim, TextBlob, Huggingface, Streamlit, Altair, Scikit Image, StanfordNLP\n",
      "Frameworks: Tensorflow, Keras, Pytorch, Pytorch-lighting, Transformers, YOLO, Langchain, LlamaIndex,\n",
      "OPEN API, MLflow, DVC, AWS(Cloud), FastAPI\n",
      "Developer Tools: Git, Github, VS Code, Docker, Linux, GPU Server(CUDA)\n",
      "Soft Skills: Mentoring, Collaboration, Communication, Quick Learner, Time management\n",
      "OPEN SOURCE CONTRIBUTION\n",
      "• NoCodeTextClassifier: A Python package for Low Code/No Code Text Classification task\n",
      "PyPI\n",
      "• eng text cleaner: A Python package for Cleaning the text from unnecessary characters\n",
      "PyPI\n",
      "EDUCATION\n",
      "Islamic University, Bangladesh\n",
      "JAN 2019 – DEC 2024\n",
      "Bsc in Computer Science & Engineering\n",
      "VOLUNTEER & LEADERSHIP EXPERIENCE\n",
      "President | IU Machine Learning and Data Science Club - IUMLDSClub\n",
      "MAY 2024. - Present\n",
      "• Mentoring Machine Learning & associate fields to the members.\n",
      "• Helped members collaborate effectively and get things done.\n",
      "• Manage club events and projects.\n",
      "\n",
      "\n",
      "Extracted Links:\n",
      "mailto: alhasib.iu.cse@gmail.com\n",
      "https://www.linkedin.com/in/md-abdullah-al-hasib-874174194/\n",
      "https://github.com/Al-Hasib\n",
      "https://www.youtube.com/@mdabdullahalhasib9081\n",
      "https://medium.com/@abdullah.iu.cse\n",
      "https://www.linkedin.com/company/kby-ai-identity-verification-sdk/\n",
      "https://www.linkedin.com/company/namespaceit/\n",
      "https://www.aionlinecourse.com\n",
      "https://www.youtube.com/watch?v=ZKxgbBqu3sc\n",
      "https://www.youtube.com/watch?v=fJR7zzWEe5g\n",
      "https://github.com/Al-Hasib/End-to-End-Forest-Cover-Type-Prediction-ML-Project\n",
      "https://pypi.org/project/NoCodeTextClassifier/\n",
      "https://pypi.org/project/eng-text-cleaner/\n",
      "https://www.facebook.com/profile.php?id=61564502450867\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "\n",
    "# Function to extract text and links from PDF\n",
    "def load_pdf_with_links(pdf_file):\n",
    "    pdf_document = fitz.open(pdf_file)  # Open the PDF\n",
    "    pdf_text = \"\"\n",
    "    pdf_links = []\n",
    "\n",
    "    # Iterate through each page\n",
    "    for page_num in range(pdf_document.page_count):\n",
    "        page = pdf_document.load_page(page_num)  # Load individual page\n",
    "        pdf_text += page.get_text(\"text\")  # Extract text\n",
    "\n",
    "        # Extract links from the page\n",
    "        links = page.get_links()\n",
    "        for link in links:\n",
    "            if 'uri' in link:  # Check if it's a URI (URL)\n",
    "                pdf_links.append(link['uri'])\n",
    "    \n",
    "    return pdf_text, pdf_links\n",
    "\n",
    "# Usage\n",
    "pdf_file_path = r\"C:\\Users\\abdullah\\projects\\Langchain\\Generative-AI\\LangChain\\RAG\\data\\My_CV_2.pdf\"\n",
    "text, links = load_pdf_with_links(pdf_file_path)\n",
    "\n",
    "print(\"Extracted Text:\")\n",
    "print(text)\n",
    "\n",
    "print(\"\\nExtracted Links:\")\n",
    "for link in links:\n",
    "    print(link)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RecursiveCharacterTextSplitter: The Versatile Powerhouse\n",
    "\n",
    "The RecursiveCharacterTextSplitter is Langchain’s most versatile text splitter. It attempts to split text on a list of characters in order, falling back to the next option if the resulting chunks are too large.\n",
    "\n",
    "When to use:\n",
    "- As a default choice for most general-purpose text splitting tasks.\n",
    "- When dealing with various document types with different structures.\n",
    "- To maintain semantic coherence in splits as much as possible.\n",
    "\n",
    "This splitter tries to split on double newlines first, then single newlines, spaces, and finally individual characters if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text = \"Your long document text here...\"\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len\n",
    ")\n",
    "\n",
    "chunks = splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ". TokenTextSplitter: Precision for Token-Based Models\n",
    "\n",
    "The TokenTextSplitter is designed to split text based on the number of tokens, which is particularly useful when working with models that have specific token limits, such as GPT-3 or any other models.\n",
    "\n",
    "When to use:\n",
    "- When working with token-sensitive models\n",
    "- To ensure that chunks fit within model token limits\n",
    "- For more precise control over input size for language models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import TokenTextSplitter\n",
    "\n",
    "text = \"Your long document text here...\"\n",
    "\n",
    "splitter = TokenTextSplitter(\n",
    "    encoding_name=\"cl100k_base\",  # OpenAI's encoding\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=20\n",
    ")\n",
    "\n",
    "chunks = splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Your long document text here...']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " MarkdownHeaderTextSplitter: Structure-Aware Splitting for Markdown\n",
    "The MarkdownHeaderTextSplitter is specially designed to handle Markdown documents, respecting the header hierarchy and document structure.\n",
    "\n",
    "When to use:\n",
    "- Specifically for Markdown documents\n",
    "- To maintain the logical structure of documentation or articles\n",
    "- When header-based organization is crucial for your RAG application \n",
    "\n",
    "This splitter creates chunks based on Markdown headers, preserving the document’s hierarchical structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
    "\n",
    "markdown_text = \"\"\"\n",
    "# Title\n",
    "## Section 1\n",
    "Content of section 1\n",
    "## Section 2\n",
    "Content of section 2\n",
    "### Subsection 2.1\n",
    "Content of subsection 2.1\n",
    "\"\"\"\n",
    "\n",
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\"),\n",
    "    (\"###\", \"Header 3\"),\n",
    "]\n",
    "\n",
    "splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "chunks = splitter.split_text(markdown_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PythonCodeTextSplitter: Tailored for Code Splitting\n",
    "\n",
    "The PythonCodeTextSplitter is designed specifically for splitting Python source code, respecting function and class boundaries.\n",
    "\n",
    "When to use:\n",
    "- When working with Python codebases\n",
    "- For code documentation or analysis tasks\n",
    "- To maintain the integrity of code structures in your splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['def function1():\\n    print(\"Hello, World!\")',\n",
       " 'class MyClass:\\n    def __init__(self):\\n        self.value = 42',\n",
       " 'def method1(self):\\n        return self.value']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import PythonCodeTextSplitter\n",
    "\n",
    "python_code = \"\"\"\n",
    "def function1():\n",
    "    print(\"Hello, World!\")\n",
    "\n",
    "class MyClass:\n",
    "    def __init__(self):\n",
    "        self.value = 42\n",
    "\n",
    "    def method1(self):\n",
    "        return self.value\n",
    "\"\"\"\n",
    "\n",
    "splitter = PythonCodeTextSplitter(\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=20\n",
    ")\n",
    "\n",
    "chunks = splitter.split_text(python_code)\n",
    "\n",
    "chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HTMLTextSplitter: Structured Splitting for Web Content\n",
    "The HTMLTextSplitter is tailored for HTML documents, maintaining the structure and hierarchy of HTML elements.\n",
    "\n",
    "When to use:\n",
    "- For processing web pages or HTML-formatted documents\n",
    "- When HTML structure is important for your RAG task\n",
    "- To extract content while preserving HTML context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'Header 1': 'Main Title'}, page_content='Main Title \\n This is a paragraph.'),\n",
       " Document(metadata={'Header 2': 'Subsection'}, page_content='Subsection \\n Another paragraph.')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!pip install langchain_text_splitter\n",
    "from langchain_text_splitters import HTMLSectionSplitter\n",
    "\n",
    "html_text = \"\"\"\n",
    "<html>\n",
    "<body>\n",
    "<h1>Main Title</h1>\n",
    "<p>This is a paragraph.</p>\n",
    "<div>\n",
    "    <h2>Subsection</h2>\n",
    "    <p>Another paragraph.</p>\n",
    "</div>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "headers_to_split_on = [(\"h1\", \"Header 1\"), (\"h2\", \"Header 2\")]\n",
    "splitter = HTMLSectionSplitter(\n",
    "    headers_to_split_on=headers_to_split_on,\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=20\n",
    ")\n",
    "\n",
    "chunks = splitter.split_text(html_text)\n",
    "\n",
    "chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SpacyTextSplitter: Advanced Linguistic Splitting\n",
    "The SpacyTextSplitter leverages the spaCy library for more advanced, language-aware text splitting.\n",
    "\n",
    "When to use:\n",
    "- For highly accurate, linguistically informed splitting\n",
    "- When working with multiple languages\n",
    "- For tasks requiring deep language understanding\n",
    "\n",
    "Remember to install spaCy and the appropriate language model before using this splitter.\n",
    "\n",
    "Remember to install spaCy and the appropriate language model before using this splitter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Your long document text here.\\n\\nIt can be in various languages.',\n",
       " 'SpaCy will handle the linguistic nuances.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import SpacyTextSplitter\n",
    "\n",
    "text = \"Your long document text here. It can be in various languages. SpaCy will handle the linguistic nuances.\"\n",
    "\n",
    "splitter = SpacyTextSplitter(\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=20\n",
    ")\n",
    "\n",
    "chunks = splitter.split_text(text)\n",
    "chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LatexTextSplitter: Specialized for LaTeX Documents\n",
    "The LatexTextSplitter is designed to handle LaTeX documents, respecting the unique structure and commands of LaTeX syntax.\n",
    "\n",
    "When to use:\n",
    "- Specifically for LaTeX documents\n",
    "- In academic or scientific document processing\n",
    "- To maintain LaTeX formatting and structure in splits\n",
    "\n",
    "This splitter attempts to maintain the integrity of LaTeX commands and environments while creating chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\\\documentclass{article}\\n\\\\begin{document}\\n\\\\section{Introduction}\\nThis is the',\n",
       " 'is the introduction.\\n\\\\section{Methodology}\\nThis is the methodology section.\\n\\\\end{document}']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import LatexTextSplitter\n",
    "\n",
    "latex_text = r\"\"\"\n",
    "\\documentclass{article}\n",
    "\\begin{document}\n",
    "\\section{Introduction}\n",
    "This is the introduction.\n",
    "\\section{Methodology}\n",
    "This is the methodology section.\n",
    "\\end{document}\n",
    "\"\"\"\n",
    "\n",
    "splitter = LatexTextSplitter(\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=20\n",
    ")\n",
    "\n",
    "chunks = splitter.split_text(latex_text)\n",
    "chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosing the right text splitter is crucial for optimizing your RAG pipeline in Langchain. Each splitter offers unique advantages suited to different document types and use cases. The RecursiveCharacterTextSplitter serves as an excellent default choice for general purposes, while specialized splitters like MarkdownHeaderTextSplitter or PythonCodeTextSplitter offer tailored solutions for specific document formats.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code\n",
    "Since Programming languages have different structures than plain text, we can split the code based on the syntax of the specific language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='def add(a, b):\\n    return a + b'),\n",
       " Document(metadata={}, page_content='class Calculator:\\n    def __init__(self):\\n        self.result = 0'),\n",
       " Document(metadata={}, page_content='def add(self, value):\\n        self.result += value\\n        return self.result'),\n",
       " Document(metadata={}, page_content='def subtract(self, value):\\n        self.result -= value\\n        return self.result'),\n",
       " Document(metadata={}, page_content='# Call the function'),\n",
       " Document(metadata={}, page_content='def main():\\n    calc = Calculator()\\n    print(calc.add(5))\\n    print(calc.subtract(2))'),\n",
       " Document(metadata={}, page_content='if __name__ == \"__main__\":\\n    main()')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter, Language\n",
    "\n",
    "PYTHON_CODE = \"\"\"\n",
    "def add(a, b):\n",
    "    return a + b\n",
    "\n",
    "class Calculator:\n",
    "    def __init__(self):\n",
    "        self.result = 0\n",
    "\n",
    "    def add(self, value):\n",
    "        self.result += value\n",
    "        return self.result\n",
    "\n",
    "    def subtract(self, value):\n",
    "        self.result -= value\n",
    "        return self.result\n",
    "\n",
    "# Call the function\n",
    "def main():\n",
    "    calc = Calculator()\n",
    "    print(calc.add(5))\n",
    "    print(calc.subtract(2))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\"\"\"\n",
    "python_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.PYTHON, chunk_size=100, chunk_overlap=0)\n",
    "    \n",
    "python_docs = python_splitter.create_documents([PYTHON_CODE])\n",
    "python_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JSON\n",
    "A nested json object can be split such that initial json keys are in all the related chunks of text. If there are any long lists inside, we can convert them into dictionaries to split. Let’s look at an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84\n",
      "{\"company\": {\"name\": \"TechCorp\", \"location\": {\"city\": \"Metropolis\", \"state\": \"NY\"}}}\n",
      "183\n",
      "{\"company\": {\"departments\": {\"0\": {\"name\": \"Research\", \"employees\": {\"0\": {\"name\": \"Alice\", \"age\": 30, \"role\": \"Scientist\"}, \"1\": {\"name\": \"Bob\", \"age\": 25, \"role\": \"Technician\"}}}}}}\n",
      "188\n",
      "{\"company\": {\"departments\": {\"1\": {\"name\": \"Development\", \"employees\": {\"0\": {\"name\": \"Charlie\", \"age\": 35, \"role\": \"Engineer\"}, \"1\": {\"name\": \"David\", \"age\": 28, \"role\": \"Developer\"}}}}}}\n",
      "70\n",
      "{\"financials\": {\"year\": 2023, \"revenue\": 1000000, \"expenses\": 750000}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveJsonSplitter\n",
    "\n",
    "# Example JSON object\n",
    "json_data = {\n",
    "    \"company\": {\n",
    "        \"name\": \"TechCorp\",\n",
    "        \"location\": {\n",
    "            \"city\": \"Metropolis\",\n",
    "            \"state\": \"NY\"\n",
    "        },\n",
    "        \"departments\": [\n",
    "            {\n",
    "                \"name\": \"Research\",\n",
    "                \"employees\": [\n",
    "                    {\"name\": \"Alice\", \"age\": 30, \"role\": \"Scientist\"},\n",
    "                    {\"name\": \"Bob\", \"age\": 25, \"role\": \"Technician\"}\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"Development\",\n",
    "                \"employees\": [\n",
    "                    {\"name\": \"Charlie\", \"age\": 35, \"role\": \"Engineer\"},\n",
    "                    {\"name\": \"David\", \"age\": 28, \"role\": \"Developer\"}\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    \"financials\": {\n",
    "        \"year\": 2023,\n",
    "        \"revenue\": 1000000,\n",
    "        \"expenses\": 750000\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "# Initialize the RecursiveJsonSplitter with a maximum chunk size\n",
    "splitter = RecursiveJsonSplitter(max_chunk_size=200, min_chunk_size=20)\n",
    "\n",
    "# Split the JSON object\n",
    "chunks = splitter.split_text(json_data, convert_lists=True)\n",
    "\n",
    "# Process the chunks as needed\n",
    "for chunk in chunks:\n",
    "    print(len(chunk))\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
