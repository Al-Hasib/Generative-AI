{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mastering Text Splitting in Langchain\n",
    "\n",
    "Retrieval-Augmented Generation (RAG) has emerged as a powerful technique for enhancing the accuracy and relevance of AI-generated responses. At the heart of RAG lies a crucial step: “text splitting”. This process involves breaking down large documents into smaller, manageable chunks that can be efficiently processed and retrieved.\n",
    "\n",
    "Langchain, a popular framework for developing applications with large language models (LLMs), offers a variety of text splitting techniques. \n",
    "\n",
    "CharacterTextSplitter: The Simple Solution\n",
    "The CharacterTextSplitter is the most basic text splitting technique in Langchain. It divides text based on a specified number of characters, making it suitable for simple, uniform text splitting tasks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLMs have limits on context window size in terms of token numbers. Even if the context size is infinite, more input tokens will lead to higher costs, and money is not infinite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text will be split only at new lines since we are using the new line (“\\n”) as the separator. \n",
    "\n",
    "Recursive\n",
    "Rather than using a single separator, we use multiple separators. This method will use each separator sequentially to split the data until the chunk reaches less than chunk_size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "text = \"Your long document text here...\"\n",
    "\n",
    "splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\\n\",  #used to avoid splitting in the middle of paragraphs.\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200\n",
    ")\n",
    "\n",
    "chunks = splitter.split_text(text) #you can also split documents using split_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Your long document text here...']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RecursiveCharacterTextSplitter: The Versatile Powerhouse\n",
    "\n",
    "The RecursiveCharacterTextSplitter is Langchain’s most versatile text splitter. It attempts to split text on a list of characters in order, falling back to the next option if the resulting chunks are too large.\n",
    "\n",
    "When to use:\n",
    "- As a default choice for most general-purpose text splitting tasks.\n",
    "- When dealing with various document types with different structures.\n",
    "- To maintain semantic coherence in splits as much as possible.\n",
    "\n",
    "This splitter tries to split on double newlines first, then single newlines, spaces, and finally individual characters if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text = \"Your long document text here...\"\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len\n",
    ")\n",
    "\n",
    "chunks = splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ". TokenTextSplitter: Precision for Token-Based Models\n",
    "\n",
    "The TokenTextSplitter is designed to split text based on the number of tokens, which is particularly useful when working with models that have specific token limits, such as GPT-3 or any other models.\n",
    "\n",
    "When to use:\n",
    "- When working with token-sensitive models\n",
    "- To ensure that chunks fit within model token limits\n",
    "- For more precise control over input size for language models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import TokenTextSplitter\n",
    "\n",
    "text = \"Your long document text here...\"\n",
    "\n",
    "splitter = TokenTextSplitter(\n",
    "    encoding_name=\"cl100k_base\",  # OpenAI's encoding\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=20\n",
    ")\n",
    "\n",
    "chunks = splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Your long document text here...']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " MarkdownHeaderTextSplitter: Structure-Aware Splitting for Markdown\n",
    "The MarkdownHeaderTextSplitter is specially designed to handle Markdown documents, respecting the header hierarchy and document structure.\n",
    "\n",
    "When to use:\n",
    "- Specifically for Markdown documents\n",
    "- To maintain the logical structure of documentation or articles\n",
    "- When header-based organization is crucial for your RAG application \n",
    "\n",
    "This splitter creates chunks based on Markdown headers, preserving the document’s hierarchical structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
    "\n",
    "markdown_text = \"\"\"\n",
    "# Title\n",
    "## Section 1\n",
    "Content of section 1\n",
    "## Section 2\n",
    "Content of section 2\n",
    "### Subsection 2.1\n",
    "Content of subsection 2.1\n",
    "\"\"\"\n",
    "\n",
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\"),\n",
    "    (\"###\", \"Header 3\"),\n",
    "]\n",
    "\n",
    "splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "chunks = splitter.split_text(markdown_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PythonCodeTextSplitter: Tailored for Code Splitting\n",
    "\n",
    "The PythonCodeTextSplitter is designed specifically for splitting Python source code, respecting function and class boundaries.\n",
    "\n",
    "When to use:\n",
    "- When working with Python codebases\n",
    "- For code documentation or analysis tasks\n",
    "- To maintain the integrity of code structures in your splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['def function1():\\n    print(\"Hello, World!\")',\n",
       " 'class MyClass:\\n    def __init__(self):\\n        self.value = 42',\n",
       " 'def method1(self):\\n        return self.value']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import PythonCodeTextSplitter\n",
    "\n",
    "python_code = \"\"\"\n",
    "def function1():\n",
    "    print(\"Hello, World!\")\n",
    "\n",
    "class MyClass:\n",
    "    def __init__(self):\n",
    "        self.value = 42\n",
    "\n",
    "    def method1(self):\n",
    "        return self.value\n",
    "\"\"\"\n",
    "\n",
    "splitter = PythonCodeTextSplitter(\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=20\n",
    ")\n",
    "\n",
    "chunks = splitter.split_text(python_code)\n",
    "\n",
    "chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HTMLTextSplitter: Structured Splitting for Web Content\n",
    "The HTMLTextSplitter is tailored for HTML documents, maintaining the structure and hierarchy of HTML elements.\n",
    "\n",
    "When to use:\n",
    "- For processing web pages or HTML-formatted documents\n",
    "- When HTML structure is important for your RAG task\n",
    "- To extract content while preserving HTML context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'Header 1': 'Main Title'}, page_content='Main Title \\n This is a paragraph.'),\n",
       " Document(metadata={'Header 2': 'Subsection'}, page_content='Subsection \\n Another paragraph.')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!pip install langchain_text_splitter\n",
    "from langchain_text_splitters import HTMLSectionSplitter\n",
    "\n",
    "html_text = \"\"\"\n",
    "<html>\n",
    "<body>\n",
    "<h1>Main Title</h1>\n",
    "<p>This is a paragraph.</p>\n",
    "<div>\n",
    "    <h2>Subsection</h2>\n",
    "    <p>Another paragraph.</p>\n",
    "</div>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "headers_to_split_on = [(\"h1\", \"Header 1\"), (\"h2\", \"Header 2\")]\n",
    "splitter = HTMLSectionSplitter(\n",
    "    headers_to_split_on=headers_to_split_on,\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=20\n",
    ")\n",
    "\n",
    "chunks = splitter.split_text(html_text)\n",
    "\n",
    "chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SpacyTextSplitter: Advanced Linguistic Splitting\n",
    "The SpacyTextSplitter leverages the spaCy library for more advanced, language-aware text splitting.\n",
    "\n",
    "When to use:\n",
    "- For highly accurate, linguistically informed splitting\n",
    "- When working with multiple languages\n",
    "- For tasks requiring deep language understanding\n",
    "\n",
    "Remember to install spaCy and the appropriate language model before using this splitter.\n",
    "\n",
    "Remember to install spaCy and the appropriate language model before using this splitter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Your long document text here.\\n\\nIt can be in various languages.',\n",
       " 'SpaCy will handle the linguistic nuances.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import SpacyTextSplitter\n",
    "\n",
    "text = \"Your long document text here. It can be in various languages. SpaCy will handle the linguistic nuances.\"\n",
    "\n",
    "splitter = SpacyTextSplitter(\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=20\n",
    ")\n",
    "\n",
    "chunks = splitter.split_text(text)\n",
    "chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LatexTextSplitter: Specialized for LaTeX Documents\n",
    "The LatexTextSplitter is designed to handle LaTeX documents, respecting the unique structure and commands of LaTeX syntax.\n",
    "\n",
    "When to use:\n",
    "- Specifically for LaTeX documents\n",
    "- In academic or scientific document processing\n",
    "- To maintain LaTeX formatting and structure in splits\n",
    "\n",
    "This splitter attempts to maintain the integrity of LaTeX commands and environments while creating chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\\\documentclass{article}\\n\\\\begin{document}\\n\\\\section{Introduction}\\nThis is the',\n",
       " 'is the introduction.\\n\\\\section{Methodology}\\nThis is the methodology section.\\n\\\\end{document}']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import LatexTextSplitter\n",
    "\n",
    "latex_text = r\"\"\"\n",
    "\\documentclass{article}\n",
    "\\begin{document}\n",
    "\\section{Introduction}\n",
    "This is the introduction.\n",
    "\\section{Methodology}\n",
    "This is the methodology section.\n",
    "\\end{document}\n",
    "\"\"\"\n",
    "\n",
    "splitter = LatexTextSplitter(\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=20\n",
    ")\n",
    "\n",
    "chunks = splitter.split_text(latex_text)\n",
    "chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosing the right text splitter is crucial for optimizing your RAG pipeline in Langchain. Each splitter offers unique advantages suited to different document types and use cases. The RecursiveCharacterTextSplitter serves as an excellent default choice for general purposes, while specialized splitters like MarkdownHeaderTextSplitter or PythonCodeTextSplitter offer tailored solutions for specific document formats.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code\n",
    "Since Programming languages have different structures than plain text, we can split the code based on the syntax of the specific language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='def add(a, b):\\n    return a + b'),\n",
       " Document(metadata={}, page_content='class Calculator:\\n    def __init__(self):\\n        self.result = 0'),\n",
       " Document(metadata={}, page_content='def add(self, value):\\n        self.result += value\\n        return self.result'),\n",
       " Document(metadata={}, page_content='def subtract(self, value):\\n        self.result -= value\\n        return self.result'),\n",
       " Document(metadata={}, page_content='# Call the function'),\n",
       " Document(metadata={}, page_content='def main():\\n    calc = Calculator()\\n    print(calc.add(5))\\n    print(calc.subtract(2))'),\n",
       " Document(metadata={}, page_content='if __name__ == \"__main__\":\\n    main()')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter, Language\n",
    "\n",
    "PYTHON_CODE = \"\"\"\n",
    "def add(a, b):\n",
    "    return a + b\n",
    "\n",
    "class Calculator:\n",
    "    def __init__(self):\n",
    "        self.result = 0\n",
    "\n",
    "    def add(self, value):\n",
    "        self.result += value\n",
    "        return self.result\n",
    "\n",
    "    def subtract(self, value):\n",
    "        self.result -= value\n",
    "        return self.result\n",
    "\n",
    "# Call the function\n",
    "def main():\n",
    "    calc = Calculator()\n",
    "    print(calc.add(5))\n",
    "    print(calc.subtract(2))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\"\"\"\n",
    "python_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.PYTHON, chunk_size=100, chunk_overlap=0)\n",
    "    \n",
    "python_docs = python_splitter.create_documents([PYTHON_CODE])\n",
    "python_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JSON\n",
    "A nested json object can be split such that initial json keys are in all the related chunks of text. If there are any long lists inside, we can convert them into dictionaries to split. Let’s look at an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84\n",
      "{\"company\": {\"name\": \"TechCorp\", \"location\": {\"city\": \"Metropolis\", \"state\": \"NY\"}}}\n",
      "183\n",
      "{\"company\": {\"departments\": {\"0\": {\"name\": \"Research\", \"employees\": {\"0\": {\"name\": \"Alice\", \"age\": 30, \"role\": \"Scientist\"}, \"1\": {\"name\": \"Bob\", \"age\": 25, \"role\": \"Technician\"}}}}}}\n",
      "188\n",
      "{\"company\": {\"departments\": {\"1\": {\"name\": \"Development\", \"employees\": {\"0\": {\"name\": \"Charlie\", \"age\": 35, \"role\": \"Engineer\"}, \"1\": {\"name\": \"David\", \"age\": 28, \"role\": \"Developer\"}}}}}}\n",
      "70\n",
      "{\"financials\": {\"year\": 2023, \"revenue\": 1000000, \"expenses\": 750000}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveJsonSplitter\n",
    "\n",
    "# Example JSON object\n",
    "json_data = {\n",
    "    \"company\": {\n",
    "        \"name\": \"TechCorp\",\n",
    "        \"location\": {\n",
    "            \"city\": \"Metropolis\",\n",
    "            \"state\": \"NY\"\n",
    "        },\n",
    "        \"departments\": [\n",
    "            {\n",
    "                \"name\": \"Research\",\n",
    "                \"employees\": [\n",
    "                    {\"name\": \"Alice\", \"age\": 30, \"role\": \"Scientist\"},\n",
    "                    {\"name\": \"Bob\", \"age\": 25, \"role\": \"Technician\"}\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"Development\",\n",
    "                \"employees\": [\n",
    "                    {\"name\": \"Charlie\", \"age\": 35, \"role\": \"Engineer\"},\n",
    "                    {\"name\": \"David\", \"age\": 28, \"role\": \"Developer\"}\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    \"financials\": {\n",
    "        \"year\": 2023,\n",
    "        \"revenue\": 1000000,\n",
    "        \"expenses\": 750000\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "# Initialize the RecursiveJsonSplitter with a maximum chunk size\n",
    "splitter = RecursiveJsonSplitter(max_chunk_size=200, min_chunk_size=20)\n",
    "\n",
    "# Split the JSON object\n",
    "chunks = splitter.split_text(json_data, convert_lists=True)\n",
    "\n",
    "# Process the chunks as needed\n",
    "for chunk in chunks:\n",
    "    print(len(chunk))\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
